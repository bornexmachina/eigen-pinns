{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4258ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multigrid_gnn_multires_physics.py\n",
    "\"\"\"\n",
    "Physics-informed Multigrid + GNN eigen-refinement\n",
    "- Exact solve only on coarsest mesh\n",
    "- Multiresolution GNN with residual + orthonormality + projection loss\n",
    "- Coarse-to-fine prolongation only\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def farthest_point_sampling(points, n_samples, seed=None):\n",
    "    \"\"\"\n",
    "    Farthest Point Sampling (FPS) for nested hierarchical sampling.\n",
    "    \n",
    "    Args:\n",
    "        points: (N, 3) array of 3D points\n",
    "        n_samples: number of points to sample\n",
    "        seed: random seed for initial point selection\n",
    "    \n",
    "    Returns:\n",
    "        indices: (n_samples,) array of selected point indices\n",
    "    \"\"\"\n",
    "    n_points = points.shape[0]\n",
    "    if n_samples >= n_points:\n",
    "        return np.arange(n_points)\n",
    "    \n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # Start with a random point\n",
    "    selected_indices = [rng.integers(0, n_points)]\n",
    "    distances = np.full(n_points, np.inf)\n",
    "    \n",
    "    for _ in range(n_samples - 1):\n",
    "        # Update distances to nearest selected point\n",
    "        last_selected = selected_indices[-1]\n",
    "        dists_to_last = np.linalg.norm(points - points[last_selected], axis=1)\n",
    "        distances = np.minimum(distances, dists_to_last)\n",
    "        \n",
    "        # Select point farthest from all selected points\n",
    "        farthest_idx = np.argmax(distances)\n",
    "        selected_indices.append(farthest_idx)\n",
    "    \n",
    "    return np.array(selected_indices)\n",
    "\n",
    "# ------------------------\n",
    "# Simple neighbor-mean corrector\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128,64,32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        n = x.shape[0]\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device).clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)\n",
    "        return self.net(h)\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid GNN solver\n",
    "# ------------------------\n",
    "class MultigridGNN:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Physics-informed GNN training\n",
    "    # ------------------------\n",
    "    def train_multiresolution(self, X_list, U_init_list, edge_index_list,\n",
    "                              epochs=1000, lr=1e-3, corr_scale=1e-2,\n",
    "                              w_res=5.0, w_orth=1.0, w_proj=1e-2,\n",
    "                              grad_clip=1.0, weight_decay=1e-6, log_every=250):\n",
    "        device = self.device\n",
    "        n_modes = U_init_list[0].shape[1]\n",
    "\n",
    "        # Build torch tensors and resolution indicators\n",
    "        x_feats_all, U_all, edge_index_all = [], [], []\n",
    "        node_offset = 0\n",
    "        max_nodes = max([X.shape[0] for X in X_list])\n",
    "        for X, U_init, edge_index in zip(X_list, U_init_list, edge_index_list):\n",
    "            res_feat = np.full((X.shape[0], 1), X.shape[0]/max_nodes)\n",
    "            x_feats_all.append(np.hstack([X, U_init, res_feat]))\n",
    "            U_all.append(U_init)\n",
    "            edge_index_all.append(edge_index + node_offset)\n",
    "            node_offset += X.shape[0]\n",
    "\n",
    "        x_feats_all = torch.FloatTensor(np.vstack(x_feats_all)).to(device)\n",
    "        U_all_tensor = torch.FloatTensor(np.vstack(U_all)).to(device)\n",
    "        edge_index_all = torch.cat(edge_index_all, dim=1).to(device)\n",
    "\n",
    "        in_dim = x_feats_all.shape[1]\n",
    "        if self.model is None:\n",
    "            self.model = SimpleCorrector(in_dim, n_modes).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "\n",
    "        # ------------------------\n",
    "        # Precompute Laplacians per level\n",
    "        L_list, M_list = [], []\n",
    "        node_offset = 0\n",
    "        for X in X_list:\n",
    "            L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "            L_list.append(sp_to_torch_sparse(L).to(device))\n",
    "            M_list.append(sp_to_torch_sparse(M).to(device))\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            corr_raw = self.model(x_feats_all, edge_index_all)\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_all_tensor + corr\n",
    "\n",
    "            # Physics-informed loss\n",
    "            loss = 0.0\n",
    "            node_offset = 0\n",
    "            L_res_total = 0.0\n",
    "            L_orth_total = 0.0\n",
    "            L_mean_total = 0.0\n",
    "            \n",
    "            for i, (L_t, M_t, U_init) in enumerate(zip(L_list, M_list, U_init_list)):\n",
    "                n_nodes = U_init.shape[0]\n",
    "                U_level = U_pred[node_offset:node_offset+n_nodes]\n",
    "\n",
    "                # Rayleigh residual\n",
    "                Lu = torch.sparse.mm(L_t, U_level)\n",
    "                Mu = torch.sparse.mm(M_t, U_level)\n",
    "                num = torch.sum(U_level * Lu, dim=0)\n",
    "                den = torch.sum(U_level * Mu, dim=0) + 1e-12\n",
    "                lambdas = num / den\n",
    "                res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "                L_res = torch.mean(res**2)\n",
    "                \n",
    "                # Orthonormality\n",
    "                Gram = U_level.t() @ Mu\n",
    "                L_orth = torch.mean((Gram - torch.eye(n_modes, device=device))**2)\n",
    "\n",
    "                # Zero-mean constraint: 1.T @ M @ u = 0 for modes 1 onwards\n",
    "                ones = torch.ones(n_nodes, 1, device=device)\n",
    "                mean_constraint = ones.t() @ Mu[:, 1:]  # Shape: (1, n_modes-1)\n",
    "                L_mean = torch.mean(mean_constraint**2)\n",
    "\n",
    "                loss += w_res * L_res + w_orth * L_orth + w_proj * L_mean\n",
    "                node_offset += n_nodes\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.model.parameters()), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if ep % log_every == 0 or ep == epochs-1:\n",
    "                print(f\"Epoch {ep:4d}: Loss={loss.item():.6f} | Res={L_res.item():.6f} | Orth={L_orth.item():.6f} | Mean={L_mean.item():.6f}\")\n",
    "\n",
    "        return U_pred.detach().cpu().numpy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L, M):\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        L_t = sp_to_torch_sparse(L).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "    \n",
    "\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None):\n",
    "    \"\"\"Visualize mesh with vertices, optionally highlighting specific points.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    # Plot full mesh with transparency\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2], \n",
    "                    triangles=mesh.connectivity, alpha=0.3)\n",
    "    \n",
    "    # Highlight specific points if provided\n",
    "    if highlight_indices is not None:\n",
    "        highlighted_verts = mesh.verts[highlight_indices]\n",
    "        ax.scatter(highlighted_verts[:, 0], highlighted_verts[:, 1], highlighted_verts[:, 2], \n",
    "                   c='fuchsia', s=10, alpha=0.8, label=f'{len(highlight_indices)} selected points')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=130, azim=-90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hierarchy = [128, 512, 1024]  # final level is full mesh\n",
    "    k_neighbors = 4\n",
    "    epochs = 5_000\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridGNN.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    # Use Farthest Point Sampling for nested hierarchy\n",
    "    print(\"Computing farthest point sampling hierarchy...\")\n",
    "    all_sampled_indices = farthest_point_sampling(X_full, hierarchy[-1], seed=42)\n",
    "    \n",
    "    # Create nested levels by taking first n points from FPS result\n",
    "    indices_per_level = {}\n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        indices_per_level[i] = all_sampled_indices[:n_points].copy()\n",
    "        print(f\"  Level {i}: {n_points} points (nested FPS)\")\n",
    "    \n",
    "\n",
    "    #for level_idx_vis, n_points in enumerate(hierarchy):\n",
    "    #    # Visualize selected points for this level\n",
    "    #    visualize_mesh(mesh, \n",
    "    #                  title=f'Level {level_idx_vis}: {n_points} FPS Selected Points',\n",
    "    #                  highlight_indices=indices_per_level[level_idx_vis])\n",
    "    #print()\n",
    "\n",
    "    solver = MultigridGNN()\n",
    "\n",
    "    # ------------------------\n",
    "    # Level 0: exact coarse solve\n",
    "    # ------------------------\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(f\"\\nLEVEL 0: exact solve on {X0.shape[0]} points...\")\n",
    "    lambda0, U0, L0, M0 = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda0,6))\n",
    "\n",
    "    # ------------------------\n",
    "    # Coarse-to-fine prolongation\n",
    "    # ------------------------\n",
    "    U_prev = U0.copy()\n",
    "    X_list, U_init_list, edge_index_list = [X0], [U0], [solver.build_knn_graph(X0, k=k_neighbors)]\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level-1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "\n",
    "        P = solver.build_prolongation(Xc, Xf, k=1)\n",
    "        U_init = P @ U_prev\n",
    "        edge_index = solver.build_knn_graph(Xf, k=k_neighbors)\n",
    "\n",
    "        X_list.append(Xf)\n",
    "        U_init_list.append(U_init)\n",
    "        edge_index_list.append(edge_index)\n",
    "\n",
    "        U_prev = U_init.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Train physics-informed GNN\n",
    "    # ------------------------\n",
    "    print(\"\\nTraining physics-informed multiresolution GNN...\")\n",
    "    U_pred_all = solver.train_multiresolution(X_list, U_init_list, edge_index_list,\n",
    "                                              epochs=epochs)\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement per level\n",
    "    # ------------------------\n",
    "    node_offset = 0\n",
    "    for level, X in enumerate(X_list):\n",
    "        n_nodes = X.shape[0]\n",
    "        U_pred = U_pred_all[node_offset:node_offset+n_nodes]\n",
    "        node_offset += n_nodes\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals_refined, _ = solver.refine_eigenvectors(U_pred, L, M)\n",
    "        print(f\"Level {level} refined eigenvalues: {np.round(vals_refined,3)}\")\n",
    "\n",
    "    return U_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61073204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Hierarchy: [128, 512, 1024, 2503]\n",
      "Computing farthest point sampling hierarchy...\n",
      "  Level 0: 128 points (nested FPS)\n",
      "  Level 1: 512 points (nested FPS)\n",
      "  Level 2: 1024 points (nested FPS)\n",
      "  Level 3: 2503 points (nested FPS)\n",
      "\n",
      "LEVEL 0: exact solve on 128 points...\n",
      "Coarse eigenvalues: [0.       0.446669 0.931944 1.450187 1.657461 2.553185 3.398102 3.599375\n",
      " 3.927035 4.400232]\n",
      "\n",
      "Training physics-informed multiresolution GNN...\n",
      "Epoch    0: Loss=2.111959 | Res=0.066544 | Orth=0.844122 | Mean=3.179780\n",
      "Epoch  250: Loss=0.426303 | Res=0.022362 | Orth=0.018540 | Mean=0.015409\n",
      "Epoch  500: Loss=0.138386 | Res=0.006003 | Orth=0.002864 | Mean=0.011143\n",
      "Epoch  750: Loss=0.082494 | Res=0.003246 | Orth=0.001405 | Mean=0.002694\n",
      "Epoch 1000: Loss=0.054337 | Res=0.002003 | Orth=0.000653 | Mean=0.003123\n",
      "Epoch 1250: Loss=0.035157 | Res=0.001082 | Orth=0.000294 | Mean=0.001795\n",
      "Epoch 1500: Loss=0.026326 | Res=0.000767 | Orth=0.000139 | Mean=0.003987\n",
      "Epoch 1750: Loss=0.022235 | Res=0.000654 | Orth=0.000121 | Mean=0.003284\n",
      "Epoch 2000: Loss=0.019649 | Res=0.000594 | Orth=0.000082 | Mean=0.005220\n",
      "Epoch 2250: Loss=0.017702 | Res=0.000546 | Orth=0.000075 | Mean=0.000930\n",
      "Epoch 2500: Loss=0.016247 | Res=0.000506 | Orth=0.000065 | Mean=0.001163\n",
      "Epoch 2750: Loss=0.015230 | Res=0.000473 | Orth=0.000128 | Mean=0.007381\n",
      "Epoch 3000: Loss=0.014092 | Res=0.000448 | Orth=0.000052 | Mean=0.003789\n",
      "Epoch 3250: Loss=0.013299 | Res=0.000429 | Orth=0.000082 | Mean=0.004924\n",
      "Epoch 3500: Loss=0.012518 | Res=0.000412 | Orth=0.000034 | Mean=0.003279\n",
      "Epoch 3750: Loss=0.011902 | Res=0.000398 | Orth=0.000048 | Mean=0.003253\n",
      "Epoch 4000: Loss=0.011384 | Res=0.000386 | Orth=0.000032 | Mean=0.004631\n",
      "Epoch 4250: Loss=0.010910 | Res=0.000374 | Orth=0.000041 | Mean=0.003299\n",
      "Epoch 4500: Loss=0.010457 | Res=0.000365 | Orth=0.000030 | Mean=0.002275\n",
      "Epoch 4750: Loss=0.010295 | Res=0.000356 | Orth=0.000094 | Mean=0.010234\n",
      "Epoch 4999: Loss=0.009746 | Res=0.000348 | Orth=0.000020 | Mean=0.002324\n",
      "Level 0 refined eigenvalues: [1.000e-03 4.610e-01 9.690e-01 1.464e+00 1.672e+00 2.569e+00 3.411e+00\n",
      " 3.650e+00 3.957e+00 4.415e+00]\n",
      "Level 1 refined eigenvalues: [0.01  0.581 1.134 1.82  2.044 2.723 3.082 3.33  4.178 4.715]\n",
      "Level 2 refined eigenvalues: [0.016 0.554 1.141 1.585 1.833 2.101 2.73  3.293 3.852 4.377]\n",
      "Level 3 refined eigenvalues: [0.041 0.618 1.204 1.66  1.777 2.235 2.639 3.575 4.2   4.636]\n"
     ]
    }
   ],
   "source": [
    "U_pred = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3240b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "cells = [('triangle', m.connectivity)]\n",
    "\n",
    "m_out = meshio.Mesh(m.verts, cells, point_data={f'v{i}': U_pred[:, i] for i in range(1, 10)})\n",
    "m_out.write('bunny_eigfuncs_farthest_point_sampling.vtu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5e815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
