{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707dcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "import scipy.sparse.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import robust_laplacian\n",
    "from Mesh import Mesh\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.sparse import csr_matrix, dia_matrix, csc_matrix\n",
    "from scipy import sparse\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ============ NETWORK ARCHITECTURE ============\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"Sine activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "class EigenfunctionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to learn eigenfunctions on point clouds.\n",
    "    Input: 3D coordinates (x, y, z)\n",
    "    Output: eigenfunction value u(x,y,z) and eigenvalue λ\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, input_dim=3, initial_eigenvalue=0.0):\n",
    "        super().__init__()\n",
    "        self.activation = Sin()\n",
    "        \n",
    "        # Learnable eigenvalue with better initialization\n",
    "        self.eigenvalue_layer = nn.Linear(1, 1, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.eigenvalue_layer.weight.fill_(initial_eigenvalue)\n",
    "        \n",
    "        # Network layers - concatenate eigenvalue at each layer\n",
    "        self.fc1 = nn.Linear(input_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc2 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc3 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc4 = nn.Linear(hidden_dim + 1, 1)  # +1 for eigenvalue\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, 3) point cloud coordinates\n",
    "        Returns:\n",
    "            u: (N, 1) eigenfunction values\n",
    "            eigenvalue: scalar learnable eigenvalue\n",
    "        \"\"\"\n",
    "        # Learn eigenvalue and broadcast to match batch size\n",
    "        eigenvalue = torch.abs(self.eigenvalue_layer(torch.ones(1, 1).to(x.device)))\n",
    "        eigenvalue_expanded = eigenvalue.expand(x.shape[0], 1)  # (N, 1)\n",
    "        \n",
    "        # Forward pass - concatenate eigenvalue at each layer\n",
    "        h = torch.cat([x, eigenvalue_expanded], dim=1)  # (N, input_dim+1)\n",
    "        h = self.activation(self.fc1(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc2(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc3(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        u = self.fc4(h)\n",
    "        \n",
    "        return u, eigenvalue\n",
    "\n",
    "\n",
    "# ============ LOSS COMPUTATION ============\n",
    "def compute_eigenvalue_loss(u, eigenvalue, L, M, X, device):\n",
    "    \"\"\"\n",
    "    Compute residual for Lu = λMu using discrete operators.\n",
    "    \n",
    "    Args:\n",
    "        u: (N, 1) predicted eigenfunction values\n",
    "        eigenvalue: scalar predicted eigenvalue\n",
    "        L: (N, N) Laplacian matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        M: (N, N) Mass matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        X: (N, 3) point cloud coordinates\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        loss: MSE of residual ||Lu - λMu||²\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()  # (N,)\n",
    "    \n",
    "    # Convert sparse matrices to torch sparse tensors if needed\n",
    "    if sparse.issparse(L):\n",
    "        L_torch = sparse_to_torch(L, device)\n",
    "        M_torch = sparse_to_torch(M, device)\n",
    "    else:\n",
    "        L_torch = L\n",
    "        M_torch = M\n",
    "    \n",
    "    # Compute Lu and λMu\n",
    "    Lu = torch.sparse.mm(L_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    lMu = eigenvalue * Mu\n",
    "    \n",
    "    # Residual loss\n",
    "    residual = Lu - lMu\n",
    "    loss = torch.mean(residual ** 2)\n",
    "    \n",
    "    return loss, Lu, Mu\n",
    "\n",
    "\n",
    "def compute_normalization_loss(u, M, device):\n",
    "    \"\"\"\n",
    "    Enforce u^T M u = 1 (mass-matrix normalization).\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()\n",
    "    \n",
    "    if sparse.issparse(M):\n",
    "        M_torch = sparse_to_torch(M, device)\n",
    "    else:\n",
    "        M_torch = M\n",
    "    \n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    norm_squared = torch.dot(u_flat, Mu)\n",
    "    \n",
    "    # Penalize deviation from unit norm\n",
    "    loss = (norm_squared - 1.0) ** 2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_orthogonality_loss(u, previous_eigenfunctions, M, device):\n",
    "    \"\"\"\n",
    "    Enforce u ⊥ u_i for all previously found eigenfunctions.\n",
    "    Uses M-orthogonality: u^T M u_i = 0\n",
    "    \"\"\"\n",
    "    if len(previous_eigenfunctions) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    \n",
    "    u_flat = u.squeeze()\n",
    "    ortho_loss = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    if sparse.issparse(M):\n",
    "        M_torch = sparse_to_torch(M, device)\n",
    "    else:\n",
    "        M_torch = M\n",
    "    \n",
    "    for u_prev in previous_eigenfunctions:\n",
    "        u_prev_flat = u_prev.squeeze()\n",
    "        # Compute u^T M u_prev\n",
    "        Mu_prev = torch.sparse.mm(M_torch, u_prev_flat.unsqueeze(1)).squeeze()\n",
    "        overlap = torch.dot(u_flat, Mu_prev)\n",
    "        ortho_loss += overlap ** 2\n",
    "    \n",
    "    return ortho_loss\n",
    "\n",
    "\n",
    "# ============ UTILITY FUNCTIONS ============\n",
    "def sparse_to_torch(sparse_matrix, device):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    # Handle any scipy sparse format\n",
    "    if sparse.issparse(sparse_matrix):\n",
    "        coo = sparse_matrix.tocoo()\n",
    "    else:\n",
    "        raise ValueError(f\"Expected scipy sparse matrix, got {type(sparse_matrix)}\")\n",
    "    \n",
    "    indices = torch.LongTensor(np.vstack((coo.row, coo.col)))\n",
    "    values = torch.FloatTensor(coo.data)\n",
    "    shape = coo.shape\n",
    "    return torch.sparse_coo_tensor(indices, values, shape).to(device)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    \"\"\"Reinitialize network weights for finding next eigenfunction.\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# ============ MAIN TRAINING FUNCTION ============\n",
    "def train_eigenvalue_pinn(X, L, M, hidden_dim=64, epochs=20000, \n",
    "                          lr=1e-3, num_eigenfunctions=5, \n",
    "                          convergence_threshold=1e-7,\n",
    "                          ortho_weight=1.0):\n",
    "    \"\"\"print(f\"The true Eigenvalues are: {np.array2string(eigvals[:10], formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "print(f\"The predicted Eigenvalues are: {np.array2string(np.array(eigenvalues), formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "    Train PINN to solve Lu = λMu eigenvalue problem.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, 3) point cloud coordinates\n",
    "        L: (N, N) Laplacian matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        M: (N, N) Mass matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        epochs: Training epochs per eigenfunction\n",
    "        lr: Learning rate\n",
    "        num_eigenfunctions: Number of eigenfunctions to find\n",
    "        convergence_threshold: Threshold for detecting convergence\n",
    "        ortho_weight: Weight for orthogonality loss (increase if eigenfunctions overlap)\n",
    "    \n",
    "    Returns:\n",
    "        eigenvalues: List of found eigenvalues\n",
    "        eigenfunctions: List of (N, 1) eigenfunctions\n",
    "        loss_history: Training loss history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert inputs to torch (handle both numpy arrays and torch tensors)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X_torch = torch.FloatTensor(X).to(device)\n",
    "    elif isinstance(X, torch.Tensor):\n",
    "        X_torch = X.float().to(device)  # Ensure float32\n",
    "    else:\n",
    "        raise ValueError(f\"X must be numpy array or torch tensor, got {type(X)}\")\n",
    "    \n",
    "    X_torch.requires_grad = True\n",
    "    \n",
    "    # Storage for results\n",
    "    eigenvalues = []\n",
    "    eigenfunctions = []\n",
    "    all_models = []\n",
    "    loss_history = {'total': [], 'eigenvalue': [], 'normalization': [], 'orthogonality': []}\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Point cloud size: {X.shape[0]} points\")\n",
    "    print(f\"Matrix format: L is {type(L).__name__}, M is {type(M).__name__}\")\n",
    "    \n",
    "    # ============ ITERATIVE EIGENFUNCTION DISCOVERY ============\n",
    "    for eig_idx in range(num_eigenfunctions):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Finding eigenfunction {eig_idx + 1}/{num_eigenfunctions}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize network with progressively larger eigenvalue guess\n",
    "        # For Laplacian: smallest eigenvalue is 0\n",
    "        if eig_idx == 0:\n",
    "            initial_eigenvalue = 0.0  # First eigenvalue for Laplacian\n",
    "        elif eig_idx > 0:\n",
    "            # Use previous eigenvalue as lower bound + small increment\n",
    "            initial_eigenvalue = eigenvalues[-1] + 0.15\n",
    "        else:\n",
    "            initial_eigenvalue = eig_idx * 0.2\n",
    "        \n",
    "        model = EigenfunctionNN(hidden_dim=hidden_dim, input_dim=X.shape[1], \n",
    "                               initial_eigenvalue=initial_eigenvalue).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.999, 0.9999))\n",
    "        \n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        loss_slope_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            u, eigenvalue = model(X_torch)\n",
    "            \n",
    "            # Compute losses\n",
    "            eig_loss, Lu, Mu = compute_eigenvalue_loss(u, eigenvalue, L, M, X_torch, device)\n",
    "            norm_loss = compute_normalization_loss(u, M, device)\n",
    "            ortho_loss = compute_orthogonality_loss(u, eigenfunctions, M, device)\n",
    "            \n",
    "            # Total loss with weighting\n",
    "            total_loss = eig_loss + norm_loss + ortho_weight * ortho_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss slope for convergence detection\n",
    "            loss_slope_history.append(eig_loss.item())\n",
    "            if len(loss_slope_history) > 1000:\n",
    "                loss_slope_history.pop(0)\n",
    "                slope = np.mean(np.diff(loss_slope_history))\n",
    "            else:\n",
    "                slope = 0.0\n",
    "            \n",
    "            # Save best model\n",
    "            if eig_loss.item() < best_loss:\n",
    "                best_loss = eig_loss.item()\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch:5d} | λ={eigenvalue.item():.6f} | \"\n",
    "                      f\"Eig loss={eig_loss.item():.2e} | Norm loss={norm_loss.item():.2e} | \"\n",
    "                      f\"Ortho loss={ortho_loss.item():.2e} | Slope={slope:.2e}\")\n",
    "            \n",
    "            # Store history\n",
    "            loss_history['total'].append(total_loss.item())\n",
    "            loss_history['eigenvalue'].append(eig_loss.item())\n",
    "            loss_history['normalization'].append(norm_loss.item())\n",
    "            loss_history['orthogonality'].append(ortho_loss.item())\n",
    "            \n",
    "            # Check for convergence and reinitialize if stuck\n",
    "            if epoch > 5000 and len(loss_slope_history) == 1000:\n",
    "                if abs(slope) < convergence_threshold:\n",
    "                    print(f\"Converged at epoch {epoch}!\")\n",
    "                    break\n",
    "        \n",
    "        # Store results for this eigenfunction\n",
    "        with torch.no_grad():\n",
    "            u_final, eigenvalue_final = best_model(X_torch)\n",
    "            eigenvalues.append(eigenvalue_final.item())\n",
    "            eigenfunctions.append(u_final.detach())\n",
    "            all_models.append(best_model)\n",
    "        \n",
    "        print(f\"\\nFound eigenvalue: λ_{eig_idx} = {eigenvalue_final.item():.6f}\")\n",
    "    \n",
    "    return eigenvalues, eigenfunctions, all_models, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f6ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629ded8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00123591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1ccd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa256225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc9103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1b2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facc9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "THIS ONE ACTUALLY WORKS OK\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ============ NETWORK ARCHITECTURE ============\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"Sine activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "class EigenfunctionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to learn eigenfunctions on point clouds.\n",
    "    Input: 3D coordinates (x, y, z)\n",
    "    Output: eigenfunction value u(x,y,z) and eigenvalue λ\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, input_dim=3, initial_eigenvalue=0.0):\n",
    "        super().__init__()\n",
    "        self.activation = Sin()\n",
    "        \n",
    "        # Learnable eigenvalue with better initialization\n",
    "        self.eigenvalue_layer = nn.Linear(1, 1, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.eigenvalue_layer.weight.fill_(initial_eigenvalue)\n",
    "        \n",
    "        # Network layers - concatenate eigenvalue at each layer\n",
    "        self.fc1 = nn.Linear(input_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc2 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc3 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc4 = nn.Linear(hidden_dim + 1, 1)  # +1 for eigenvalue\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, 3) point cloud coordinates\n",
    "        Returns:\n",
    "            u: (N, 1) eigenfunction values\n",
    "            eigenvalue: scalar learnable eigenvalue\n",
    "        \"\"\"\n",
    "        # Learn eigenvalue and broadcast to match batch size\n",
    "        eigenvalue = torch.abs(self.eigenvalue_layer(torch.ones(1, 1).to(x.device)))\n",
    "        eigenvalue_expanded = eigenvalue.expand(x.shape[0], 1)  # (N, 1)\n",
    "        \n",
    "        # Forward pass - concatenate eigenvalue at each layer\n",
    "        h = torch.cat([x, eigenvalue_expanded], dim=1)  # (N, input_dim+1)\n",
    "        h = self.activation(self.fc1(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc2(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc3(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        u = self.fc4(h)\n",
    "        \n",
    "        return u, eigenvalue\n",
    "\n",
    "\n",
    "# ============ LOSS COMPUTATION ============\n",
    "def compute_eigenvalue_loss(u, eigenvalue, L_torch, M_torch):\n",
    "    \"\"\"\n",
    "    Compute residual for Lu = λMu using discrete operators.\n",
    "    Returns:\n",
    "        loss: MSE of residual ||Lu - λMu||²\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()  # (N,)\n",
    "    \n",
    "    # Compute Lu and λMu\n",
    "    Lu = torch.sparse.mm(L_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    residual = Lu - eigenvalue * Mu\n",
    "    \n",
    "    return torch.mean(residual ** 2), Lu, Mu\n",
    "\n",
    "\n",
    "def compute_normalization_loss(u, M_torch):\n",
    "    \"\"\"\n",
    "    Enforce u^T M u = 1 (mass-matrix normalization).\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()    \n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    norm_squared = torch.dot(u_flat, Mu)\n",
    "\n",
    "    return (norm_squared - 1.0) ** 2\n",
    "\n",
    "\n",
    "def compute_orthogonality_loss(u, previous_eigenfunctions, M_torch):\n",
    "    \"\"\"\n",
    "    Enforce u ⊥ u_i for all previously found eigenfunctions.\n",
    "    Uses M-orthogonality: u^T M u_i = 0\n",
    "    \"\"\"\n",
    "    if not previous_eigenfunctions:\n",
    "        return torch.tensor(0.0, device=M_torch.device)\n",
    "    \n",
    "    u_flat = u.squeeze()\n",
    "    ortho_loss = torch.tensor(0.0, device=M_torch.device)\n",
    "    \n",
    "    \n",
    "    for u_prev in previous_eigenfunctions:\n",
    "        u_prev_flat = u_prev.squeeze()\n",
    "        # Compute u^T M u_prev\n",
    "        Mu_prev = torch.sparse.mm(M_torch, u_prev_flat.unsqueeze(1)).squeeze()\n",
    "        overlap = torch.dot(u_flat, Mu_prev)\n",
    "        ortho_loss += overlap ** 2\n",
    "    \n",
    "    return ortho_loss\n",
    "\n",
    "\n",
    "# ============ UTILITY FUNCTIONS ============\n",
    "def sparse_to_torch(sparse_matrix, device):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    # Handle any scipy sparse format\n",
    "    if sparse.issparse(sparse_matrix):\n",
    "        coo = sparse_matrix.tocoo()\n",
    "    else:\n",
    "        raise ValueError(f\"Expected scipy sparse matrix, got {type(sparse_matrix)}\")\n",
    "    \n",
    "    indices = torch.LongTensor(np.vstack((coo.row, coo.col)))\n",
    "    values = torch.FloatTensor(coo.data)\n",
    "    shape = coo.shape\n",
    "    return torch.sparse_coo_tensor(indices, values, shape).to(device)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    \"\"\"Reinitialize network weights for finding next eigenfunction.\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# ============ MAIN TRAINING FUNCTION ============\n",
    "def train_eigenvalue_pinn(X, L, M, hidden_dim=64, epochs=20000, \n",
    "                          lr=1e-3, num_eigenfunctions=5, \n",
    "                          convergence_threshold=1e-7,\n",
    "                          ortho_weight=1.0):\n",
    "    \"\"\"print(f\"The true Eigenvalues are: {np.array2string(eigvals[:10], formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "print(f\"The predicted Eigenvalues are: {np.array2string(np.array(eigenvalues), formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "    Train PINN to solve Lu = λMu eigenvalue problem.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, 3) point cloud coordinates\n",
    "        L: (N, N) Laplacian matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        M: (N, N) Mass matrix (scipy sparse: csr, csc, dia, etc.)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        epochs: Training epochs per eigenfunction\n",
    "        lr: Learning rate\n",
    "        num_eigenfunctions: Number of eigenfunctions to find\n",
    "        convergence_threshold: Threshold for detecting convergence\n",
    "        ortho_weight: Weight for orthogonality loss (increase if eigenfunctions overlap)\n",
    "    \n",
    "    Returns:\n",
    "        eigenvalues: List of found eigenvalues\n",
    "        eigenfunctions: List of (N, 1) eigenfunctions\n",
    "        loss_history: Training loss history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare inputs\n",
    "    X_torch = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    X_torch.requires_grad = True\n",
    "\n",
    "    # Pre-convert sparse matrices\n",
    "    L_torch = sparse_to_torch(L, device) if sparse.issparse(L) else L.to(device)\n",
    "    M_torch = sparse_to_torch(M, device) if sparse.issparse(M) else M.to(device)\n",
    "    \n",
    "    # Storage for results\n",
    "    eigenvalues = []\n",
    "    eigenfunctions = []\n",
    "    all_models = []\n",
    "    loss_history = {'total': [], 'eig': [], 'norm': [], 'ortho': []}\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Point cloud size: {X.shape[0]} points\")\n",
    "    print(f\"Matrix format: L is {type(L).__name__}, M is {type(M).__name__}\")\n",
    "    \n",
    "    # ============ ITERATIVE EIGENFUNCTION DISCOVERY ============\n",
    "    for eig_idx in range(num_eigenfunctions):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Finding eigenfunction {eig_idx + 1}/{num_eigenfunctions}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize network with progressively larger eigenvalue guess\n",
    "        # For Laplacian: smallest eigenvalue is 0\n",
    "        if eig_idx == 0:\n",
    "            initial_eigenvalue = 0.0  # First eigenvalue for Laplacian\n",
    "        elif eig_idx > 0:\n",
    "            # Use previous eigenvalue as lower bound + small increment\n",
    "            initial_eigenvalue = eigenvalues[-1] + 0.15\n",
    "        else:\n",
    "            initial_eigenvalue = eig_idx * 0.2\n",
    "        \n",
    "        model = EigenfunctionNN(hidden_dim=hidden_dim, input_dim=X.shape[1], \n",
    "                               initial_eigenvalue=initial_eigenvalue).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.999, 0.9999))\n",
    "        \n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        ema_slope = 1.0\n",
    "        prev_loss = None\n",
    "        \n",
    "        for epoch in trange(epochs, desc=f\"Eigen {eig_idx+1}/{num_eigenfunctions}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            u, eigenvalue = model(X_torch)\n",
    "            \n",
    "            # Compute losses\n",
    "            eig_loss, _, _ = compute_eigenvalue_loss(u, eigenvalue, L_torch, M_torch)\n",
    "            norm_loss = compute_normalization_loss(u, M_torch)\n",
    "            ortho_loss = compute_orthogonality_loss(u, eigenfunctions, M_torch)\n",
    "            \n",
    "            # Total loss with weighting\n",
    "            total_loss = eig_loss + norm_loss + ortho_weight * ortho_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Convergence tracking\n",
    "            if prev_loss is not None:\n",
    "                ema_slope = 0.75 * ema_slope + 0.25 * abs(prev_loss - eig_loss.item())\n",
    "            prev_loss = eig_loss.item()\n",
    "\n",
    "            if ema_slope < convergence_threshold and epoch > 2000:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            # Save best\n",
    "            if eig_loss.item() < best_loss:\n",
    "                best_loss = eig_loss.item()\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            # Logging every 500 epochs\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch:5d} | λ={eigenvalue.item():.6f} | \"\n",
    "                      f\"Eig={eig_loss.item():.2e} | Norm={norm_loss.item():.2e} | \"\n",
    "                      f\"Ortho={ortho_loss.item():.2e} | EMA slope={ema_slope:.2e}\")\n",
    "\n",
    "            # Lightweight history\n",
    "            if epoch % 100 == 0:\n",
    "                loss_history['total'].append(total_loss.item())\n",
    "                loss_history['eig'].append(eig_loss.item())\n",
    "                loss_history['norm'].append(norm_loss.item())\n",
    "                loss_history['ortho'].append(ortho_loss.item())\n",
    "\n",
    "        # Store results\n",
    "        with torch.no_grad():\n",
    "            u_final, eigenvalue_final = best_model(X_torch)\n",
    "            eigenvalues.append(eigenvalue_final.item())\n",
    "            eigenfunctions.append(u_final.detach())\n",
    "            all_models.append(best_model)\n",
    "\n",
    "        print(f\"\\nFound eigenvalue: λ_{eig_idx} = {eigenvalue_final.item():.6f}\")\n",
    "    \n",
    "    return eigenvalues, eigenfunctions, all_models, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeb5bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "Training on device: cuda\n",
      "Point cloud size: 2503 points\n",
      "Matrix format: L is Tensor, M is Tensor\n",
      "\n",
      "============================================================\n",
      "Finding eigenfunction 1/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:   1%|          | 39/5000 [00:00<00:12, 384.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | λ=0.000000 | Eig=1.25e-05 | Norm=6.37e-02 | Ortho=0.00e+00 | EMA slope=1.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:  11%|█▏        | 572/5000 [00:01<00:09, 486.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | λ=0.000000 | Eig=2.59e-05 | Norm=2.79e-02 | Ortho=0.00e+00 | EMA slope=2.85e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:  22%|██▏       | 1089/5000 [00:02<00:08, 466.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | λ=0.000000 | Eig=1.26e-04 | Norm=1.49e-02 | Ortho=0.00e+00 | EMA slope=4.47e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:  31%|███       | 1560/5000 [00:03<00:07, 467.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | λ=0.000000 | Eig=2.07e-04 | Norm=8.62e-03 | Ortho=0.00e+00 | EMA slope=1.67e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:  42%|████▏     | 2078/5000 [00:04<00:06, 470.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | λ=0.000000 | Eig=2.48e-04 | Norm=7.20e-03 | Ortho=0.00e+00 | EMA slope=3.23e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 1/5:  48%|████▊     | 2412/5000 [00:05<00:05, 461.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at epoch 2412\n",
      "\n",
      "Found eigenvalue: λ_0 = 0.000000\n",
      "\n",
      "============================================================\n",
      "Finding eigenfunction 2/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 2/5:   1%|          | 41/5000 [00:00<00:12, 408.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | λ=0.150000 | Eig=4.41e-06 | Norm=6.20e-02 | Ortho=2.99e-01 | EMA slope=1.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 2/5:  11%|█         | 553/5000 [00:01<00:10, 414.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | λ=0.117508 | Eig=1.02e-05 | Norm=3.66e-02 | Ortho=1.47e-02 | EMA slope=1.91e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 2/5:  21%|██▏       | 1064/5000 [00:02<00:09, 413.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | λ=0.088292 | Eig=1.13e-05 | Norm=2.52e-03 | Ortho=1.20e-02 | EMA slope=7.87e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 2/5:  32%|███▏      | 1582/5000 [00:03<00:07, 434.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | λ=0.069385 | Eig=1.14e-05 | Norm=8.90e-03 | Ortho=2.82e-03 | EMA slope=1.82e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 2/5:  40%|████      | 2001/5000 [00:04<00:07, 417.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | λ=0.054506 | Eig=8.34e-06 | Norm=2.79e-03 | Ortho=1.26e-03 | EMA slope=2.29e-08\n",
      "Converged at epoch 2001\n",
      "\n",
      "Found eigenvalue: λ_1 = 0.151000\n",
      "\n",
      "============================================================\n",
      "Finding eigenfunction 3/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 3/5:   1%|          | 41/5000 [00:00<00:12, 402.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | λ=0.301000 | Eig=6.79e-06 | Norm=3.22e-01 | Ortho=9.68e-01 | EMA slope=1.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 3/5:  11%|█         | 542/5000 [00:01<00:10, 405.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | λ=0.291753 | Eig=2.40e-05 | Norm=1.38e-07 | Ortho=3.79e-02 | EMA slope=2.10e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 3/5:  21%|██▏       | 1069/5000 [00:02<00:09, 399.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | λ=0.275056 | Eig=2.76e-05 | Norm=3.46e-05 | Ortho=1.09e-02 | EMA slope=4.56e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 3/5:  31%|███       | 1558/5000 [00:03<00:08, 404.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | λ=0.257851 | Eig=2.57e-05 | Norm=2.70e-06 | Ortho=4.31e-03 | EMA slope=3.82e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 3/5:  40%|████      | 2001/5000 [00:05<00:07, 398.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | λ=0.246277 | Eig=2.45e-05 | Norm=7.71e-05 | Ortho=4.13e-03 | EMA slope=3.26e-08\n",
      "Converged at epoch 2001\n",
      "\n",
      "Found eigenvalue: λ_2 = 0.302000\n",
      "\n",
      "============================================================\n",
      "Finding eigenfunction 4/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 4/5:   1%|          | 40/5000 [00:00<00:12, 393.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | λ=0.452000 | Eig=5.82e-06 | Norm=1.28e-01 | Ortho=1.44e+00 | EMA slope=1.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 4/5:  11%|█         | 554/5000 [00:01<00:11, 387.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | λ=0.416359 | Eig=1.37e-05 | Norm=1.11e-02 | Ortho=4.25e-03 | EMA slope=4.03e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 4/5:  21%|██▏       | 1065/5000 [00:02<00:10, 389.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | λ=0.397675 | Eig=1.51e-05 | Norm=3.43e-03 | Ortho=1.76e-02 | EMA slope=5.33e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 4/5:  31%|███▏      | 1572/5000 [00:04<00:08, 385.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | λ=0.383297 | Eig=1.30e-05 | Norm=1.70e-03 | Ortho=2.48e-03 | EMA slope=3.58e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 4/5:  40%|████      | 2001/5000 [00:05<00:07, 383.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | λ=0.373618 | Eig=1.14e-05 | Norm=1.98e-03 | Ortho=9.83e-04 | EMA slope=2.62e-08\n",
      "Converged at epoch 2001\n",
      "\n",
      "Found eigenvalue: λ_3 = 0.453000\n",
      "\n",
      "============================================================\n",
      "Finding eigenfunction 5/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 5/5:   1%|          | 32/5000 [00:00<00:15, 317.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | λ=0.603000 | Eig=1.10e-05 | Norm=2.24e-03 | Ortho=1.39e+00 | EMA slope=1.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 5/5:  11%|█▏        | 572/5000 [00:01<00:12, 354.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | λ=0.472570 | Eig=1.49e-05 | Norm=2.31e-02 | Ortho=3.09e-02 | EMA slope=2.97e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 5/5:  21%|██        | 1049/5000 [00:02<00:10, 362.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | λ=0.392971 | Eig=1.53e-05 | Norm=8.17e-03 | Ortho=2.26e-02 | EMA slope=1.52e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 5/5:  31%|███       | 1541/5000 [00:04<00:09, 376.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | λ=0.339904 | Eig=1.57e-05 | Norm=9.32e-03 | Ortho=2.43e-02 | EMA slope=5.59e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eigen 5/5:  40%|████      | 2001/5000 [00:05<00:08, 361.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | λ=0.303474 | Eig=1.47e-05 | Norm=8.37e-03 | Ortho=1.67e-02 | EMA slope=1.95e-08\n",
      "Converged at epoch 2001\n",
      "\n",
      "Found eigenvalue: λ_4 = 0.599559\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "λ_0 = 0.000000\n",
      "λ_1 = 0.151000\n",
      "λ_2 = 0.302000\n",
      "λ_3 = 0.453000\n",
      "λ_4 = 0.599559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "X = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = X, connectivity = m.connectivity)\n",
    "\n",
    "L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K_igl, M_igl = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = eigh(K_igl,M_igl)\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K_ = torch.from_numpy(K_igl).float().to(device)\n",
    "M_ = torch.from_numpy(M_igl).float().to(device)\n",
    "X_ = torch.from_numpy(m.verts).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "eigenvalues, eigenfunctions, models, history = train_eigenvalue_pinn(\n",
    "    X_, K_, M_, \n",
    "    hidden_dim=128, \n",
    "    epochs=5000, \n",
    "    lr=1e-3, \n",
    "    num_eigenfunctions=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for i, lam in enumerate(eigenvalues):\n",
    "    print(f\"λ_{i} = {lam:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fac0b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.14900000393390656,\n",
       " 0.23257632553577423,\n",
       " 0.3801209032535553,\n",
       " 0.5295212864875793,\n",
       " 0.6682814955711365,\n",
       " 0.8164870142936707,\n",
       " 0.9114035964012146,\n",
       " 1.0432565212249756,\n",
       " 1.1872462034225464,\n",
       " 1.3285917043685913,\n",
       " 1.4192572832107544,\n",
       " 1.479326844215393,\n",
       " 1.6303268671035767,\n",
       " 1.7489262819290161,\n",
       " 1.886279582977295,\n",
       " 2.008985757827759,\n",
       " 2.1078410148620605,\n",
       " 2.2441799640655518,\n",
       " 2.305201768875122,\n",
       " 2.432590961456299,\n",
       " 2.5144846439361572,\n",
       " 2.6198272705078125,\n",
       " 2.655687093734741,\n",
       " 2.7635750770568848,\n",
       " 2.839317560195923,\n",
       " 2.986765146255493,\n",
       " 3.1014328002929688,\n",
       " 3.231534481048584,\n",
       " 3.28542160987854,\n",
       " 3.336639404296875,\n",
       " 3.4122958183288574,\n",
       " 3.5568699836730957,\n",
       " 3.69858455657959,\n",
       " 3.761384963989258,\n",
       " 3.8405449390411377,\n",
       " 3.943140745162964,\n",
       " 4.037494659423828,\n",
       " 4.180729866027832,\n",
       " 4.321310520172119,\n",
       " 4.421263694763184,\n",
       " 4.499535083770752,\n",
       " 4.598731517791748,\n",
       " 4.745480060577393,\n",
       " 4.890344619750977,\n",
       " 5.039665222167969,\n",
       " 5.177450656890869,\n",
       " 5.362771034240723,\n",
       " 5.5051445960998535,\n",
       " 5.676225662231445]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b50ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04958a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e4a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b3e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef45f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29393ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb97726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8b40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccdb31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "THIS ONE ACTUALLY WORKS OK\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ============ NETWORK ARCHITECTURE ============\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"Sine activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "class EigenfunctionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to learn eigenfunctions on point clouds.\n",
    "    Input: 3D coordinates (x, y, z)\n",
    "    Output: eigenfunction value u(x,y,z) and eigenvalue λ\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, input_dim=3, initial_eigenvalue=0.0):\n",
    "        super().__init__()\n",
    "        self.activation = Sin()\n",
    "        \n",
    "        # Learnable eigenvalue with better initialization\n",
    "        self.eigenvalue_layer = nn.Linear(1, 1, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.eigenvalue_layer.weight.fill_(initial_eigenvalue)\n",
    "        \n",
    "        # Network layers - concatenate eigenvalue at each layer\n",
    "        self.fc1 = nn.Linear(input_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc2 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc3 = nn.Linear(hidden_dim + 1, hidden_dim)  # +1 for eigenvalue\n",
    "        self.fc4 = nn.Linear(hidden_dim + 1, 1)  # +1 for eigenvalue\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, 3) point cloud coordinates\n",
    "        Returns:\n",
    "            u: (N, 1) eigenfunction values\n",
    "            eigenvalue: scalar learnable eigenvalue\n",
    "        \"\"\"\n",
    "        # Learn eigenvalue and broadcast to match batch size\n",
    "        eigenvalue = torch.abs(self.eigenvalue_layer(torch.ones(1, 1).to(x.device)))\n",
    "        eigenvalue_expanded = eigenvalue.expand(x.shape[0], 1)  # (N, 1)\n",
    "        \n",
    "        # Forward pass - concatenate eigenvalue at each layer\n",
    "        h = torch.cat([x, eigenvalue_expanded], dim=1)  # (N, input_dim+1)\n",
    "        h = self.activation(self.fc1(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc2(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        h = self.activation(self.fc3(h))\n",
    "        \n",
    "        h = torch.cat([h, eigenvalue_expanded], dim=1)  # (N, hidden_dim+1)\n",
    "        u = self.fc4(h)\n",
    "        \n",
    "        return u, eigenvalue\n",
    "\n",
    "\n",
    "# ============ LOSS COMPUTATION ============\n",
    "def compute_eigenvalue_loss(u, eigenvalue, L_torch, M_torch):\n",
    "    \"\"\"\n",
    "    Compute residual for Lu = λMu using discrete operators.\n",
    "    Returns:\n",
    "        loss: MSE of residual ||Lu - λMu||²\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()  # (N,)\n",
    "    \n",
    "    # Compute Lu and λMu\n",
    "    Lu = torch.sparse.mm(L_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    residual = Lu - eigenvalue * Mu\n",
    "    \n",
    "    return torch.mean(residual ** 2), Lu, Mu\n",
    "\n",
    "\n",
    "def compute_normalization_loss(u, M_torch):\n",
    "    \"\"\"\n",
    "    Enforce u^T M u = 1 (mass-matrix normalization).\n",
    "    \"\"\"\n",
    "    u_flat = u.squeeze()    \n",
    "    Mu = torch.sparse.mm(M_torch, u_flat.unsqueeze(1)).squeeze()\n",
    "    norm_squared = torch.dot(u_flat, Mu)\n",
    "\n",
    "    return (norm_squared - 1.0) ** 2\n",
    "\n",
    "\n",
    "def compute_orthogonality_loss(u, previous_eigenfunctions, M_torch):\n",
    "    \"\"\"\n",
    "    Enforce u ⊥ u_i for all previously found eigenfunctions.\n",
    "    Uses M-orthogonality: u^T M u_i = 0\n",
    "    \"\"\"\n",
    "    if not previous_eigenfunctions:\n",
    "        return torch.tensor(0.0, device=M_torch.device)\n",
    "    \n",
    "    u_flat = u.squeeze()\n",
    "    ortho_loss = torch.tensor(0.0, device=M_torch.device)\n",
    "    \n",
    "    \n",
    "    for u_prev in previous_eigenfunctions:\n",
    "        u_prev_flat = u_prev.squeeze()\n",
    "        # Compute u^T M u_prev\n",
    "        Mu_prev = torch.sparse.mm(M_torch, u_prev_flat.unsqueeze(1)).squeeze()\n",
    "        overlap = torch.dot(u_flat, Mu_prev)\n",
    "        ortho_loss += overlap ** 2\n",
    "    \n",
    "    return ortho_loss\n",
    "\n",
    "\n",
    "# ============ UTILITY FUNCTIONS ============\n",
    "def sparse_to_torch(sparse_matrix, device):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    # Handle any scipy sparse format\n",
    "    if sparse.issparse(sparse_matrix):\n",
    "        coo = sparse_matrix.tocoo()\n",
    "    else:\n",
    "        raise ValueError(f\"Expected scipy sparse matrix, got {type(sparse_matrix)}\")\n",
    "    \n",
    "    indices = torch.LongTensor(np.vstack((coo.row, coo.col)))\n",
    "    values = torch.FloatTensor(coo.data)\n",
    "    shape = coo.shape\n",
    "    return torch.sparse_coo_tensor(indices, values, shape).to(device)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    \"\"\"Reinitialize network weights for finding next eigenfunction.\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# ============ MAIN TRAINING FUNCTION ============\n",
    "def train_eigenvalue_pinn_adaptive(\n",
    "    X, L, M,\n",
    "    hidden_dim=64,\n",
    "    epochs=20000,\n",
    "    lr=1e-3,\n",
    "    num_eigenfunctions=5,\n",
    "    convergence_threshold=1e-7,\n",
    "    ortho_weight=25.0,\n",
    "    minibatch_size=None,\n",
    "    perturbation_factor=0.002\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaptive PINN for solving Lu = λMu using a single network:\n",
    "      - Stores converged eigenfunctions before reinitialization\n",
    "      - Adaptive in-loop reinitialization (like Schrödinger PINN)\n",
    "      - Point perturbation + minibatching\n",
    "      - Orthogonality enforcement\n",
    "    \"\"\"\n",
    "    # ======== Setup ========\n",
    "    X_torch = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    N = X_torch.shape[0]\n",
    "    L_torch = sparse_to_torch(L, device) if sparse.issparse(L) else L.to(device)\n",
    "    M_torch = sparse_to_torch(M, device) if sparse.issparse(M) else M.to(device)\n",
    "    domain_scale = (X_torch.max(0).values - X_torch.min(0).values).mean()\n",
    "\n",
    "    if minibatch_size is None or minibatch_size > N:\n",
    "        minibatch_size = N\n",
    "    num_batches = max(1, N // minibatch_size)\n",
    "\n",
    "    # ======== Initialize network ========\n",
    "    model = EigenfunctionNN(hidden_dim=hidden_dim, input_dim=X.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.999, 0.9999))\n",
    "\n",
    "    # ======== Storage ========\n",
    "    eigenfunctions = []\n",
    "    eigenvalues = []\n",
    "    all_models = []\n",
    "    loss_history = {'total': [], 'eig': [], 'norm': [], 'ortho': []}\n",
    "\n",
    "    eig_counter = 0  # Tracks how many eigenfunctions have been found\n",
    "    ema_slope, prev_loss = 1.0, None\n",
    "\n",
    "    print(f\"Training on device: {device}, Total points: {N}, Minibatch size: {minibatch_size}\")\n",
    "\n",
    "    # ======== Training Loop ========\n",
    "    for epoch in range(epochs):\n",
    "        # ======== Perturb and shuffle points ========\n",
    "        noise = perturbation_factor * domain_scale * torch.randn_like(X_torch)\n",
    "        perturbed_points = (X_torch + noise).clamp(X_torch.min(0).values, X_torch.max(0).values)\n",
    "        shuffled_points = perturbed_points[torch.randperm(N)]\n",
    "        shuffled_points.requires_grad = True\n",
    "\n",
    "        total_epoch_loss = 0.0\n",
    "\n",
    "        # ======== Minibatch ========\n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * minibatch_size\n",
    "            end = start + minibatch_size\n",
    "            x_batch = shuffled_points[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u, _ = model(x_batch)\n",
    "            u = u.view(-1, 1)\n",
    "\n",
    "            # Rayleigh quotient\n",
    "            Lu = torch.sparse.mm(L_torch, u)\n",
    "            Mu = torch.sparse.mm(M_torch, u)\n",
    "            numerator = torch.sum(u * Lu)\n",
    "            denominator = torch.sum(u * Mu) + 1e-8\n",
    "            eigenvalue = numerator / denominator\n",
    "\n",
    "            # Normalized residual\n",
    "            eig_residual = Lu - eigenvalue * Mu\n",
    "            eig_loss = torch.mean(eig_residual ** 2) / (torch.mean(u ** 2) + 1e-8)\n",
    "\n",
    "            # Normalization\n",
    "            norm_loss = (torch.sum(u * Mu) - 1.0) ** 2\n",
    "\n",
    "            # Orthogonality\n",
    "            ortho_loss = compute_orthogonality_loss(u, eigenfunctions, M_torch)\n",
    "\n",
    "            total_loss = eig_loss + norm_loss + ortho_weight * ortho_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_epoch_loss += total_loss.item()\n",
    "\n",
    "        # ======== EMA slope convergence detection ========\n",
    "        avg_loss = total_epoch_loss / num_batches\n",
    "        if prev_loss is not None:\n",
    "            ema_slope = 0.75 * ema_slope + 0.25 * abs(prev_loss - avg_loss)\n",
    "        prev_loss = avg_loss\n",
    "\n",
    "        # ======== Adaptive reinitialization ========\n",
    "        trigger_reweight = (ema_slope < convergence_threshold and ema_slope > 0 and epoch > 2000)\n",
    "\n",
    "        if trigger_reweight:\n",
    "            # 1️⃣ Store converged eigenfunction first\n",
    "            with torch.no_grad():\n",
    "                u_full, _ = model(X_torch)\n",
    "                u_full = u_full.view(-1, 1)\n",
    "                Lu_full = torch.sparse.mm(L_torch, u_full)\n",
    "                Mu_full = torch.sparse.mm(M_torch, u_full)\n",
    "                λ_full = torch.sum(u_full * Lu_full) / (torch.sum(u_full * Mu_full) + 1e-8)\n",
    "\n",
    "                eigenvalues.append(λ_full.item())\n",
    "                eigenfunctions.append(u_full.detach())\n",
    "                all_models.append(copy.deepcopy(model))\n",
    "\n",
    "            # 2️⃣ Reinitialize weights for next eigenfunction\n",
    "            model.apply(initialize_weights)\n",
    "            eig_counter += 1\n",
    "            print(f\"Epoch {epoch} [Adaptive Reweight] | Eigenfunctions found: {eig_counter}\")\n",
    "\n",
    "            # 3️⃣ Stop training if enough eigenfunctions found\n",
    "            if eig_counter >= num_eigenfunctions:\n",
    "                print(f\"All {num_eigenfunctions} eigenfunctions found. Stopping training.\")\n",
    "                break\n",
    "\n",
    "        # ======== Logging ========\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch:5d} | λ ≈ {eigenvalue.item():.6f} | Loss={avg_loss:.2e} | EMA slope={ema_slope:.2e}\")\n",
    "        if epoch % 100 == 0:\n",
    "            loss_history['total'].append(avg_loss)\n",
    "            loss_history['eig'].append(eig_loss.item())\n",
    "            loss_history['norm'].append(norm_loss.item())\n",
    "            loss_history['ortho'].append(ortho_loss.item())\n",
    "\n",
    "    # ======== Return results ========\n",
    "    return eigenvalues, eigenfunctions, all_models, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c90a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0054d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155787cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "Training on device: cuda\n",
      "Total points: 2503 | Minibatch size: 2503 | Batches per epoch: 1\n",
      "\n",
      "============================================================\n",
      "Training eigenfunction 1/10\n",
      "============================================================\n",
      "Epoch     0 | λ ≈ 328.459198 | Loss=3.16e+01 | EMA slope=1.00e+00\n",
      "Epoch   500 | λ ≈ 345.488892 | Loss=3.38e+01 | EMA slope=4.33e+00\n",
      "Epoch  1000 | λ ≈ 389.421600 | Loss=3.19e+01 | EMA slope=2.39e+00\n",
      "Epoch  1500 | λ ≈ 379.023010 | Loss=2.97e+01 | EMA slope=1.48e+00\n",
      "Epoch  2000 | λ ≈ 363.382111 | Loss=2.89e+01 | EMA slope=1.38e+00\n",
      "Epoch  2500 | λ ≈ 377.447205 | Loss=3.11e+01 | EMA slope=2.78e+00\n",
      "Epoch  3000 | λ ≈ 378.192871 | Loss=3.01e+01 | EMA slope=2.37e+00\n",
      "Epoch  3500 | λ ≈ 394.451294 | Loss=2.85e+01 | EMA slope=1.88e+00\n",
      "Epoch  4000 | λ ≈ 388.962158 | Loss=2.60e+01 | EMA slope=3.04e+00\n",
      "Epoch  4500 | λ ≈ 408.374176 | Loss=2.87e+01 | EMA slope=1.60e+00\n",
      "→ Found eigenvalue λ_0 = 267.581848\n",
      "\n",
      "============================================================\n",
      "Training eigenfunction 2/10\n",
      "============================================================\n",
      "Epoch     0 | λ ≈ 313.996216 | Loss=2.78e+01 | EMA slope=1.00e+00\n",
      "Epoch   500 | λ ≈ 395.967621 | Loss=5.01e+01 | EMA slope=2.11e+00\n",
      "Epoch  1000 | λ ≈ 394.629272 | Loss=3.52e+01 | EMA slope=2.67e+00\n",
      "Epoch  1500 | λ ≈ 382.064392 | Loss=3.10e+01 | EMA slope=3.16e+00\n",
      "Epoch  2000 | λ ≈ 374.302734 | Loss=2.97e+01 | EMA slope=1.15e+00\n",
      "Epoch  2500 | λ ≈ 367.648315 | Loss=2.56e+01 | EMA slope=2.15e+00\n",
      "Epoch  3000 | λ ≈ 372.353271 | Loss=3.01e+01 | EMA slope=3.24e+00\n",
      "Epoch  3500 | λ ≈ 377.919922 | Loss=2.73e+01 | EMA slope=1.22e+00\n",
      "Epoch  4000 | λ ≈ 377.097961 | Loss=2.66e+01 | EMA slope=3.49e+00\n",
      "Epoch  4500 | λ ≈ 382.368164 | Loss=2.68e+01 | EMA slope=3.54e+00\n",
      "→ Found eigenvalue λ_1 = 279.425842\n",
      "\n",
      "============================================================\n",
      "Training eigenfunction 3/10\n",
      "============================================================\n",
      "Epoch     0 | λ ≈ 7.418307 | Loss=1.93e+00 | EMA slope=1.00e+00\n",
      "Epoch   500 | λ ≈ 377.258850 | Loss=3.70e+01 | EMA slope=1.18e+00\n",
      "Epoch  1000 | λ ≈ 385.326935 | Loss=3.17e+01 | EMA slope=2.20e+00\n",
      "Epoch  1500 | λ ≈ 363.664703 | Loss=2.71e+01 | EMA slope=2.77e+00\n",
      "Epoch  2000 | λ ≈ 381.913239 | Loss=2.73e+01 | EMA slope=1.70e+00\n",
      "Epoch  2500 | λ ≈ 382.275513 | Loss=2.91e+01 | EMA slope=1.22e+00\n",
      "Epoch  3000 | λ ≈ 386.453339 | Loss=2.93e+01 | EMA slope=1.70e+00\n",
      "Epoch  3500 | λ ≈ 373.536743 | Loss=2.62e+01 | EMA slope=2.01e+00\n",
      "Epoch  4000 | λ ≈ 397.390747 | Loss=2.78e+01 | EMA slope=3.89e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m M_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(M_igl)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m X_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(m\u001b[38;5;241m.\u001b[39mverts)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m eigenvalues, eigenfunctions, models, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eigenvalue_pinn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_eigenfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESULTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 231\u001b[0m, in \u001b[0;36mtrain_eigenvalue_pinn\u001b[0;34m(X, L, M, hidden_dim, epochs, lr, num_eigenfunctions, convergence_threshold, ortho_weight, minibatch_size, perturbation_factor)\u001b[0m\n\u001b[1;32m    229\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m eig_loss \u001b[38;5;241m+\u001b[39m norm_loss \u001b[38;5;241m+\u001b[39m ortho_weight \u001b[38;5;241m*\u001b[39m ortho_loss\n\u001b[1;32m    230\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 231\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     total_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# ======== Convergence and logging ========\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/optimizer.py:149\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/adam.py:949\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 949\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/deltapinns/lib/python3.10/site-packages/torch/optim/adam.py:689\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    682\u001b[0m             device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    683\u001b[0m                 device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay\n\u001b[1;32m    684\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "X = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = X, connectivity = m.connectivity)\n",
    "\n",
    "L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K_igl, M_igl = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = eigh(K_igl,M_igl)\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K_ = torch.from_numpy(K_igl).float().to(device)\n",
    "M_ = torch.from_numpy(M_igl).float().to(device)\n",
    "X_ = torch.from_numpy(m.verts).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "eigenvalues, eigenfunctions, models, history = train_eigenvalue_pinn(\n",
    "    X_, K_, M_, \n",
    "    hidden_dim=128, \n",
    "    epochs=5000, \n",
    "    lr=1e-2, \n",
    "    num_eigenfunctions=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for i, lam in enumerate(eigenvalues):\n",
    "    print(f\"λ_{i} = {lam:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e840c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true Eigenvalues are: [0.000 0.160 0.425 0.438 0.538 0.612 0.896 1.274 1.496 1.643]\n",
      "The pred Eigenvalues are: [0.000 0.015 0.043 0.151 0.383 0.386 0.492 0.387 0.341 0.332]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The true Eigenvalues are: {np.array2string(eigvals[:10], formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "print(f\"The pred Eigenvalues are: {np.array2string(np.array(eigenvalues), formatter={'float': lambda x: f'{x:.3f}'})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The true Eigenvalues are: {np.array2string(eigvals[:10], formatter={'float': lambda x: f'{x:.3f}'})}\")\n",
    "print(f\"The pred Eigenvalues are: {np.array2string(np.array(eigenvalues), formatter={'float': lambda x: f'{x:.3f}'})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13713515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed1ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596499a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
