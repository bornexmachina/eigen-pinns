{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a5d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multigrid_gnn_multires_physics.py\n",
    "\"\"\"\n",
    "Physics-informed Multigrid + GNN eigen-refinement\n",
    "- Exact solve only on coarsest mesh\n",
    "- Multiresolution GNN with residual + orthonormality + projection loss\n",
    "- Coarse-to-fine prolongation only\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def farthest_point_sampling(points, n_samples, seed=None):\n",
    "    \"\"\"\n",
    "    Farthest Point Sampling (FPS) for nested hierarchical sampling.\n",
    "    \n",
    "    Args:\n",
    "        points: (N, 3) array of 3D points\n",
    "        n_samples: number of points to sample\n",
    "        seed: random seed for initial point selection\n",
    "    \n",
    "    Returns:\n",
    "        indices: (n_samples,) array of selected point indices\n",
    "    \"\"\"\n",
    "    n_points = points.shape[0]\n",
    "    if n_samples >= n_points:\n",
    "        return np.arange(n_points)\n",
    "    \n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # Start with a random point\n",
    "    selected_indices = [rng.integers(0, n_points)]\n",
    "    distances = np.full(n_points, np.inf)\n",
    "    \n",
    "    for _ in range(n_samples - 1):\n",
    "        # Update distances to nearest selected point\n",
    "        last_selected = selected_indices[-1]\n",
    "        dists_to_last = np.linalg.norm(points - points[last_selected], axis=1)\n",
    "        distances = np.minimum(distances, dists_to_last)\n",
    "        \n",
    "        # Select point farthest from all selected points\n",
    "        farthest_idx = np.argmax(distances)\n",
    "        selected_indices.append(farthest_idx)\n",
    "    \n",
    "    return np.array(selected_indices)\n",
    "\n",
    "def voxel_downsampling(points, hierarchy_levels):\n",
    "    \"\"\"\n",
    "    Voxel grid downsampling for nested hierarchical sampling.\n",
    "    \n",
    "    Args:\n",
    "        points: (N, 3) array of 3D points\n",
    "        hierarchy_levels: list of target point counts for each level\n",
    "    \n",
    "    Returns:\n",
    "        indices_per_level: dict mapping level index to array of selected point indices\n",
    "    \"\"\"\n",
    "    n_points = points.shape[0]\n",
    "    \n",
    "    # Compute bounding box\n",
    "    min_bound = points.min(axis=0)\n",
    "    max_bound = points.max(axis=0)\n",
    "    extent = max_bound - min_bound\n",
    "    \n",
    "    indices_per_level = {}\n",
    "    \n",
    "    for level_idx, target_count in enumerate(hierarchy_levels):\n",
    "        if target_count >= n_points:\n",
    "            indices_per_level[level_idx] = np.arange(n_points)\n",
    "            continue\n",
    "        \n",
    "        # Estimate voxel size based on target count\n",
    "        # Volume per voxel ≈ total_volume / target_count\n",
    "        volume = np.prod(extent)\n",
    "        voxel_volume = volume / target_count\n",
    "        voxel_size = voxel_volume ** (1/3)\n",
    "        \n",
    "        # Compute voxel grid dimensions\n",
    "        grid_dims = np.ceil(extent / voxel_size).astype(int) + 1\n",
    "        \n",
    "        # Assign each point to a voxel\n",
    "        voxel_indices = ((points - min_bound) / voxel_size).astype(int)\n",
    "        voxel_indices = np.clip(voxel_indices, 0, grid_dims - 1)\n",
    "        \n",
    "        # Flatten voxel indices to 1D\n",
    "        voxel_ids = (voxel_indices[:, 0] * grid_dims[1] * grid_dims[2] + \n",
    "                     voxel_indices[:, 1] * grid_dims[2] + \n",
    "                     voxel_indices[:, 2])\n",
    "        \n",
    "        # Select one point per occupied voxel (closest to voxel center)\n",
    "        unique_voxels = np.unique(voxel_ids)\n",
    "        selected_indices = []\n",
    "        \n",
    "        for voxel_id in unique_voxels:\n",
    "            mask = (voxel_ids == voxel_id)\n",
    "            voxel_points_idx = np.where(mask)[0]\n",
    "            \n",
    "            if len(voxel_points_idx) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute voxel center\n",
    "            voxel_3d = np.array([\n",
    "                voxel_id // (grid_dims[1] * grid_dims[2]),\n",
    "                (voxel_id // grid_dims[2]) % grid_dims[1],\n",
    "                voxel_id % grid_dims[2]\n",
    "            ])\n",
    "            voxel_center = min_bound + (voxel_3d + 0.5) * voxel_size\n",
    "            \n",
    "            # Find point closest to voxel center\n",
    "            voxel_points = points[voxel_points_idx]\n",
    "            distances = np.linalg.norm(voxel_points - voxel_center, axis=1)\n",
    "            closest_idx = voxel_points_idx[np.argmin(distances)]\n",
    "            selected_indices.append(closest_idx)\n",
    "            \n",
    "            # Stop if we've reached target count\n",
    "            if len(selected_indices) >= target_count:\n",
    "                break\n",
    "        \n",
    "        indices_per_level[level_idx] = np.array(selected_indices[:target_count])\n",
    "    \n",
    "    return indices_per_level\n",
    "\n",
    "# ------------------------\n",
    "# Simple neighbor-mean corrector\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128,64,32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        n = x.shape[0]\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device).clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)\n",
    "        return self.net(h)\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid GNN solver\n",
    "# ------------------------\n",
    "class MultigridGNN:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Physics-informed GNN training\n",
    "    # ------------------------\n",
    "    def train_multiresolution(self, X_list, U_init_list, edge_index_list,\n",
    "                              epochs=1000, lr=1e-3, corr_scale=1e-2,\n",
    "                              w_res=10.0, w_orth=1.0, w_proj=0.001,\n",
    "                              grad_clip=1.0, weight_decay=1e-6, log_every=250):\n",
    "        device = self.device\n",
    "        n_modes = U_init_list[0].shape[1]\n",
    "\n",
    "        # Build torch tensors and resolution indicators\n",
    "        x_feats_all, U_all, edge_index_all = [], [], []\n",
    "        node_offset = 0\n",
    "        max_nodes = max([X.shape[0] for X in X_list])\n",
    "        for X, U_init, edge_index in zip(X_list, U_init_list, edge_index_list):\n",
    "            res_feat = np.full((X.shape[0], 1), X.shape[0]/max_nodes)\n",
    "            x_feats_all.append(np.hstack([X, U_init, res_feat]))\n",
    "            U_all.append(U_init)\n",
    "            edge_index_all.append(edge_index + node_offset)\n",
    "            node_offset += X.shape[0]\n",
    "\n",
    "        x_feats_all = torch.FloatTensor(np.vstack(x_feats_all)).to(device)\n",
    "        U_all_tensor = torch.FloatTensor(np.vstack(U_all)).to(device)\n",
    "        edge_index_all = torch.cat(edge_index_all, dim=1).to(device)\n",
    "\n",
    "        in_dim = x_feats_all.shape[1]\n",
    "        if self.model is None:\n",
    "            self.model = SimpleCorrector(in_dim, n_modes).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "\n",
    "        # ------------------------\n",
    "        # Precompute Laplacians per level\n",
    "        L_list, M_list = [], []\n",
    "        node_offset = 0\n",
    "        for X in X_list:\n",
    "            L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "            L_list.append(sp_to_torch_sparse(L).to(device))\n",
    "            M_list.append(sp_to_torch_sparse(M).to(device))\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            corr_raw = self.model(x_feats_all, edge_index_all)\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_all_tensor + corr\n",
    "\n",
    "            # Physics-informed loss\n",
    "            loss = 0.0\n",
    "            node_offset = 0\n",
    "            L_res_total = 0.0\n",
    "            L_orth_total = 0.0\n",
    "            L_mean_total = 0.0\n",
    "            \n",
    "            for i, (L_t, M_t, U_init) in enumerate(zip(L_list, M_list, U_init_list)):\n",
    "                n_nodes = U_init.shape[0]\n",
    "                U_level = U_pred[node_offset:node_offset+n_nodes]\n",
    "\n",
    "                # Rayleigh residual\n",
    "                Lu = torch.sparse.mm(L_t, U_level)\n",
    "                Mu = torch.sparse.mm(M_t, U_level)\n",
    "                num = torch.sum(U_level * Lu, dim=0)\n",
    "                den = torch.sum(U_level * Mu, dim=0) + 1e-12\n",
    "                lambdas = num / den\n",
    "                res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "                L_res = torch.mean(res**2)\n",
    "                \n",
    "                # Orthonormality\n",
    "                Gram = U_level.t() @ Mu\n",
    "                L_orth = torch.mean((Gram - torch.eye(n_modes, device=device))**2)\n",
    "\n",
    "                # Zero-mean constraint: 1.T @ M @ u = 0 for modes 1 onwards\n",
    "                ones = torch.ones(n_nodes, 1, device=device)\n",
    "                mean_constraint = ones.t() @ Mu[:, 1:]  # Shape: (1, n_modes-1)\n",
    "                L_mean = torch.mean(mean_constraint**2)\n",
    "\n",
    "                loss += w_res * L_res + w_orth * L_orth + w_proj * L_mean\n",
    "                node_offset += n_nodes\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.model.parameters()), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if ep % log_every == 0 or ep == epochs-1:\n",
    "                print(f\"Epoch {ep:4d}: Loss={loss.item():.6f} | Res={L_res.item():.6f} | Orth={L_orth.item():.6f} | Mean={L_mean.item():.6f}\")\n",
    "\n",
    "        return U_pred.detach().cpu().numpy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L, M):\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        L_t = sp_to_torch_sparse(L).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "    \n",
    "\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None):\n",
    "    \"\"\"Visualize mesh with vertices, optionally highlighting specific points.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    # Plot full mesh with transparency\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2], \n",
    "                    triangles=mesh.connectivity, alpha=0.3)\n",
    "    \n",
    "    # Highlight specific points if provided\n",
    "    if highlight_indices is not None:\n",
    "        highlighted_verts = mesh.verts[highlight_indices]\n",
    "        ax.scatter(highlighted_verts[:, 0], highlighted_verts[:, 1], highlighted_verts[:, 2], \n",
    "                   c='fuchsia', s=10, alpha=0.8, label=f'{len(highlight_indices)} selected points')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=130, azim=-90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hierarchy = [128, 512, 1024]  # final level is full mesh\n",
    "    k_neighbors = 4\n",
    "    epochs = 2_000\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridGNN.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    # Use Voxel Grid Downsampling for nested hierarchy\n",
    "    print(\"Computing voxel grid downsampling hierarchy...\")\n",
    "    indices_per_level = voxel_downsampling(X_full, hierarchy)\n",
    "    \n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        actual_count = len(indices_per_level[i])\n",
    "        print(f\"  Level {i}: {actual_count} points (voxel downsampling, target: {n_points})\")\n",
    "    \n",
    "\n",
    "    #for level_idx_vis, n_points in enumerate(hierarchy):\n",
    "    #    # Visualize selected points for this level\n",
    "    #    visualize_mesh(mesh, \n",
    "    #                  title=f'Level {level_idx_vis}: {len(indices_per_level[level_idx_vis])} Voxel Downsampled Points',\n",
    "    #                  highlight_indices=indices_per_level[level_idx_vis])\n",
    "    #print()\n",
    "\n",
    "    solver = MultigridGNN()\n",
    "\n",
    "    # ------------------------\n",
    "    # Level 0: exact coarse solve\n",
    "    # ------------------------\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(f\"\\nLEVEL 0: exact solve on {X0.shape[0]} points...\")\n",
    "    lambda0, U0, L0, M0 = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda0,6))\n",
    "\n",
    "    # ------------------------\n",
    "    # Coarse-to-fine prolongation\n",
    "    # ------------------------\n",
    "    U_prev = U0.copy()\n",
    "    X_list, U_init_list, edge_index_list = [X0], [U0], [solver.build_knn_graph(X0, k=k_neighbors)]\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level-1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "\n",
    "        P = solver.build_prolongation(Xc, Xf, k=1)\n",
    "        U_init = P @ U_prev\n",
    "        edge_index = solver.build_knn_graph(Xf, k=k_neighbors)\n",
    "\n",
    "        X_list.append(Xf)\n",
    "        U_init_list.append(U_init)\n",
    "        edge_index_list.append(edge_index)\n",
    "\n",
    "        U_prev = U_init.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Train physics-informed GNN\n",
    "    # ------------------------\n",
    "    print(\"\\nTraining physics-informed multiresolution GNN...\")\n",
    "    U_pred_all = solver.train_multiresolution(X_list, U_init_list, edge_index_list,\n",
    "                                              epochs=epochs)\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement per level\n",
    "    # ------------------------\n",
    "    node_offset = 0\n",
    "    for level, X in enumerate(X_list):\n",
    "        n_nodes = X.shape[0]\n",
    "        U_pred = U_pred_all[node_offset:node_offset+n_nodes]\n",
    "        node_offset += n_nodes\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals_refined, _ = solver.refine_eigenvectors(U_pred, L, M)\n",
    "        print(f\"Level {level} refined eigenvalues: {np.round(vals_refined,3)}\")\n",
    "\n",
    "    return U_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097100d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Hierarchy: [128, 512, 1024, 2503]\n",
      "Computing voxel grid downsampling hierarchy...\n",
      "  Level 0: 88 points (voxel downsampling, target: 128)\n",
      "  Level 1: 213 points (voxel downsampling, target: 512)\n",
      "  Level 2: 333 points (voxel downsampling, target: 1024)\n",
      "  Level 3: 2503 points (voxel downsampling, target: 2503)\n",
      "\n",
      "LEVEL 0: exact solve on 88 points...\n",
      "Coarse eigenvalues: [0.       0.453462 0.857634 1.240641 1.531008 1.982735 2.189365 2.65285\n",
      " 2.661715 3.49083 ]\n",
      "\n",
      "Training physics-informed multiresolution GNN...\n",
      "Epoch    0: Loss=0.765165 | Res=0.014793 | Orth=0.010652 | Mean=0.289682\n",
      "Epoch  250: Loss=0.161164 | Res=0.002690 | Orth=0.003411 | Mean=0.015210\n",
      "Epoch  500: Loss=0.052279 | Res=0.000616 | Orth=0.000621 | Mean=0.001848\n",
      "Epoch  750: Loss=0.031968 | Res=0.000352 | Orth=0.000137 | Mean=0.001570\n",
      "Epoch 1000: Loss=0.024991 | Res=0.000293 | Orth=0.000071 | Mean=0.001218\n",
      "Epoch 1250: Loss=0.020800 | Res=0.000264 | Orth=0.000058 | Mean=0.002592\n",
      "Epoch 1500: Loss=0.018149 | Res=0.000253 | Orth=0.000047 | Mean=0.002654\n",
      "Epoch 1750: Loss=0.016303 | Res=0.000246 | Orth=0.000024 | Mean=0.000248\n",
      "Epoch 1999: Loss=0.015044 | Res=0.000241 | Orth=0.000046 | Mean=0.004969\n",
      "Level 0 refined eigenvalues: [0.    0.459 0.873 1.258 1.536 1.996 2.197 2.666 2.688 3.569]\n",
      "Level 1 refined eigenvalues: [0.    0.434 0.899 1.252 1.445 1.741 2.135 2.873 3.125 3.645]\n",
      "Level 2 refined eigenvalues: [0.    0.43  0.907 1.271 1.429 1.916 2.271 2.949 3.207 3.637]\n",
      "Level 3 refined eigenvalues: [0.    0.436 0.984 1.253 1.399 1.55  2.117 3.25  3.56  3.707]\n"
     ]
    }
   ],
   "source": [
    "U_pred = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a41311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "cells = [('triangle', m.connectivity)]\n",
    "\n",
    "m_out = meshio.Mesh(m.verts, cells, point_data={f'v{i}': U_pred[:, i] for i in range(1, 10)})\n",
    "m_out.write('bunny_eigfuncs_voxel_downsampling.vtu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66758b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cdc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c65a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee66bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc2086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08fc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a9166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde33e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cfdf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d1aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282aca8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9128d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multigrid_gnn_multires_physics.py\n",
    "\"\"\"\n",
    "Physics-informed Multigrid + GNN eigen-refinement\n",
    "- Exact solve only on coarsest mesh\n",
    "- Multiresolution GNN with residual + orthonormality + projection loss\n",
    "- Coarse-to-fine prolongation only\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def farthest_point_sampling(points, n_samples, seed=None):\n",
    "    \"\"\"\n",
    "    Farthest Point Sampling (FPS) for nested hierarchical sampling.\n",
    "    \n",
    "    Args:\n",
    "        points: (N, 3) array of 3D points\n",
    "        n_samples: number of points to sample\n",
    "        seed: random seed for initial point selection\n",
    "    \n",
    "    Returns:\n",
    "        indices: (n_samples,) array of selected point indices\n",
    "    \"\"\"\n",
    "    n_points = points.shape[0]\n",
    "    if n_samples >= n_points:\n",
    "        return np.arange(n_points)\n",
    "    \n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # Start with a random point\n",
    "    selected_indices = [rng.integers(0, n_points)]\n",
    "    distances = np.full(n_points, np.inf)\n",
    "    \n",
    "    for _ in range(n_samples - 1):\n",
    "        # Update distances to nearest selected point\n",
    "        last_selected = selected_indices[-1]\n",
    "        dists_to_last = np.linalg.norm(points - points[last_selected], axis=1)\n",
    "        distances = np.minimum(distances, dists_to_last)\n",
    "        \n",
    "        # Select point farthest from all selected points\n",
    "        farthest_idx = np.argmax(distances)\n",
    "        selected_indices.append(farthest_idx)\n",
    "    \n",
    "    return np.array(selected_indices)\n",
    "\n",
    "def voxel_downsampling(points, hierarchy_levels):\n",
    "    \"\"\"\n",
    "    Voxel grid downsampling for nested hierarchical sampling.\n",
    "    \n",
    "    Args:\n",
    "        points: (N, 3) array of 3D points\n",
    "        hierarchy_levels: list of target point counts for each level\n",
    "    \n",
    "    Returns:\n",
    "        indices_per_level: dict mapping level index to array of selected point indices\n",
    "    \"\"\"\n",
    "    n_points = points.shape[0]\n",
    "    \n",
    "    # Compute bounding box\n",
    "    min_bound = points.min(axis=0)\n",
    "    max_bound = points.max(axis=0)\n",
    "    extent = max_bound - min_bound\n",
    "    \n",
    "    indices_per_level = {}\n",
    "    \n",
    "    for level_idx, target_count in enumerate(hierarchy_levels):\n",
    "        if target_count >= n_points:\n",
    "            indices_per_level[level_idx] = np.arange(n_points)\n",
    "            continue\n",
    "        \n",
    "        # Estimate voxel size based on target count\n",
    "        # Volume per voxel ≈ total_volume / target_count\n",
    "        volume = np.prod(extent)\n",
    "        voxel_volume = volume / target_count\n",
    "        voxel_size = voxel_volume ** (1/3)\n",
    "        \n",
    "        # Compute voxel grid dimensions\n",
    "        grid_dims = np.ceil(extent / voxel_size).astype(int) + 1\n",
    "        \n",
    "        # Assign each point to a voxel\n",
    "        voxel_indices = ((points - min_bound) / voxel_size).astype(int)\n",
    "        voxel_indices = np.clip(voxel_indices, 0, grid_dims - 1)\n",
    "        \n",
    "        # Flatten voxel indices to 1D\n",
    "        voxel_ids = (voxel_indices[:, 0] * grid_dims[1] * grid_dims[2] + \n",
    "                     voxel_indices[:, 1] * grid_dims[2] + \n",
    "                     voxel_indices[:, 2])\n",
    "        \n",
    "        # Select one point per occupied voxel (closest to voxel center)\n",
    "        unique_voxels = np.unique(voxel_ids)\n",
    "        selected_indices = []\n",
    "        \n",
    "        for voxel_id in unique_voxels:\n",
    "            mask = (voxel_ids == voxel_id)\n",
    "            voxel_points_idx = np.where(mask)[0]\n",
    "            \n",
    "            if len(voxel_points_idx) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute voxel center\n",
    "            voxel_3d = np.array([\n",
    "                voxel_id // (grid_dims[1] * grid_dims[2]),\n",
    "                (voxel_id // grid_dims[2]) % grid_dims[1],\n",
    "                voxel_id % grid_dims[2]\n",
    "            ])\n",
    "            voxel_center = min_bound + (voxel_3d + 0.5) * voxel_size\n",
    "            \n",
    "            # Find point closest to voxel center\n",
    "            voxel_points = points[voxel_points_idx]\n",
    "            distances = np.linalg.norm(voxel_points - voxel_center, axis=1)\n",
    "            closest_idx = voxel_points_idx[np.argmin(distances)]\n",
    "            selected_indices.append(closest_idx)\n",
    "            \n",
    "            # Stop if we've reached target count\n",
    "            if len(selected_indices) >= target_count:\n",
    "                break\n",
    "        \n",
    "        indices_per_level[level_idx] = np.array(selected_indices[:target_count])\n",
    "    \n",
    "    return indices_per_level\n",
    "\n",
    "# ------------------------\n",
    "# Simple neighbor-mean corrector\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128,64,32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        n = x.shape[0]\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device).clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)\n",
    "        return self.net(h)\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid GNN solver\n",
    "# ------------------------\n",
    "class MultigridGNN:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Physics-informed GNN training\n",
    "    # ------------------------\n",
    "    def train_multiresolution(self, X_list, U_init_list, edge_index_list, P_list, R_list,\n",
    "                              epochs=1000, lr=1e-3, corr_scale=1e-2,\n",
    "                              w_res=10.0, w_orth=1.0, w_proj=1e-3, w_cycle=1.0,\n",
    "                              grad_clip=1.0, weight_decay=1e-6, log_every=100):\n",
    "        device = self.device\n",
    "        n_modes = U_init_list[0].shape[1]\n",
    "\n",
    "        # Build torch tensors and resolution indicators\n",
    "        x_feats_all, U_all, edge_index_all = [], [], []\n",
    "        node_offset = 0\n",
    "        max_nodes = max([X.shape[0] for X in X_list])\n",
    "        for X, U_init, edge_index in zip(X_list, U_init_list, edge_index_list):\n",
    "            res_feat = np.full((X.shape[0], 1), X.shape[0]/max_nodes)\n",
    "            x_feats_all.append(np.hstack([X, U_init, res_feat]))\n",
    "            U_all.append(U_init)\n",
    "            edge_index_all.append(edge_index + node_offset)\n",
    "            node_offset += X.shape[0]\n",
    "\n",
    "        x_feats_all = torch.FloatTensor(np.vstack(x_feats_all)).to(device)\n",
    "        U_all_tensor = torch.FloatTensor(np.vstack(U_all)).to(device)\n",
    "        edge_index_all = torch.cat(edge_index_all, dim=1).to(device)\n",
    "\n",
    "        in_dim = x_feats_all.shape[1]\n",
    "        if self.model is None:\n",
    "            self.model = SimpleCorrector(in_dim, n_modes).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "\n",
    "        # ------------------------\n",
    "        # Precompute Laplacians per level\n",
    "        L_list, M_list = [], []\n",
    "        node_offset = 0\n",
    "        for X in X_list:\n",
    "            L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "            L_list.append(sp_to_torch_sparse(L).to(device))\n",
    "            M_list.append(sp_to_torch_sparse(M).to(device))\n",
    "\n",
    "        # Convert prolongation and restriction to torch sparse\n",
    "        P_list_torch = [sp_to_torch_sparse(P).to(device) for P in P_list]\n",
    "        R_list_torch = [sp_to_torch_sparse(R).to(device) for R in R_list]\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            corr_raw = self.model(x_feats_all, edge_index_all)\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_all_tensor + corr\n",
    "\n",
    "            # Physics-informed loss\n",
    "            loss = 0.0\n",
    "            node_offset = 0\n",
    "            L_res_total = 0.0\n",
    "            L_orth_total = 0.0\n",
    "            L_mean_total = 0.0\n",
    "            L_cycle_total = 0.0\n",
    "            \n",
    "            for i, (L_t, M_t, U_init) in enumerate(zip(L_list, M_list, U_init_list)):\n",
    "                n_nodes = U_init.shape[0]\n",
    "                U_level = U_pred[node_offset:node_offset+n_nodes]\n",
    "\n",
    "                # Rayleigh residual\n",
    "                Lu = torch.sparse.mm(L_t, U_level)\n",
    "                Mu = torch.sparse.mm(M_t, U_level)\n",
    "                num = torch.sum(U_level * Lu, dim=0)\n",
    "                den = torch.sum(U_level * Mu, dim=0) + 1e-12\n",
    "                lambdas = num / den\n",
    "                res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "                L_res = torch.mean(res**2)\n",
    "\n",
    "                # Orthonormality\n",
    "                Gram = U_level.t() @ Mu\n",
    "                L_orth = torch.mean((Gram - torch.eye(n_modes, device=device))**2)\n",
    "\n",
    "                # Zero-mean constraint: 1.T @ M @ u = 0 for modes 1 onwards\n",
    "                ones = torch.ones(n_nodes, 1, device=device)\n",
    "                mean_constraint = ones.t() @ Mu[:, 1:]  # Shape: (1, n_modes-1)\n",
    "                L_mean = torch.mean(mean_constraint**2)\n",
    "\n",
    "                L_res_total += L_res\n",
    "                L_orth_total += L_orth\n",
    "                L_mean_total += L_mean\n",
    "                \n",
    "                loss += w_res * L_res + w_orth * L_orth + w_proj * L_mean\n",
    "                \n",
    "                # Prolongation-restriction cycle consistency loss (for fine levels)\n",
    "                if i > 0:\n",
    "                    node_offset_coarse = sum([U_init_list[j].shape[0] for j in range(i)])\n",
    "                    U_coarse = U_pred[node_offset_coarse:node_offset_coarse+U_init_list[i-1].shape[0]]\n",
    "                    \n",
    "                    # Cycle: coarse -> prolong -> restrict -> should match coarse\n",
    "                    U_prolonged = torch.sparse.mm(P_list_torch[i-1], U_coarse)\n",
    "                    U_cycled = torch.sparse.mm(R_list_torch[i-1], U_prolonged)\n",
    "                    L_cycle = torch.mean((U_cycled - U_coarse)**2)\n",
    "                    \n",
    "                    L_cycle_total += L_cycle\n",
    "                    loss += w_cycle * L_cycle\n",
    "                \n",
    "                node_offset += n_nodes\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.model.parameters()), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if ep % log_every == 0 or ep == epochs-1:\n",
    "                print(f\"Epoch {ep:4d}: Loss={loss.item():.6f} | Res={L_res_total.item():.6f} | Orth={L_orth_total.item():.6f} | Mean={L_mean_total.item():.6f} | Cycle={L_cycle_total.item():.6f}\")\n",
    "\n",
    "        return U_pred.detach().cpu().numpy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L, M):\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        L_t = sp_to_torch_sparse(L).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "    \n",
    "\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None):\n",
    "    \"\"\"Visualize mesh with vertices, optionally highlighting specific points.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    # Plot full mesh with transparency\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2], \n",
    "                    triangles=mesh.connectivity, alpha=0.3)\n",
    "    \n",
    "    # Highlight specific points if provided\n",
    "    if highlight_indices is not None:\n",
    "        highlighted_verts = mesh.verts[highlight_indices]\n",
    "        ax.scatter(highlighted_verts[:, 0], highlighted_verts[:, 1], highlighted_verts[:, 2], \n",
    "                   c='fuchsia', s=10, alpha=0.8, label=f'{len(highlight_indices)} selected points')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=130, azim=-90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hierarchy = [128, 512, 1024]  # final level is full mesh\n",
    "    k_neighbors = 4\n",
    "    epochs = 2000\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridGNN.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    # Use Voxel Grid Downsampling for nested hierarchy\n",
    "    print(\"Computing voxel grid downsampling hierarchy...\")\n",
    "    indices_per_level = voxel_downsampling(X_full, hierarchy)\n",
    "    \n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        actual_count = len(indices_per_level[i])\n",
    "        print(f\"  Level {i}: {actual_count} points (voxel downsampling, target: {n_points})\")\n",
    "    \n",
    "\n",
    "    #for level_idx_vis, n_points in enumerate(hierarchy):\n",
    "    #    # Visualize selected points for this level\n",
    "    #    visualize_mesh(mesh, \n",
    "    #                  title=f'Level {level_idx_vis}: {len(indices_per_level[level_idx_vis])} Voxel Downsampled Points',\n",
    "    #                  highlight_indices=indices_per_level[level_idx_vis])\n",
    "    #print()\n",
    "\n",
    "    solver = MultigridGNN()\n",
    "\n",
    "    # ------------------------\n",
    "    # Level 0: exact coarse solve\n",
    "    # ------------------------\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(f\"\\nLEVEL 0: exact solve on {X0.shape[0]} points...\")\n",
    "    lambda0, U0, L0, M0 = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda0,6))\n",
    "\n",
    "    # ------------------------\n",
    "    # Coarse-to-fine prolongation and build restriction operators\n",
    "    # ------------------------\n",
    "    U_prev = U0.copy()\n",
    "    X_list, U_init_list, edge_index_list = [X0], [U0], [solver.build_knn_graph(X0, k=k_neighbors)]\n",
    "    P_list, R_list = [], []  # Prolongation and restriction operators\n",
    "    \n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level-1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "\n",
    "        P = solver.build_prolongation(Xc, Xf, k=1)\n",
    "        R = solver.build_prolongation(Xf, Xc, k=1)  # Restriction: fine -> coarse\n",
    "        U_init = P @ U_prev\n",
    "        edge_index = solver.build_knn_graph(Xf, k=k_neighbors)\n",
    "\n",
    "        X_list.append(Xf)\n",
    "        U_init_list.append(U_init)\n",
    "        edge_index_list.append(edge_index)\n",
    "        P_list.append(P)\n",
    "        R_list.append(R)\n",
    "\n",
    "        # Check initial prolongation-restriction cycle error\n",
    "        U_cycled = R @ P @ U_prev\n",
    "        cycle_error = np.linalg.norm(U_cycled - U_prev, 'fro') / (np.linalg.norm(U_prev, 'fro') + 1e-12)\n",
    "        print(f\"Level {level-1}->{level} initial P-R cycle error: {cycle_error:.6f}\")\n",
    "\n",
    "        U_prev = U_init.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Train physics-informed GNN\n",
    "    # ------------------------\n",
    "    print(\"\\nTraining physics-informed multiresolution GNN...\")\n",
    "    U_pred_all = solver.train_multiresolution(X_list, U_init_list, edge_index_list, P_list, R_list,\n",
    "                                              epochs=epochs)\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement per level\n",
    "    # ------------------------\n",
    "    node_offset = 0\n",
    "    for level, X in enumerate(X_list):\n",
    "        n_nodes = X.shape[0]\n",
    "        U_pred = U_pred_all[node_offset:node_offset+n_nodes]\n",
    "        node_offset += n_nodes\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals_refined, _ = solver.refine_eigenvectors(U_pred, L, M)\n",
    "        print(f\"Level {level} refined eigenvalues: {np.round(vals_refined,3)}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Check final prolongation-restriction cycle error\n",
    "    # ------------------------\n",
    "    print(\"\\nFinal P-R cycle errors after training:\")\n",
    "    node_offset = 0\n",
    "    for level in range(len(X_list)):\n",
    "        n_nodes = X_list[level].shape[0]\n",
    "        U_level = U_pred_all[node_offset:node_offset+n_nodes]\n",
    "        \n",
    "        if level > 0:\n",
    "            node_offset_coarse = sum([X_list[j].shape[0] for j in range(level)])\n",
    "            U_coarse = U_pred_all[node_offset_coarse:node_offset_coarse+X_list[level-1].shape[0]]\n",
    "            \n",
    "            U_prolonged = P_list[level-1] @ U_coarse\n",
    "            U_cycled = R_list[level-1] @ U_prolonged\n",
    "            cycle_error = np.linalg.norm(U_cycled - U_coarse, 'fro') / (np.linalg.norm(U_coarse, 'fro') + 1e-12)\n",
    "            print(f\"Level {level-1}->{level} final P-R cycle error: {cycle_error:.6f}\")\n",
    "        \n",
    "        node_offset += n_nodes\n",
    "\n",
    "    return U_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ae610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Hierarchy: [128, 512, 1024, 2503]\n",
      "Computing voxel grid downsampling hierarchy...\n",
      "  Level 0: 88 points (voxel downsampling, target: 128)\n",
      "  Level 1: 213 points (voxel downsampling, target: 512)\n",
      "  Level 2: 333 points (voxel downsampling, target: 1024)\n",
      "  Level 3: 2503 points (voxel downsampling, target: 2503)\n",
      "\n",
      "LEVEL 0: exact solve on 88 points...\n",
      "Coarse eigenvalues: [0.       0.453462 0.857634 1.240641 1.531008 1.982735 2.189365 2.65285\n",
      " 2.661715 3.49083 ]\n",
      "Level 0->1 initial P-R cycle error: 0.059772\n",
      "Level 1->2 initial P-R cycle error: 0.152141\n",
      "Level 2->3 initial P-R cycle error: 0.000000\n",
      "\n",
      "Training physics-informed multiresolution GNN...\n",
      "Epoch    0: Loss=0.773112 | Res=0.074177 | Orth=0.022503 | Mean=0.565491 | Cycle=0.008273\n",
      "Epoch  100: Loss=0.587270 | Res=0.056266 | Orth=0.018414 | Mean=0.251478 | Cycle=0.005950\n",
      "Epoch  200: Loss=0.349678 | Res=0.032046 | Orth=0.023138 | Mean=0.098935 | Cycle=0.005976\n",
      "Epoch  300: Loss=0.166444 | Res=0.014262 | Orth=0.017540 | Mean=0.101423 | Cycle=0.006180\n",
      "Epoch  400: Loss=0.092518 | Res=0.007679 | Orth=0.009233 | Mean=0.072750 | Cycle=0.006425\n",
      "Epoch  500: Loss=0.058513 | Res=0.004584 | Orth=0.006064 | Mean=0.072895 | Cycle=0.006535\n",
      "Epoch  600: Loss=0.048042 | Res=0.003672 | Orth=0.004724 | Mean=0.064447 | Cycle=0.006538\n",
      "Epoch  700: Loss=0.042071 | Res=0.003141 | Orth=0.004040 | Mean=0.064262 | Cycle=0.006560\n",
      "Epoch  800: Loss=0.038082 | Res=0.002782 | Orth=0.003627 | Mean=0.064802 | Cycle=0.006574\n",
      "Epoch  900: Loss=0.035242 | Res=0.002528 | Orth=0.003340 | Mean=0.060857 | Cycle=0.006559\n",
      "Epoch 1000: Loss=0.032785 | Res=0.002313 | Orth=0.003058 | Mean=0.061362 | Cycle=0.006540\n",
      "Epoch 1100: Loss=0.030757 | Res=0.002131 | Orth=0.002870 | Mean=0.058607 | Cycle=0.006515\n",
      "Epoch 1200: Loss=0.029076 | Res=0.001988 | Orth=0.002657 | Mean=0.051136 | Cycle=0.006492\n",
      "Epoch 1300: Loss=0.027702 | Res=0.001868 | Orth=0.002509 | Mean=0.048339 | Cycle=0.006464\n",
      "Epoch 1400: Loss=0.026585 | Res=0.001767 | Orth=0.002412 | Mean=0.052312 | Cycle=0.006451\n",
      "Epoch 1500: Loss=0.025579 | Res=0.001683 | Orth=0.002266 | Mean=0.044191 | Cycle=0.006439\n",
      "Epoch 1600: Loss=0.024686 | Res=0.001606 | Orth=0.002157 | Mean=0.044853 | Cycle=0.006428\n",
      "Epoch 1700: Loss=0.023885 | Res=0.001538 | Orth=0.002067 | Mean=0.040149 | Cycle=0.006401\n",
      "Epoch 1800: Loss=0.023231 | Res=0.001480 | Orth=0.001991 | Mean=0.040177 | Cycle=0.006397\n",
      "Epoch 1900: Loss=0.022652 | Res=0.001431 | Orth=0.001920 | Mean=0.039430 | Cycle=0.006381\n",
      "Epoch 1999: Loss=0.022169 | Res=0.001387 | Orth=0.001864 | Mean=0.043931 | Cycle=0.006388\n",
      "Level 0 refined eigenvalues: [0.    0.46  0.868 1.256 1.536 1.997 2.198 2.675 2.693 3.567]\n",
      "Level 1 refined eigenvalues: [0.    0.436 0.898 1.259 1.443 1.746 2.158 2.897 3.126 3.677]\n",
      "Level 2 refined eigenvalues: [0.    0.439 0.912 1.281 1.431 1.916 2.306 2.968 3.191 3.652]\n",
      "Level 3 refined eigenvalues: [0.    0.445 0.993 1.374 1.406 1.53  2.092 3.24  3.601 3.667]\n",
      "\n",
      "Final P-R cycle errors after training:\n",
      "Level 0->1 final P-R cycle error: 0.241783\n",
      "Level 1->2 final P-R cycle error: 0.406406\n",
      "Level 2->3 final P-R cycle error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "U_pred = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f2f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "cells = [('triangle', m.connectivity)]\n",
    "\n",
    "m_out = meshio.Mesh(m.verts, cells, point_data={f'v{i}': U_pred[:, i] for i in range(1, 10)})\n",
    "m_out.write('bunny_eigfuncs_voxel_restricted.vtu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589a69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
