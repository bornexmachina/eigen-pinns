{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0014d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Neural Rayleigh-Ritz Eigensolver\n",
    "---------------------------------------------\n",
    "Author: <you>\n",
    "Version: 2.1 (fixed + includes small/medium tests)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MATRIX HIERARCHY\n",
    "# ===============================================================\n",
    "\n",
    "def build_hierarchy(K_full, M_full, levels=None, method='uniform'):\n",
    "    \"\"\"\n",
    "    Build matrix hierarchy by downsampling.\n",
    "    \"\"\"\n",
    "    if levels is None:\n",
    "        levels = {0: 2**5, 1: 2**10, 2: 2**15, 3: 2**20}\n",
    "\n",
    "    n_full = K_full.shape[0]\n",
    "    K_hierarchy, M_hierarchy, idx_hierarchy = {}, {}, {}\n",
    "\n",
    "    for level, size in sorted(levels.items()):\n",
    "        size = int(size)\n",
    "        if size >= n_full:\n",
    "            idx = np.arange(n_full)\n",
    "        else:\n",
    "            if method == 'uniform':\n",
    "                idx = np.linspace(0, n_full - 1, num=size)\n",
    "                idx = np.unique(np.round(idx).astype(int))\n",
    "                if idx.size < size:\n",
    "                    remaining = set(range(n_full)) - set(idx)\n",
    "                    extra = np.array(sorted(list(remaining)))[: (size - idx.size)]\n",
    "                    idx = np.sort(np.concatenate([idx, extra]))\n",
    "            elif method == 'random':\n",
    "                np.random.seed(42 + level)\n",
    "                idx = np.sort(np.random.choice(n_full, size, replace=False))\n",
    "            elif method == 'importance':\n",
    "                idx = leverage_score_sampling(K_full, size, seed=42 + level)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        K_hierarchy[level] = K_full[np.ix_(idx, idx)]\n",
    "        M_hierarchy[level] = M_full[np.ix_(idx, idx)]\n",
    "        idx_hierarchy[level] = idx\n",
    "\n",
    "        print(f\"Built level {level}: {size} DOFs\")\n",
    "\n",
    "    return K_hierarchy, M_hierarchy, idx_hierarchy\n",
    "\n",
    "\n",
    "def leverage_score_sampling(K, target_size, seed=0):\n",
    "    \"\"\"Approximate leverage-score sampling.\"\"\"\n",
    "    n = K.shape[0]\n",
    "    if n > 10000:\n",
    "        scores = np.abs(np.diag(K)).astype(float)\n",
    "    else:\n",
    "        scores = np.sum(np.abs(K) ** 2, axis=1).astype(float)\n",
    "    scores = 0.9 * (scores / np.sum(scores)) + 0.1 / n\n",
    "    np.random.seed(seed)\n",
    "    idx = np.sort(np.random.choice(n, target_size, replace=False, p=scores / scores.sum()))\n",
    "    return idx\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# RAYLEIGH–RITZ REFINEMENT\n",
    "# ===============================================================\n",
    "\n",
    "def simple_rayleigh_ritz_refinement(λ_init, u_init, K, M, n_iter=5):\n",
    "    \"\"\"Simple Rayleigh–Ritz refinement.\"\"\"\n",
    "    λ_refined, u_refined = λ_init.copy(), u_init.copy()\n",
    "    print(f\"  Simple Rayleigh–Ritz refinement: {n_iter} iterations\")\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i in range(len(λ_refined)):\n",
    "            u = u_refined[:, i]\n",
    "            Ku, Mu = K @ u, M @ u\n",
    "            λ_refined[i] = (u.T @ Ku) / (u.T @ Mu)\n",
    "            for j in range(i):\n",
    "                overlap = u.T @ M @ u_refined[:, j]\n",
    "                u -= overlap * u_refined[:, j]\n",
    "            u_refined[:, i] = u / np.sqrt(u.T @ M @ u)\n",
    "\n",
    "            if it == n_iter - 1:\n",
    "                res = np.linalg.norm(K @ u - λ_refined[i] * M @ u)\n",
    "                print(f\"    Eigenpair {i}: residual={res:.2e}, λ={λ_refined[i]:.6f}\")\n",
    "\n",
    "    return λ_refined, u_refined\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# UPSCALER NETWORK\n",
    "# ===============================================================\n",
    "\n",
    "class HierarchicalUpscaler(nn.Module):\n",
    "    \"\"\"Neural upscaler network.\"\"\"\n",
    "\n",
    "    def __init__(self, n_coarse, n_fine, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = min(n_coarse * hidden_factor, max(32, n_fine // 2))\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_coarse, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, n_fine)\n",
    "        )\n",
    "\n",
    "    def forward(self, u_coarse):\n",
    "        was_1d = False\n",
    "        if u_coarse.dim() == 1:\n",
    "            u_coarse = u_coarse.unsqueeze(0)\n",
    "            was_1d = True\n",
    "        out = self.net(u_coarse)\n",
    "        return out.squeeze(0) if was_1d else out\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# LOSS & TRAINING\n",
    "# ===============================================================\n",
    "\n",
    "def compute_loss(u_fine, λ, K_fine, M_fine, u_previous=None, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {'residual': 1.0, 'normalization': 10.0,\n",
    "                   'orthogonality': 5.0, 'smoothness': 0.01}\n",
    "\n",
    "    Ku = K_fine @ u_fine\n",
    "    Mu = M_fine @ u_fine\n",
    "    residual = Ku - λ * Mu\n",
    "\n",
    "    losses = {\n",
    "        'residual': torch.mean(residual ** 2),\n",
    "        'normalization': (u_fine.T @ Mu - 1.0) ** 2,\n",
    "        'orthogonality': torch.zeros((), device=u_fine.device),\n",
    "        'smoothness': torch.mean((u_fine[1:] - u_fine[:-1]) ** 2) if u_fine.numel() > 1 else 0\n",
    "    }\n",
    "\n",
    "    if u_previous is not None:\n",
    "        for u_prev in u_previous:\n",
    "            if not isinstance(u_prev, torch.Tensor):\n",
    "                u_prev = torch.tensor(u_prev, dtype=u_fine.dtype, device=u_fine.device)\n",
    "            overlap = (u_fine.T @ (M_fine @ u_prev))\n",
    "            losses['orthogonality'] += overlap ** 2\n",
    "\n",
    "    total = sum(weights[k] * losses[k] for k in losses)\n",
    "    return total, losses\n",
    "\n",
    "\n",
    "def adaptive_weights(epoch):\n",
    "    return {\n",
    "        'residual': 1.0,\n",
    "        'normalization': 10.0 * np.exp(-epoch / 100),\n",
    "        'orthogonality': 5.0,\n",
    "        'smoothness': 0.01\n",
    "    }\n",
    "\n",
    "\n",
    "def train_upscaler(level_from, level_to,\n",
    "                   λ_coarse, u_coarse,\n",
    "                   K_fine, M_fine,\n",
    "                   n_eigenpairs=10,\n",
    "                   n_epochs=1000,\n",
    "                   device='cuda'):\n",
    "    n_coarse, n_fine = u_coarse.shape[0], K_fine.shape[0]\n",
    "    print(f\"Training upscaler: {n_coarse} → {n_fine} (level {level_from} → {level_to})\")\n",
    "\n",
    "    λ_fine = np.zeros(n_eigenpairs)\n",
    "    u_fine_all = []\n",
    "\n",
    "    K_t = torch.tensor(K_fine, dtype=torch.float32, device=device)\n",
    "    M_t = torch.tensor(M_fine, dtype=torch.float32, device=device)\n",
    "\n",
    "    for i in range(n_eigenpairs):\n",
    "        print(f\"  Eigenpair {i + 1}/{n_eigenpairs}\")\n",
    "        model = HierarchicalUpscaler(n_coarse, n_fine).to(device)\n",
    "        lam = nn.Parameter(torch.tensor(float(λ_coarse[i]), dtype=torch.float32, device=device))\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + [lam], lr=1e-3)\n",
    "\n",
    "        u_c = torch.tensor(u_coarse[:, i], dtype=torch.float32, device=device)\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            u_f = model(u_c)\n",
    "            weights = adaptive_weights(epoch)\n",
    "            loss, _ = compute_loss(u_f, lam, K_t, M_t, u_previous=u_fine_all, weights=weights)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"    Epoch {epoch}: loss={loss.item():.2e}, λ={lam.item():.6f}\")\n",
    "\n",
    "            if loss.item() < best_loss - 1e-12:\n",
    "                best_loss, patience_counter = loss.item(), 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter > 200:\n",
    "                    print(f\"    Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        λ_fine[i] = lam.detach().cpu().item()\n",
    "        u_fine_all.append(u_f.detach())\n",
    "\n",
    "    u_fine_matrix = torch.stack(u_fine_all, dim=1).cpu().numpy()\n",
    "    return λ_fine, u_fine_matrix\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# PIPELINE\n",
    "# ===============================================================\n",
    "\n",
    "def hierarchical_eigensolve(K_full, M_full, n_eigenpairs=10, levels=None, method='uniform', device='cuda'):\n",
    "    print(\"Building matrix hierarchy...\")\n",
    "    K_hierarchy, M_hierarchy, _ = build_hierarchy(K_full, M_full, levels, method)\n",
    "    level_keys = sorted(K_hierarchy.keys())\n",
    "\n",
    "    K_coarse, M_coarse = K_hierarchy[level_keys[0]], M_hierarchy[level_keys[0]]\n",
    "    print(f\"Coarsest level size: {K_coarse.shape}\")\n",
    "\n",
    "    if K_coarse.shape[0] <= 2000:\n",
    "        λ_current, u_current = eigh(K_coarse, M_coarse)\n",
    "    else:\n",
    "        λ_current, u_current = eigsh(csr_matrix(K_coarse), k=n_eigenpairs, M=csr_matrix(M_coarse), which='SM')\n",
    "\n",
    "    λ_current = λ_current[:n_eigenpairs]\n",
    "    u_current = u_current[:, :n_eigenpairs]\n",
    "    print(f\"Coarsest eigenvalues: {λ_current}\")\n",
    "\n",
    "    for i in range(1, len(level_keys)):\n",
    "        l_from, l_to = level_keys[i - 1], level_keys[i]\n",
    "        print(f\"\\nRefinement step {i}: level {l_from} → {l_to}\")\n",
    "        Kf, Mf = K_hierarchy[l_to], M_hierarchy[l_to]\n",
    "        λ_current, u_current = train_upscaler(l_from, l_to, λ_current, u_current, Kf, Mf,\n",
    "                                              n_eigenpairs=n_eigenpairs, device=device)\n",
    "        λ_current, u_current = simple_rayleigh_ritz_refinement(λ_current, u_current, Kf, Mf, n_iter=3)\n",
    "\n",
    "    return λ_current, u_current\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# TEST UTILITIES\n",
    "# ===============================================================\n",
    "\n",
    "def generate_test_matrices(n, matrix_type='laplacian'):\n",
    "    if matrix_type == 'laplacian':\n",
    "        K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "        M = np.eye(n)\n",
    "    elif matrix_type == 'tridiagonal':\n",
    "        main_diag = 2 + 0.1 * np.arange(n)\n",
    "        K = np.diag(main_diag) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "        M = np.diag(1 + 0.01 * np.arange(n))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown matrix type.\")\n",
    "    return K, M\n",
    "\n",
    "\n",
    "def verify_eigenpairs(λ, u, K, M, name=\"\"):\n",
    "    print(f\"\\n{name} verification\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(len(λ)):\n",
    "        res = np.linalg.norm(K @ u[:, i] - λ[i] * M @ u[:, i])\n",
    "        print(f\"  λ[{i}] = {λ[i]:.6f}, residual = {res:.2e}\")\n",
    "    U = u[:, :len(λ)]\n",
    "    off_diag = np.abs(U.T @ M @ U - np.eye(len(λ)))\n",
    "    print(f\"  Max off-diagonal: {np.max(off_diag):.2e}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# TEST RUNNERS\n",
    "# ===============================================================\n",
    "\n",
    "def run_quick_test():\n",
    "    print(\"\\n===== QUICK TEST (n=128) =====\")\n",
    "    K, M = generate_test_matrices(128)\n",
    "    levels = {0: 16, 1: 128}\n",
    "    λ, u = hierarchical_eigensolve(K, M, n_eigenpairs=3, levels=levels)\n",
    "    verify_eigenpairs(λ, u, K, M, \"Quick test\")\n",
    "    print(\"✓ Quick test done.\")\n",
    "\n",
    "\n",
    "def run_small_test():\n",
    "    print(\"\\n===== SMALL TEST (n=512) =====\")\n",
    "    K, M = generate_test_matrices(512)\n",
    "    levels = {0: 32, 1: 128, 2: 512}\n",
    "    λ, u = hierarchical_eigensolve(K, M, n_eigenpairs=5, levels=levels)\n",
    "    verify_eigenpairs(λ, u, K, M, \"Small test\")\n",
    "    print(\"✓ Small test done.\")\n",
    "\n",
    "\n",
    "def run_medium_test():\n",
    "    print(\"\\n===== MEDIUM TEST (n=4096, sparse) =====\")\n",
    "    K, M = generate_test_matrices(4096)\n",
    "    levels = {0: 64, 1: 512, 2: 2048, 3: 4096}\n",
    "    λ, u = hierarchical_eigensolve(K, M, n_eigenpairs=5, levels=levels)\n",
    "    verify_eigenpairs(λ, u, K, M, \"Medium test\")\n",
    "    print(\"✓ Medium test done.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MAIN\n",
    "# ===============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_quick_test()\n",
    "    # Uncomment to test more levels:\n",
    "    # run_small_test()\n",
    "    # run_medium_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0031d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5be60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b604fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb664c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d40df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511dc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e44a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8f6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f2f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import eigh, solve\n",
    "\n",
    "# =========================\n",
    "# Device Configuration\n",
    "# =========================\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (CUDA if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "# =========================\n",
    "# Hierarchy Construction\n",
    "# =========================\n",
    "def build_hierarchy(K_full, M_full, levels=None, method='uniform'):\n",
    "    \"\"\"\n",
    "    Build a hierarchy of downsampled matrices.\n",
    "    \"\"\"\n",
    "    if levels is None:\n",
    "        levels = {0: 32, 1: 256, 2: 2048, 3: K_full.shape[0]}\n",
    "\n",
    "    n_full = K_full.shape[0]\n",
    "    K_hierarchy, M_hierarchy, idx_hierarchy = {}, {}, {}\n",
    "\n",
    "    for level, size in sorted(levels.items()):\n",
    "        if size > n_full:\n",
    "            size = n_full\n",
    "\n",
    "        if method == 'uniform':\n",
    "            idx = np.linspace(0, n_full - 1, size, dtype=int)\n",
    "        elif method == 'random':\n",
    "            np.random.seed(42 + level)\n",
    "            idx = np.sort(np.random.choice(n_full, size, replace=False))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        K_hierarchy[level] = K_full[np.ix_(idx, idx)]\n",
    "        M_hierarchy[level] = M_full[np.ix_(idx, idx)]\n",
    "        idx_hierarchy[level] = idx\n",
    "\n",
    "        print(f\"Built level {level}: {size} DOFs\")\n",
    "\n",
    "    return K_hierarchy, M_hierarchy, idx_hierarchy\n",
    "\n",
    "# =========================\n",
    "# Coarse → Fine Projection\n",
    "# =========================\n",
    "def project_coarse_to_fine(u_coarse, idx_coarse, n_fine, method='linear'):\n",
    "    \"\"\"\n",
    "    Project coarse vector to fine mesh via simple linear mapping.\n",
    "    \"\"\"\n",
    "    u_fine = np.zeros(n_fine, dtype=float)\n",
    "    u_fine[idx_coarse] = u_coarse\n",
    "    return u_fine\n",
    "\n",
    "# =========================\n",
    "# Neural Upscaler\n",
    "# =========================\n",
    "class HierarchicalUpscaler(nn.Module):\n",
    "    def __init__(self, n_coarse, n_fine, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = max(min(n_coarse * hidden_factor, n_fine // 2), 32)\n",
    "        # Input: n_coarse, output: n_fine\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_coarse, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, n_fine)\n",
    "        )\n",
    "\n",
    "    def forward(self, u_coarse):\n",
    "        \"\"\"\n",
    "        u_coarse: (n_coarse,) or (batch, n_coarse)\n",
    "        Returns: (n_fine,) or (batch, n_fine)\n",
    "        \"\"\"\n",
    "        if u_coarse.dim() == 1:\n",
    "            return self.net(u_coarse.unsqueeze(0)).squeeze(0)\n",
    "        else:\n",
    "            return self.net(u_coarse)\n",
    "\n",
    "# =========================\n",
    "# Loss Function\n",
    "# =========================\n",
    "def compute_loss(u_fine, λ, K_fine, M_fine, u_previous=None, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {'residual': 1.0, 'normalization': 10.0, 'orthogonality': 5.0, 'smoothness': 0.01}\n",
    "\n",
    "    losses = {}\n",
    "    Ku = K_fine @ u_fine\n",
    "    Mu = M_fine @ u_fine\n",
    "\n",
    "    # Residual: (K - λM)u ≈ 0\n",
    "    losses['residual'] = torch.mean((Ku - λ * Mu) ** 2)\n",
    "    \n",
    "    # Normalization: u^T M u = 1\n",
    "    losses['normalization'] = ((u_fine.T @ Mu) - 1.0) ** 2\n",
    "\n",
    "    # Orthogonality to previous eigenvectors\n",
    "    losses['orthogonality'] = torch.tensor(0.0, device=u_fine.device, dtype=u_fine.dtype)\n",
    "    if u_previous is not None and len(u_previous) > 0:\n",
    "        for u_prev in u_previous:\n",
    "            overlap = (u_fine.T @ (M_fine @ u_prev))\n",
    "            losses['orthogonality'] += overlap ** 2\n",
    "\n",
    "    # Smoothness regularization\n",
    "    losses['smoothness'] = torch.mean((u_fine[1:] - u_fine[:-1]) ** 2)\n",
    "    \n",
    "    total = sum(weights[k] * losses[k] for k in losses)\n",
    "    return total, losses\n",
    "\n",
    "def adaptive_weights(epoch, losses_history=None):\n",
    "    return {\n",
    "        'residual': 1.0,\n",
    "        'normalization': 10.0 * np.exp(-epoch / 100),\n",
    "        'orthogonality': 5.0,\n",
    "        'smoothness': 0.01\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Upscaler Training\n",
    "# =========================\n",
    "def train_upscaler(level_from, level_to, λ_coarse, u_coarse, K_fine, M_fine, idx_coarse, \n",
    "                   n_eigenpairs=10, n_epochs=1000, lr=1e-3, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    n_coarse = u_coarse.shape[0]\n",
    "    n_fine = K_fine.shape[0]\n",
    "\n",
    "    print(f\"Training upscaler: {n_coarse} → {n_fine} (level {level_from} → {level_to})\")\n",
    "\n",
    "    λ_fine = torch.zeros(n_eigenpairs)\n",
    "    u_fine_all = []\n",
    "\n",
    "    # Move matrices to GPU\n",
    "    K_t = torch.tensor(K_fine, dtype=torch.float32, device=device)\n",
    "    M_t = torch.tensor(M_fine, dtype=torch.float32, device=device)\n",
    "\n",
    "    for i in range(n_eigenpairs):\n",
    "        print(f\"  Eigenpair {i+1}/{n_eigenpairs}\")\n",
    "\n",
    "        # Initialize eigenvalue from coarse solution\n",
    "        λ_init = λ_coarse[i]\n",
    "        \n",
    "        # Convert coarse eigenvector to tensor and move to GPU\n",
    "        u_c = torch.tensor(u_coarse[:, i], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Initialize eigenvalue as trainable parameter on GPU\n",
    "        λ = torch.tensor(λ_init, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "        # Create and move upscaler model to GPU\n",
    "        model = HierarchicalUpscaler(n_coarse, n_fine).to(device)\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + [λ], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.5)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Upscale coarse eigenvector to fine mesh\n",
    "            u_f = model(u_c)\n",
    "            \n",
    "            # Compute loss with adaptive weights\n",
    "            weights = adaptive_weights(epoch)\n",
    "            loss, loss_dict = compute_loss(u_f, λ, K_t, M_t, u_previous=u_fine_all, weights=weights)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "            # Logging\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"    Epoch {epoch}: Loss={loss.item():.2e}, λ={λ.item():.6f}, \"\n",
    "                      f\"residual={loss_dict['residual'].item():.2e}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter > 200:\n",
    "                    print(f\"    Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        # Store results (move back to CPU for storage)\n",
    "        λ_fine[i] = λ.detach().cpu()\n",
    "        u_fine_all.append(u_f.detach())  # Keep on GPU for next iteration\n",
    "\n",
    "    # Convert to numpy (move to CPU)\n",
    "    u_fine_matrix = torch.stack(u_fine_all, dim=1).cpu().numpy()\n",
    "    return λ_fine.numpy(), u_fine_matrix\n",
    "\n",
    "# =========================\n",
    "# Simple Rayleigh–Ritz Refinement\n",
    "# =========================\n",
    "def simple_rayleigh_ritz_refinement(λ_init, u_init, K, M, n_iter=3):\n",
    "    \"\"\"\n",
    "    Refine eigenpairs using Rayleigh quotient iteration and Gram-Schmidt orthogonalization.\n",
    "    \"\"\"\n",
    "    λ_refined = λ_init.copy()\n",
    "    u_refined = u_init.copy()\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i in range(len(λ_init)):\n",
    "            u = u_refined[:, i]\n",
    "            Ku = K @ u\n",
    "            Mu = M @ u\n",
    "            \n",
    "            # Update eigenvalue using Rayleigh quotient\n",
    "            λ_refined[i] = (u.T @ Ku) / (u.T @ Mu)\n",
    "\n",
    "            # Orthogonalize against previous eigenvectors\n",
    "            for j in range(i):\n",
    "                overlap = u.T @ M @ u_refined[:, j]\n",
    "                u -= overlap * u_refined[:, j]\n",
    "\n",
    "            # Normalize\n",
    "            norm = np.sqrt(u.T @ M @ u)\n",
    "            if norm > 1e-10:\n",
    "                u_refined[:, i] = u / norm\n",
    "            else:\n",
    "                print(f\"Warning: Near-zero norm at iteration {it}, eigenvector {i}\")\n",
    "\n",
    "    return λ_refined, u_refined\n",
    "\n",
    "# =========================\n",
    "# Hierarchical Eigen Solve\n",
    "# =========================\n",
    "def hierarchical_eigensolve(K_full, M_full, n_eigenpairs=10, method='uniform', levels=None, device=None):\n",
    "    \"\"\"\n",
    "    Solve generalized eigenvalue problem K u = λ M u using hierarchical neural upscaling.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    # Build hierarchy of coarse meshes\n",
    "    K_h, M_h, idx_h = build_hierarchy(K_full, M_full, levels=levels, method=method)\n",
    "    level_keys = sorted(K_h.keys())\n",
    "\n",
    "    # Solve coarsest level exactly (on CPU with scipy)\n",
    "    print(\"\\nSolving coarsest level...\")\n",
    "    K_coarse = K_h[level_keys[0]]\n",
    "    M_coarse = M_h[level_keys[0]]\n",
    "    λ_current, u_current = eigh(K_coarse, M_coarse)\n",
    "    λ_current = λ_current[:n_eigenpairs]\n",
    "    u_current = u_current[:, :n_eigenpairs]\n",
    "    \n",
    "    print(f\"Coarsest eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "\n",
    "    # Progressive refinement through hierarchy\n",
    "    for i in range(1, len(level_keys)):\n",
    "        level_from = level_keys[i-1]\n",
    "        level_to = level_keys[i]\n",
    "\n",
    "        print(f\"\\n--- Refining from level {level_from} to level {level_to} ---\")\n",
    "        \n",
    "        # Neural upscaling (on GPU)\n",
    "        λ_current, u_current = train_upscaler(\n",
    "            level_from, level_to,\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            idx_coarse=idx_h[level_from],\n",
    "            n_eigenpairs=n_eigenpairs,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Post-refinement with Rayleigh-Ritz (on CPU)\n",
    "        print(f\"  Applying Rayleigh-Ritz refinement...\")\n",
    "        λ_current, u_current = simple_rayleigh_ritz_refinement(\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            n_iter=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  Refined eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "\n",
    "    return λ_current, u_current\n",
    "\n",
    "# =========================\n",
    "# Test Matrices\n",
    "# =========================\n",
    "def generate_test_matrices(n, matrix_type='laplacian'):\n",
    "    \"\"\"\n",
    "    Generate test matrix pairs (K, M) for eigenvalue problems.\n",
    "    \"\"\"\n",
    "    if matrix_type == 'laplacian':\n",
    "        K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.eye(n)\n",
    "    elif matrix_type == 'tridiagonal':\n",
    "        K = np.diag(2 + 0.1 * np.arange(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.diag(1 + 0.01 * np.arange(n))\n",
    "    elif matrix_type == 'random_spd':\n",
    "        np.random.seed(42)\n",
    "        A = np.random.randn(n, n)\n",
    "        B = np.random.randn(n, n)\n",
    "        K = A @ A.T + n * np.eye(n)\n",
    "        M = B @ B.T + n * np.eye(n)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {matrix_type}\")\n",
    "    return K, M\n",
    "\n",
    "# =========================\n",
    "# Verification\n",
    "# =========================\n",
    "def verify_eigenpairs(λ, u, K, M, name=\"\"):\n",
    "    \"\"\"\n",
    "    Verify quality of computed eigenpairs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name} Verification:\")\n",
    "    n_eig = len(λ)\n",
    "    for i in range(n_eig):\n",
    "        res = np.linalg.norm(K @ u[:, i] - λ[i] * M @ u[:, i])\n",
    "        norm = u[:, i].T @ M @ u[:, i]\n",
    "        print(f\"  λ[{i}]={λ[i]:.6f}, residual={res:.2e}, norm={norm:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# Example Usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = get_device()\n",
    "    \n",
    "    # Generate test problem\n",
    "    n = 4096\n",
    "    K, M = generate_test_matrices(n, matrix_type='laplacian')\n",
    "    \n",
    "    # Solve using hierarchical method\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HIERARCHICAL EIGENSOLVE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    λ_hier, u_hier = hierarchical_eigensolve(K, M, n_eigenpairs=10, device=device)\n",
    "    verify_eigenpairs(λ_hier, u_hier, K, M, name=\"Hierarchical\")\n",
    "    \n",
    "    # Compare with direct solver\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIRECT EIGENSOLVE (for comparison)\")\n",
    "    print(\"=\" * 60)\n",
    "    λ_direct, u_direct = eigh(K, M)\n",
    "    λ_direct = λ_direct[:10]\n",
    "    u_direct = u_direct[:, :10]\n",
    "    verify_eigenpairs(λ_direct, u_direct, K, M, name=\"Direct\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Eigenvalue errors:\")\n",
    "    for i in range(10):\n",
    "        err = abs(λ_hier[i] - λ_direct[i]) / λ_direct[i]\n",
    "        print(f\"  Mode {i}: λ_hier={λ_hier[i]:.6f}, λ_direct={λ_direct[i]:.6f}, rel_err={err:.2e}\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4309c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BUNNY TEST (n = 2503) =====\n",
      "Computing eigen values\n",
      "Built level 0: 1024 DOFs\n",
      "Built level 1: 2503 DOFs\n",
      "\n",
      "Solving coarsest level...\n",
      "Coarsest eigenvalues: [49.23205092 50.75561199 51.81860899 54.6515191  54.8980312 ]\n",
      "\n",
      "--- Refining from level 0 to level 1 ---\n",
      "Training upscaler: 1024 → 2503 (level 0 → 1)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=49.233051, residual=1.11e-02\n",
      "    Epoch 100: Loss=2.44e-02, λ=49.285015, residual=2.40e-02\n",
      "    Epoch 200: Loss=1.13e-02, λ=49.280731, residual=1.10e-02\n",
      "    Epoch 300: Loss=7.64e-03, λ=49.253986, residual=7.30e-03\n",
      "    Epoch 400: Loss=5.59e-03, λ=49.218933, residual=5.25e-03\n",
      "    Epoch 500: Loss=4.18e-03, λ=49.183060, residual=3.82e-03\n",
      "    Epoch 600: Loss=3.15e-03, λ=49.150063, residual=2.78e-03\n",
      "    Epoch 700: Loss=2.39e-03, λ=49.121319, residual=2.01e-03\n",
      "    Epoch 800: Loss=1.82e-03, λ=49.096901, residual=1.46e-03\n",
      "    Epoch 900: Loss=1.38e-03, λ=49.076340, residual=1.06e-03\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=50.756611, residual=1.01e-02\n",
      "    Epoch 100: Loss=3.13e-02, λ=50.804802, residual=3.07e-02\n",
      "    Epoch 200: Loss=1.40e-02, λ=50.806156, residual=1.37e-02\n",
      "    Epoch 300: Loss=9.41e-03, λ=50.785835, residual=9.06e-03\n",
      "    Epoch 400: Loss=6.94e-03, λ=50.757877, residual=6.56e-03\n",
      "    Epoch 500: Loss=5.27e-03, λ=50.728905, residual=4.87e-03\n",
      "    Epoch 600: Loss=4.05e-03, λ=50.702179, residual=3.63e-03\n",
      "    Epoch 700: Loss=3.12e-03, λ=50.679096, residual=2.68e-03\n",
      "    Epoch 800: Loss=2.37e-03, λ=50.660172, residual=1.94e-03\n",
      "    Epoch 900: Loss=1.74e-03, λ=50.645370, residual=1.38e-03\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=51.819607, residual=9.11e-03\n",
      "    Epoch 100: Loss=3.09e-02, λ=51.876686, residual=2.99e-02\n",
      "    Epoch 200: Loss=1.26e-02, λ=51.877274, residual=1.22e-02\n",
      "    Epoch 300: Loss=8.20e-03, λ=51.853157, residual=7.85e-03\n",
      "    Epoch 400: Loss=5.89e-03, λ=51.821308, residual=5.53e-03\n",
      "    Epoch 500: Loss=4.28e-03, λ=51.789177, residual=3.90e-03\n",
      "    Epoch 600: Loss=3.07e-03, λ=51.760132, residual=2.69e-03\n",
      "    Epoch 700: Loss=2.18e-03, λ=51.735352, residual=1.81e-03\n",
      "    Epoch 800: Loss=1.56e-03, λ=51.714546, residual=1.19e-03\n",
      "    Epoch 900: Loss=1.11e-03, λ=51.696789, residual=7.83e-04\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=54.652519, residual=1.06e-02\n",
      "    Epoch 100: Loss=2.74e-02, λ=54.683083, residual=2.70e-02\n",
      "    Epoch 200: Loss=1.34e-02, λ=54.646912, residual=1.30e-02\n",
      "    Epoch 300: Loss=9.19e-03, λ=54.596550, residual=8.82e-03\n",
      "    Epoch 400: Loss=6.71e-03, λ=54.549698, residual=6.33e-03\n",
      "    Epoch 500: Loss=5.02e-03, λ=54.510822, residual=4.61e-03\n",
      "    Epoch 600: Loss=3.83e-03, λ=54.480270, residual=3.39e-03\n",
      "    Epoch 700: Loss=2.95e-03, λ=54.456974, residual=2.50e-03\n",
      "    Epoch 800: Loss=2.25e-03, λ=54.439537, residual=1.83e-03\n",
      "    Epoch 900: Loss=1.68e-03, λ=54.426605, residual=1.32e-03\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.76e+00, λ=54.899029, residual=1.15e-02\n",
      "    Epoch 100: Loss=1.46e-02, λ=54.938625, residual=1.42e-02\n",
      "    Epoch 200: Loss=7.14e-03, λ=54.913322, residual=6.80e-03\n",
      "    Epoch 300: Loss=3.97e-03, λ=54.881790, residual=3.62e-03\n",
      "    Epoch 400: Loss=2.18e-03, λ=54.860153, residual=1.83e-03\n",
      "    Epoch 500: Loss=1.29e-03, λ=54.850891, residual=9.41e-04\n",
      "    Epoch 600: Loss=8.92e-04, λ=54.849995, residual=5.47e-04\n",
      "    Epoch 700: Loss=6.95e-04, λ=54.852203, residual=3.54e-04\n",
      "    Epoch 800: Loss=5.76e-04, λ=54.854874, residual=2.42e-04\n",
      "    Epoch 900: Loss=4.85e-04, λ=54.857300, residual=1.70e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.81e+00, λ=57.459797, residual=8.75e-03\n",
      "    Epoch 100: Loss=1.86e-02, λ=57.497337, residual=1.83e-02\n",
      "    Epoch 200: Loss=8.92e-03, λ=57.465637, residual=8.57e-03\n",
      "    Epoch 300: Loss=5.13e-03, λ=57.432652, residual=4.78e-03\n",
      "    Epoch 400: Loss=3.23e-03, λ=57.409912, residual=2.87e-03\n",
      "    Epoch 500: Loss=2.28e-03, λ=57.394821, residual=1.92e-03\n",
      "    Epoch 600: Loss=1.77e-03, λ=57.383896, residual=1.41e-03\n",
      "    Epoch 700: Loss=1.45e-03, λ=57.375431, residual=1.08e-03\n",
      "    Epoch 800: Loss=1.19e-03, λ=57.369137, residual=8.34e-04\n",
      "    Epoch 900: Loss=9.67e-04, λ=57.365208, residual=6.35e-04\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=59.615185, residual=9.44e-03\n",
      "    Epoch 100: Loss=2.76e-02, λ=59.651363, residual=2.72e-02\n",
      "    Epoch 200: Loss=1.36e-02, λ=59.618710, residual=1.32e-02\n",
      "    Epoch 300: Loss=9.24e-03, λ=59.566452, residual=8.88e-03\n",
      "    Epoch 400: Loss=6.74e-03, λ=59.513699, residual=6.36e-03\n",
      "    Epoch 500: Loss=5.08e-03, λ=59.465706, residual=4.67e-03\n",
      "    Epoch 600: Loss=3.91e-03, λ=59.423729, residual=3.47e-03\n",
      "    Epoch 700: Loss=3.02e-03, λ=59.388016, residual=2.57e-03\n",
      "    Epoch 800: Loss=2.31e-03, λ=59.358635, residual=1.87e-03\n",
      "    Epoch 900: Loss=1.70e-03, λ=59.335388, residual=1.33e-03\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=63.206741, residual=1.15e-02\n",
      "    Epoch 100: Loss=9.35e-03, λ=63.163528, residual=9.00e-03\n",
      "    Epoch 200: Loss=2.82e-03, λ=63.110481, residual=2.45e-03\n",
      "    Epoch 300: Loss=9.92e-04, λ=63.091511, residual=6.26e-04\n",
      "    Epoch 400: Loss=6.46e-04, λ=63.082741, residual=2.87e-04\n",
      "    Epoch 500: Loss=5.31e-04, λ=63.072723, residual=1.73e-04\n",
      "    Epoch 600: Loss=4.69e-04, λ=63.064640, residual=1.17e-04\n",
      "    Epoch 700: Loss=4.33e-04, λ=63.060169, residual=8.82e-05\n",
      "    Epoch 800: Loss=4.13e-04, λ=63.059437, residual=7.28e-05\n",
      "    Epoch 900: Loss=3.67e-04, λ=63.061806, residual=5.59e-05\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=65.118729, residual=1.01e-02\n",
      "    Epoch 100: Loss=2.79e-02, λ=65.163216, residual=2.70e-02\n",
      "    Epoch 200: Loss=1.32e-02, λ=65.135445, residual=1.29e-02\n",
      "    Epoch 300: Loss=8.55e-03, λ=65.098244, residual=8.17e-03\n",
      "    Epoch 400: Loss=5.94e-03, λ=65.067780, residual=5.55e-03\n",
      "    Epoch 500: Loss=4.30e-03, λ=65.045235, residual=3.89e-03\n",
      "    Epoch 600: Loss=3.22e-03, λ=65.028175, residual=2.80e-03\n",
      "    Epoch 700: Loss=2.47e-03, λ=65.013817, residual=2.05e-03\n",
      "    Epoch 800: Loss=1.91e-03, λ=65.000084, residual=1.51e-03\n",
      "    Epoch 900: Loss=1.47e-03, λ=64.986351, residual=1.11e-03\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=65.454506, residual=1.23e-02\n",
      "    Epoch 100: Loss=1.01e-02, λ=65.434975, residual=9.73e-03\n",
      "    Epoch 200: Loss=2.97e-03, λ=65.405548, residual=2.60e-03\n",
      "    Epoch 300: Loss=1.35e-03, λ=65.411804, residual=9.85e-04\n",
      "    Epoch 400: Loss=8.66e-04, λ=65.425331, residual=5.03e-04\n",
      "    Epoch 500: Loss=6.61e-04, λ=65.431808, residual=3.07e-04\n",
      "    Epoch 600: Loss=5.41e-04, λ=65.430519, residual=1.96e-04\n",
      "    Epoch 700: Loss=4.73e-04, λ=65.424034, residual=1.36e-04\n",
      "    Epoch 800: Loss=4.37e-04, λ=65.414604, residual=9.66e-05\n",
      "    Epoch 900: Loss=3.77e-04, λ=65.403488, residual=6.70e-05\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [47.025356 49.5391   49.842953 53.669647 55.164696]\n",
      "\n",
      "Bunny test Verification:\n",
      "  λ[0]=47.025356, residual=1.69e+00, norm=1.000000\n",
      "  λ[1]=49.539101, residual=2.07e+00, norm=1.000000\n",
      "  λ[2]=49.842953, residual=1.35e+00, norm=1.000000\n",
      "  λ[3]=53.669647, residual=2.03e+00, norm=1.000000\n",
      "  λ[4]=55.164696, residual=6.44e-01, norm=1.000000\n",
      "  λ[5]=57.550709, residual=1.31e+00, norm=1.000000\n",
      "  λ[6]=57.624187, residual=2.07e+00, norm=1.000000\n",
      "  λ[7]=63.459072, residual=3.84e-01, norm=1.000000\n",
      "  λ[8]=63.953407, residual=1.77e+00, norm=1.000000\n",
      "  λ[9]=64.781006, residual=3.97e-01, norm=1.000000\n",
      "✓ Bunny test done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== BUNNY TEST (n = 2503) =====\")\n",
    "from Mesh import Mesh\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "K, M = m.computeLaplacian()\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = eigh(K,M)\n",
    "\n",
    "#levels = {0: 256, 1: 512, 2: 1024, 3: 2503}\n",
    "levels = {0: 1024, 1: 2503}\n",
    "λ_hier, u_hier = hierarchical_eigensolve(K, M, n_eigenpairs=10, levels=levels, device=device)\n",
    "verify_eigenpairs(λ_hier, u_hier, K, M, \"Bunny test\")\n",
    "print(\"✓ Bunny test done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c210ee0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.025356, 49.5391  , 49.842953, 53.669647, 55.164696, 57.55071 ,\n",
       "       57.624187, 63.459072, 63.953407, 64.781006], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ_hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42bdd4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.78671343e-15, 1.60038744e-01, 4.25258130e-01, 4.38250633e-01,\n",
       "       5.38463815e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "079f25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import eigh, solve\n",
    "\n",
    "# =========================\n",
    "# Device Configuration\n",
    "# =========================\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (CUDA if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "# =========================\n",
    "# Hierarchy Construction\n",
    "# =========================\n",
    "def build_hierarchy(K_full, M_full, levels=None, method='uniform', preserve_boundary=True):\n",
    "    \"\"\"\n",
    "    Build a hierarchy of downsampled matrices.\n",
    "    For meshes with null eigenvalues, this can cause spectrum shift.\n",
    "    \"\"\"\n",
    "    if levels is None:\n",
    "        levels = {0: 32, 1: 256, 2: 2048, 3: K_full.shape[0]}\n",
    "\n",
    "    n_full = K_full.shape[0]\n",
    "    K_hierarchy, M_hierarchy, idx_hierarchy = {}, {}, {}\n",
    "\n",
    "    for level, size in sorted(levels.items()):\n",
    "        if size > n_full:\n",
    "            size = n_full\n",
    "\n",
    "        if method == 'uniform':\n",
    "            idx = np.linspace(0, n_full - 1, size, dtype=int)\n",
    "        elif method == 'random':\n",
    "            np.random.seed(42 + level)\n",
    "            idx = np.sort(np.random.choice(n_full, size, replace=False))\n",
    "        elif method == 'maxdist':\n",
    "            # Farthest point sampling for better coverage\n",
    "            idx = farthest_point_sampling(K_full, size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        K_hierarchy[level] = K_full[np.ix_(idx, idx)]\n",
    "        M_hierarchy[level] = M_full[np.ix_(idx, idx)]\n",
    "        idx_hierarchy[level] = idx\n",
    "\n",
    "        print(f\"Built level {level}: {size} DOFs\")\n",
    "\n",
    "    return K_hierarchy, M_hierarchy, idx_hierarchy\n",
    "\n",
    "def farthest_point_sampling(K, n_samples):\n",
    "    \"\"\"\n",
    "    Select points using farthest point sampling (better for mesh downsampling).\n",
    "    Uses the mass matrix structure as a proxy for distance.\n",
    "    \"\"\"\n",
    "    n = K.shape[0]\n",
    "    if n_samples >= n:\n",
    "        return np.arange(n)\n",
    "    \n",
    "    indices = [0]\n",
    "    distances = np.full(n, np.inf)\n",
    "    \n",
    "    for _ in range(n_samples - 1):\n",
    "        last_idx = indices[-1]\n",
    "        # Update distances (simple heuristic using row norms)\n",
    "        new_dist = np.abs(K[last_idx, :])\n",
    "        distances = np.minimum(distances, new_dist)\n",
    "        distances[indices] = -np.inf\n",
    "        indices.append(np.argmax(distances))\n",
    "    \n",
    "    return np.sort(np.array(indices))\n",
    "\n",
    "# =========================\n",
    "# Coarse → Fine Projection\n",
    "# =========================\n",
    "def project_coarse_to_fine(u_coarse, idx_coarse, n_fine, method='linear'):\n",
    "    \"\"\"\n",
    "    Project coarse vector to fine mesh via simple linear mapping.\n",
    "    \"\"\"\n",
    "    u_fine = np.zeros(n_fine, dtype=float)\n",
    "    u_fine[idx_coarse] = u_coarse\n",
    "    return u_fine\n",
    "\n",
    "# =========================\n",
    "# Neural Upscaler\n",
    "# =========================\n",
    "class HierarchicalUpscaler(nn.Module):\n",
    "    def __init__(self, n_coarse, n_fine, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = max(min(n_coarse * hidden_factor, n_fine // 2), 32)\n",
    "        # Input: n_coarse, output: n_fine\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_coarse, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, n_fine)\n",
    "        )\n",
    "\n",
    "    def forward(self, u_coarse):\n",
    "        \"\"\"\n",
    "        u_coarse: (n_coarse,) or (batch, n_coarse)\n",
    "        Returns: (n_fine,) or (batch, n_fine)\n",
    "        \"\"\"\n",
    "        if u_coarse.dim() == 1:\n",
    "            return self.net(u_coarse.unsqueeze(0)).squeeze(0)\n",
    "        else:\n",
    "            return self.net(u_coarse)\n",
    "\n",
    "# =========================\n",
    "# Loss Function\n",
    "# =========================\n",
    "def compute_loss(u_fine, λ, K_fine, M_fine, u_previous=None, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {'residual': 1.0, 'normalization': 10.0, 'orthogonality': 5.0, 'smoothness': 0.01}\n",
    "\n",
    "    losses = {}\n",
    "    Ku = K_fine @ u_fine\n",
    "    Mu = M_fine @ u_fine\n",
    "\n",
    "    # Residual: (K - λM)u ≈ 0\n",
    "    losses['residual'] = torch.mean((Ku - λ * Mu) ** 2)\n",
    "    \n",
    "    # Normalization: u^T M u = 1\n",
    "    losses['normalization'] = ((u_fine.T @ Mu) - 1.0) ** 2\n",
    "\n",
    "    # Orthogonality to previous eigenvectors\n",
    "    losses['orthogonality'] = torch.tensor(0.0, device=u_fine.device, dtype=u_fine.dtype)\n",
    "    if u_previous is not None and len(u_previous) > 0:\n",
    "        for u_prev in u_previous:\n",
    "            overlap = (u_fine.T @ (M_fine @ u_prev))\n",
    "            losses['orthogonality'] += overlap ** 2\n",
    "\n",
    "    # Smoothness regularization\n",
    "    losses['smoothness'] = torch.mean((u_fine[1:] - u_fine[:-1]) ** 2)\n",
    "    \n",
    "    total = sum(weights[k] * losses[k] for k in losses)\n",
    "    return total, losses\n",
    "\n",
    "def adaptive_weights(epoch, losses_history=None):\n",
    "    return {\n",
    "        'residual': 1.0,\n",
    "        'normalization': 10.0 * np.exp(-epoch / 100),\n",
    "        'orthogonality': 5.0,\n",
    "        'smoothness': 0.01\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Upscaler Training\n",
    "# =========================\n",
    "def train_upscaler(level_from, level_to, λ_coarse, u_coarse, K_fine, M_fine, idx_coarse, \n",
    "                   n_eigenpairs=10, n_epochs=1000, lr=1e-3, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    n_coarse = u_coarse.shape[0]\n",
    "    n_fine = K_fine.shape[0]\n",
    "\n",
    "    print(f\"Training upscaler: {n_coarse} → {n_fine} (level {level_from} → {level_to})\")\n",
    "\n",
    "    λ_fine = torch.zeros(n_eigenpairs)\n",
    "    u_fine_all = []\n",
    "\n",
    "    # Move matrices to GPU\n",
    "    K_t = torch.tensor(K_fine, dtype=torch.float32, device=device)\n",
    "    M_t = torch.tensor(M_fine, dtype=torch.float32, device=device)\n",
    "\n",
    "    for i in range(n_eigenpairs):\n",
    "        print(f\"  Eigenpair {i+1}/{n_eigenpairs}\")\n",
    "\n",
    "        # Initialize eigenvalue from coarse solution\n",
    "        λ_init = λ_coarse[i]\n",
    "        \n",
    "        # Convert coarse eigenvector to tensor and move to GPU\n",
    "        u_c = torch.tensor(u_coarse[:, i], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Initialize eigenvalue as trainable parameter on GPU\n",
    "        λ = torch.tensor(λ_init, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "        # Create and move upscaler model to GPU\n",
    "        model = HierarchicalUpscaler(n_coarse, n_fine).to(device)\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + [λ], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.5)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Upscale coarse eigenvector to fine mesh\n",
    "            u_f = model(u_c)\n",
    "            \n",
    "            # Compute loss with adaptive weights\n",
    "            weights = adaptive_weights(epoch)\n",
    "            loss, loss_dict = compute_loss(u_f, λ, K_t, M_t, u_previous=u_fine_all, weights=weights)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "            # Logging\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"    Epoch {epoch}: Loss={loss.item():.2e}, λ={λ.item():.6f}, \"\n",
    "                      f\"residual={loss_dict['residual'].item():.2e}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter > 200:\n",
    "                    print(f\"    Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        # Store results (move back to CPU for storage)\n",
    "        λ_fine[i] = λ.detach().cpu()\n",
    "        u_fine_all.append(u_f.detach())  # Keep on GPU for next iteration\n",
    "\n",
    "    # Convert to numpy (move to CPU)\n",
    "    u_fine_matrix = torch.stack(u_fine_all, dim=1).cpu().numpy()\n",
    "    return λ_fine.numpy(), u_fine_matrix\n",
    "\n",
    "# =========================\n",
    "# Simple Rayleigh–Ritz Refinement\n",
    "# =========================\n",
    "def simple_rayleigh_ritz_refinement(λ_init, u_init, K, M, n_iter=3):\n",
    "    \"\"\"\n",
    "    Refine eigenpairs using Rayleigh quotient iteration and Gram-Schmidt orthogonalization.\n",
    "    \"\"\"\n",
    "    λ_refined = λ_init.copy()\n",
    "    u_refined = u_init.copy()\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i in range(len(λ_init)):\n",
    "            u = u_refined[:, i]\n",
    "            Ku = K @ u\n",
    "            Mu = M @ u\n",
    "            \n",
    "            # Update eigenvalue using Rayleigh quotient\n",
    "            λ_refined[i] = (u.T @ Ku) / (u.T @ Mu)\n",
    "\n",
    "            # Orthogonalize against previous eigenvectors\n",
    "            for j in range(i):\n",
    "                overlap = u.T @ M @ u_refined[:, j]\n",
    "                u -= overlap * u_refined[:, j]\n",
    "\n",
    "            # Normalize\n",
    "            norm = np.sqrt(u.T @ M @ u)\n",
    "            if norm > 1e-10:\n",
    "                u_refined[:, i] = u / norm\n",
    "            else:\n",
    "                print(f\"Warning: Near-zero norm at iteration {it}, eigenvector {i}\")\n",
    "\n",
    "    return λ_refined, u_refined\n",
    "\n",
    "# =========================\n",
    "# Hierarchical Eigen Solve\n",
    "# =========================\n",
    "def hierarchical_eigensolve(K_full, M_full, n_eigenpairs=10, method='uniform', levels=None, device=None, \n",
    "                           skip_null_modes=0, regularization=1e-8):\n",
    "    \"\"\"\n",
    "    Solve generalized eigenvalue problem K u = λ M u using hierarchical neural upscaling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    skip_null_modes : int\n",
    "        Number of near-zero eigenvalues to skip (e.g., rigid body modes)\n",
    "    regularization : float\n",
    "        Small value added to K diagonal for stability\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    # Add small regularization to avoid numerical issues with null modes\n",
    "    K_reg = K_full + regularization * M_full\n",
    "    \n",
    "    # Build hierarchy of coarse meshes\n",
    "    K_h, M_h, idx_h = build_hierarchy(K_reg, M_full, levels=levels, method=method)\n",
    "    level_keys = sorted(K_h.keys())\n",
    "\n",
    "    # Solve coarsest level exactly (on CPU with scipy)\n",
    "    print(\"\\nSolving coarsest level...\")\n",
    "    K_coarse = K_h[level_keys[0]]\n",
    "    M_coarse = M_h[level_keys[0]]\n",
    "    \n",
    "    # Request more modes to account for skipped ones\n",
    "    n_modes_coarse = n_eigenpairs + skip_null_modes\n",
    "    λ_all, u_all = eigh(K_coarse, M_coarse)\n",
    "    \n",
    "    # Skip null/near-zero modes\n",
    "    if skip_null_modes > 0:\n",
    "        print(f\"Skipping first {skip_null_modes} modes (null/rigid body modes)\")\n",
    "        print(f\"Skipped eigenvalues: {λ_all[:skip_null_modes]}\")\n",
    "        λ_current = λ_all[skip_null_modes:skip_null_modes + n_eigenpairs]\n",
    "        u_current = u_all[:, skip_null_modes:skip_null_modes + n_eigenpairs]\n",
    "    else:\n",
    "        λ_current = λ_all[:n_eigenpairs]\n",
    "        u_current = u_all[:, :n_eigenpairs]\n",
    "    \n",
    "    print(f\"Using eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "\n",
    "    # Progressive refinement through hierarchy\n",
    "    for i in range(1, len(level_keys)):\n",
    "        level_from = level_keys[i-1]\n",
    "        level_to = level_keys[i]\n",
    "\n",
    "        print(f\"\\n--- Refining from level {level_from} to level {level_to} ---\")\n",
    "        \n",
    "        # Neural upscaling (on GPU)\n",
    "        λ_current, u_current = train_upscaler(\n",
    "            level_from, level_to,\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            idx_coarse=idx_h[level_from],\n",
    "            n_eigenpairs=n_eigenpairs,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Post-refinement with Rayleigh-Ritz (on CPU)\n",
    "        print(f\"  Applying Rayleigh-Ritz refinement...\")\n",
    "        λ_current, u_current = simple_rayleigh_ritz_refinement(\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            n_iter=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  Refined eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "    \n",
    "    # Remove regularization from final eigenvalues\n",
    "    λ_current = λ_current - regularization\n",
    "\n",
    "    return λ_current, u_current\n",
    "\n",
    "# =========================\n",
    "# Test Matrices\n",
    "# =========================\n",
    "def generate_test_matrices(n, matrix_type='laplacian'):\n",
    "    \"\"\"\n",
    "    Generate test matrix pairs (K, M) for eigenvalue problems.\n",
    "    \"\"\"\n",
    "    if matrix_type == 'laplacian':\n",
    "        K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.eye(n)\n",
    "    elif matrix_type == 'tridiagonal':\n",
    "        K = np.diag(2 + 0.1 * np.arange(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.diag(1 + 0.01 * np.arange(n))\n",
    "    elif matrix_type == 'random_spd':\n",
    "        np.random.seed(42)\n",
    "        A = np.random.randn(n, n)\n",
    "        B = np.random.randn(n, n)\n",
    "        K = A @ A.T + n * np.eye(n)\n",
    "        M = B @ B.T + n * np.eye(n)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {matrix_type}\")\n",
    "    return K, M\n",
    "\n",
    "# =========================\n",
    "# Verification\n",
    "# =========================\n",
    "def verify_eigenpairs(λ, u, K, M, name=\"\"):\n",
    "    \"\"\"\n",
    "    Verify quality of computed eigenpairs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name} Verification:\")\n",
    "    n_eig = len(λ)\n",
    "    for i in range(n_eig):\n",
    "        res = np.linalg.norm(K @ u[:, i] - λ[i] * M @ u[:, i])\n",
    "        norm = u[:, i].T @ M @ u[:, i]\n",
    "        print(f\"  λ[{i}]={λ[i]:.6f}, residual={res:.2e}, norm={norm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b3c3803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BUNNY TEST (n = 2503) =====\n",
      "Computing eigen values\n",
      "Built level 0: 1024 DOFs\n",
      "Built level 1: 2503 DOFs\n",
      "\n",
      "Solving coarsest level...\n",
      "Skipping first 1 modes (null/rigid body modes)\n",
      "Skipped eigenvalues: [49.23205192]\n",
      "Using eigenvalues: [50.75561299 51.81860999 54.6515201  54.8980322  57.45879678]\n",
      "\n",
      "--- Refining from level 0 to level 1 ---\n",
      "Training upscaler: 1024 → 2503 (level 0 → 1)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=50.756611, residual=1.02e-02\n",
      "    Epoch 100: Loss=3.25e-02, λ=50.793591, residual=3.14e-02\n",
      "    Epoch 200: Loss=1.44e-02, λ=50.774284, residual=1.41e-02\n",
      "    Epoch 300: Loss=9.75e-03, λ=50.731632, residual=9.39e-03\n",
      "    Epoch 400: Loss=7.24e-03, λ=50.681938, residual=6.87e-03\n",
      "    Epoch 500: Loss=5.57e-03, λ=50.632553, residual=5.16e-03\n",
      "    Epoch 600: Loss=4.33e-03, λ=50.586979, residual=3.90e-03\n",
      "    Epoch 700: Loss=3.36e-03, λ=50.546860, residual=2.91e-03\n",
      "    Epoch 800: Loss=2.54e-03, λ=50.513058, residual=2.12e-03\n",
      "    Epoch 900: Loss=1.86e-03, λ=50.485706, residual=1.51e-03\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=51.819611, residual=9.54e-03\n",
      "    Epoch 100: Loss=3.37e-02, λ=51.877384, residual=3.25e-02\n",
      "    Epoch 200: Loss=1.41e-02, λ=51.884335, residual=1.37e-02\n",
      "    Epoch 300: Loss=9.02e-03, λ=51.865601, residual=8.66e-03\n",
      "    Epoch 400: Loss=6.33e-03, λ=51.835751, residual=5.96e-03\n",
      "    Epoch 500: Loss=4.54e-03, λ=51.802677, residual=4.16e-03\n",
      "    Epoch 600: Loss=3.30e-03, λ=51.770710, residual=2.91e-03\n",
      "    Epoch 700: Loss=2.42e-03, λ=51.742046, residual=2.03e-03\n",
      "    Epoch 800: Loss=1.79e-03, λ=51.717484, residual=1.42e-03\n",
      "    Epoch 900: Loss=1.32e-03, λ=51.697044, residual=9.96e-04\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=54.652519, residual=9.45e-03\n",
      "    Epoch 100: Loss=2.75e-02, λ=54.675911, residual=2.71e-02\n",
      "    Epoch 200: Loss=1.33e-02, λ=54.626659, residual=1.30e-02\n",
      "    Epoch 300: Loss=9.16e-03, λ=54.560326, residual=8.79e-03\n",
      "    Epoch 400: Loss=6.88e-03, λ=54.494663, residual=6.49e-03\n",
      "    Epoch 500: Loss=5.38e-03, λ=54.433681, residual=4.95e-03\n",
      "    Epoch 600: Loss=4.27e-03, λ=54.378613, residual=3.80e-03\n",
      "    Epoch 700: Loss=3.36e-03, λ=54.330471, residual=2.87e-03\n",
      "    Epoch 800: Loss=2.56e-03, λ=54.290195, residual=2.09e-03\n",
      "    Epoch 900: Loss=1.87e-03, λ=54.258163, residual=1.48e-03\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.75e+00, λ=54.899033, residual=1.09e-02\n",
      "    Epoch 100: Loss=1.31e-02, λ=54.924873, residual=1.28e-02\n",
      "    Epoch 200: Loss=5.65e-03, λ=54.872692, residual=5.33e-03\n",
      "    Epoch 300: Loss=2.60e-03, λ=54.825268, residual=2.28e-03\n",
      "    Epoch 400: Loss=1.30e-03, λ=54.797260, residual=9.85e-04\n",
      "    Epoch 500: Loss=8.12e-04, λ=54.778503, residual=4.94e-04\n",
      "    Epoch 600: Loss=6.14e-04, λ=54.763474, residual=2.99e-04\n",
      "    Epoch 700: Loss=5.16e-04, λ=54.751602, residual=2.05e-04\n",
      "    Epoch 800: Loss=4.52e-04, λ=54.742512, residual=1.49e-04\n",
      "    Epoch 900: Loss=3.97e-04, λ=54.735600, residual=1.09e-04\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=57.459797, residual=1.05e-02\n",
      "    Epoch 100: Loss=1.57e-02, λ=57.481415, residual=1.54e-02\n",
      "    Epoch 200: Loss=7.31e-03, λ=57.432831, residual=6.97e-03\n",
      "    Epoch 300: Loss=4.12e-03, λ=57.389908, residual=3.78e-03\n",
      "    Epoch 400: Loss=2.63e-03, λ=57.356319, residual=2.29e-03\n",
      "    Epoch 500: Loss=1.87e-03, λ=57.327530, residual=1.52e-03\n",
      "    Epoch 600: Loss=1.44e-03, λ=57.301331, residual=1.09e-03\n",
      "    Epoch 700: Loss=1.17e-03, λ=57.276596, residual=8.09e-04\n",
      "    Epoch 800: Loss=9.53e-04, λ=57.253025, residual=6.02e-04\n",
      "    Epoch 900: Loss=7.71e-04, λ=57.231117, residual=4.41e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=59.615185, residual=1.03e-02\n",
      "    Epoch 100: Loss=3.13e-02, λ=59.664951, residual=3.07e-02\n",
      "    Epoch 200: Loss=1.45e-02, λ=59.649590, residual=1.41e-02\n",
      "    Epoch 300: Loss=9.46e-03, λ=59.610779, residual=9.08e-03\n",
      "    Epoch 400: Loss=6.84e-03, λ=59.568478, residual=6.45e-03\n",
      "    Epoch 500: Loss=5.19e-03, λ=59.528667, residual=4.79e-03\n",
      "    Epoch 600: Loss=4.06e-03, λ=59.492912, residual=3.62e-03\n",
      "    Epoch 700: Loss=3.19e-03, λ=59.461731, residual=2.74e-03\n",
      "    Epoch 800: Loss=2.47e-03, λ=59.435471, residual=2.03e-03\n",
      "    Epoch 900: Loss=1.85e-03, λ=59.414249, residual=1.47e-03\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=63.206741, residual=1.01e-02\n",
      "    Epoch 100: Loss=1.16e-02, λ=63.200787, residual=1.12e-02\n",
      "    Epoch 200: Loss=3.84e-03, λ=63.151794, residual=3.47e-03\n",
      "    Epoch 300: Loss=1.14e-03, λ=63.131462, residual=7.67e-04\n",
      "    Epoch 400: Loss=6.61e-04, λ=63.125168, residual=2.96e-04\n",
      "    Epoch 500: Loss=5.37e-04, λ=63.122005, residual=1.77e-04\n",
      "    Epoch 600: Loss=4.85e-04, λ=63.117764, residual=1.31e-04\n",
      "    Epoch 700: Loss=4.44e-04, λ=63.112602, residual=1.01e-04\n",
      "    Epoch 800: Loss=4.24e-04, λ=63.106934, residual=8.64e-05\n",
      "    Epoch 900: Loss=3.69e-04, λ=63.100880, residual=6.22e-05\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=65.118736, residual=1.00e-02\n",
      "    Epoch 100: Loss=2.31e-02, λ=65.160149, residual=2.26e-02\n",
      "    Epoch 200: Loss=1.08e-02, λ=65.122833, residual=1.04e-02\n",
      "    Epoch 300: Loss=6.61e-03, λ=65.076859, residual=6.23e-03\n",
      "    Epoch 400: Loss=4.40e-03, λ=65.037102, residual=4.01e-03\n",
      "    Epoch 500: Loss=3.14e-03, λ=65.002655, residual=2.75e-03\n",
      "    Epoch 600: Loss=2.36e-03, λ=64.971306, residual=1.97e-03\n",
      "    Epoch 700: Loss=1.83e-03, λ=64.942268, residual=1.44e-03\n",
      "    Epoch 800: Loss=1.44e-03, λ=64.915794, residual=1.06e-03\n",
      "    Epoch 900: Loss=1.12e-03, λ=64.892456, residual=7.79e-04\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.76e+00, λ=65.454506, residual=1.25e-02\n",
      "    Epoch 100: Loss=1.21e-02, λ=65.452576, residual=1.17e-02\n",
      "    Epoch 200: Loss=3.81e-03, λ=65.404671, residual=3.44e-03\n",
      "    Epoch 300: Loss=1.69e-03, λ=65.386063, residual=1.32e-03\n",
      "    Epoch 400: Loss=1.04e-03, λ=65.376076, residual=6.66e-04\n",
      "    Epoch 500: Loss=7.63e-04, λ=65.368645, residual=3.91e-04\n",
      "    Epoch 600: Loss=6.25e-04, λ=65.363213, residual=2.56e-04\n",
      "    Epoch 700: Loss=5.47e-04, λ=65.359726, residual=1.83e-04\n",
      "    Epoch 800: Loss=4.96e-04, λ=65.358170, residual=1.42e-04\n",
      "    Epoch 900: Loss=4.42e-04, λ=65.358231, residual=1.09e-04\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=65.583824, residual=1.14e-02\n",
      "    Epoch 100: Loss=1.11e-02, λ=65.583679, residual=1.07e-02\n",
      "    Epoch 200: Loss=3.24e-03, λ=65.536957, residual=2.89e-03\n",
      "    Epoch 300: Loss=1.54e-03, λ=65.512550, residual=1.19e-03\n",
      "    Epoch 400: Loss=9.98e-04, λ=65.497154, residual=6.42e-04\n",
      "    Epoch 500: Loss=7.69e-04, λ=65.484001, residual=4.12e-04\n",
      "    Epoch 600: Loss=6.56e-04, λ=65.471375, residual=3.00e-04\n",
      "    Epoch 700: Loss=5.75e-04, λ=65.460251, residual=2.23e-04\n",
      "    Epoch 800: Loss=5.30e-04, λ=65.451408, residual=1.88e-04\n",
      "    Epoch 900: Loss=5.16e-04, λ=65.445213, residual=1.93e-04\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [47.643097 49.32561  50.786327 54.297787 55.756786]\n",
      "\n",
      "Bunny test Verification:\n",
      "  λ[0]=47.643097, residual=2.14e+00, norm=1.000000\n",
      "  λ[1]=49.325611, residual=1.63e+00, norm=1.000000\n",
      "  λ[2]=50.786327, residual=2.14e+00, norm=1.000000\n",
      "  λ[3]=54.297787, residual=5.14e-01, norm=1.000000\n",
      "  λ[4]=55.756786, residual=1.04e+00, norm=1.000000\n",
      "  λ[5]=57.886600, residual=2.22e+00, norm=1.000000\n",
      "  λ[6]=62.671200, residual=3.96e-01, norm=1.000000\n",
      "  λ[7]=62.814228, residual=1.40e+00, norm=1.000000\n",
      "  λ[8]=65.458038, residual=5.22e-01, norm=1.000000\n",
      "  λ[9]=65.248344, residual=5.44e-01, norm=1.000000\n",
      "✓ Bunny test done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== BUNNY TEST (n = 2503) =====\")\n",
    "from Mesh import Mesh\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "K, M = m.computeLaplacian()\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = eigh(K,M)\n",
    "\n",
    "#levels = {0: 256, 1: 512, 2: 1024, 3: 2503}\n",
    "levels = {0: 1024, 1: 2503}\n",
    "λ_hier, u_hier = hierarchical_eigensolve(\n",
    "    K, M,\n",
    "    n_eigenpairs=10,\n",
    "    levels=levels,\n",
    "    skip_null_modes=1,      # Skip the ~0 eigenvalue\n",
    "    regularization=1e-6,     # Small stabilizing shift\n",
    "    method='uniform',        # or 'maxdist' for better sampling\n",
    "    device=device\n",
    ")\n",
    "verify_eigenpairs(λ_hier, u_hier, K, M, \"Bunny test\")\n",
    "print(\"✓ Bunny test done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f09c649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import eigh, solve\n",
    "\n",
    "# =========================\n",
    "# Device Configuration\n",
    "# =========================\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (CUDA if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "# =========================\n",
    "# Hierarchy Construction\n",
    "# =========================\n",
    "def build_hierarchy(K_full, M_full, levels=None, method='uniform', preserve_boundary=True):\n",
    "    \"\"\"\n",
    "    Build a hierarchy of downsampled matrices.\n",
    "    For meshes with null eigenvalues, this can cause spectrum shift.\n",
    "    \"\"\"\n",
    "    if levels is None:\n",
    "        levels = {0: 32, 1: 256, 2: 2048, 3: K_full.shape[0]}\n",
    "\n",
    "    n_full = K_full.shape[0]\n",
    "    K_hierarchy, M_hierarchy, idx_hierarchy = {}, {}, {}\n",
    "\n",
    "    for level, size in sorted(levels.items()):\n",
    "        if size > n_full:\n",
    "            size = n_full\n",
    "\n",
    "        if method == 'uniform':\n",
    "            idx = np.linspace(0, n_full - 1, size, dtype=int)\n",
    "        elif method == 'random':\n",
    "            np.random.seed(42 + level)\n",
    "            idx = np.sort(np.random.choice(n_full, size, replace=False))\n",
    "        elif method == 'maxdist':\n",
    "            # Farthest point sampling for better coverage\n",
    "            idx = farthest_point_sampling(K_full, size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        K_hierarchy[level] = K_full[np.ix_(idx, idx)]\n",
    "        M_hierarchy[level] = M_full[np.ix_(idx, idx)]\n",
    "        idx_hierarchy[level] = idx\n",
    "\n",
    "        print(f\"Built level {level}: {size} DOFs\")\n",
    "\n",
    "    return K_hierarchy, M_hierarchy, idx_hierarchy\n",
    "\n",
    "def farthest_point_sampling(K, n_samples):\n",
    "    \"\"\"\n",
    "    Select points using farthest point sampling (better for mesh downsampling).\n",
    "    Uses the mass matrix structure as a proxy for distance.\n",
    "    \"\"\"\n",
    "    n = K.shape[0]\n",
    "    if n_samples >= n:\n",
    "        return np.arange(n)\n",
    "    \n",
    "    indices = [0]\n",
    "    distances = np.full(n, np.inf)\n",
    "    \n",
    "    for _ in range(n_samples - 1):\n",
    "        last_idx = indices[-1]\n",
    "        # Update distances (simple heuristic using row norms)\n",
    "        new_dist = np.abs(K[last_idx, :])\n",
    "        distances = np.minimum(distances, new_dist)\n",
    "        distances[indices] = -np.inf\n",
    "        indices.append(np.argmax(distances))\n",
    "    \n",
    "    return np.sort(np.array(indices))\n",
    "\n",
    "# =========================\n",
    "# Coarse → Fine Projection\n",
    "# =========================\n",
    "def project_coarse_to_fine(u_coarse, idx_coarse, n_fine, method='interpolate'):\n",
    "    \"\"\"\n",
    "    Project coarse vector to fine mesh with interpolation.\n",
    "    \"\"\"\n",
    "    u_fine = np.zeros(n_fine, dtype=float)\n",
    "    u_fine[idx_coarse] = u_coarse\n",
    "    \n",
    "    if method == 'interpolate':\n",
    "        # Linear interpolation between coarse points\n",
    "        for i in range(len(idx_coarse) - 1):\n",
    "            i_start = idx_coarse[i]\n",
    "            i_end = idx_coarse[i + 1]\n",
    "            if i_end - i_start > 1:\n",
    "                # Interpolate values between coarse points\n",
    "                u_fine[i_start:i_end+1] = np.linspace(u_coarse[i], u_coarse[i+1], i_end - i_start + 1)\n",
    "    \n",
    "    return u_fine\n",
    "\n",
    "# =========================\n",
    "# Neural Upscaler\n",
    "# =========================\n",
    "class HierarchicalUpscaler(nn.Module):\n",
    "    def __init__(self, n_coarse, n_fine, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = max(min(n_coarse * hidden_factor, n_fine // 2), 32)\n",
    "        # Input: n_coarse, output: n_fine\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_coarse, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, n_fine)\n",
    "        )\n",
    "\n",
    "    def forward(self, u_coarse):\n",
    "        \"\"\"\n",
    "        u_coarse: (n_coarse,) or (batch, n_coarse)\n",
    "        Returns: (n_fine,) or (batch, n_fine)\n",
    "        \"\"\"\n",
    "        if u_coarse.dim() == 1:\n",
    "            return self.net(u_coarse.unsqueeze(0)).squeeze(0)\n",
    "        else:\n",
    "            return self.net(u_coarse)\n",
    "\n",
    "# =========================\n",
    "# Loss Function\n",
    "# =========================\n",
    "def compute_loss(u_fine, λ, K_fine, M_fine, u_previous=None, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {'residual': 1.0, 'normalization': 10.0, 'orthogonality': 5.0, 'smoothness': 0.01}\n",
    "\n",
    "    losses = {}\n",
    "    Ku = K_fine @ u_fine\n",
    "    Mu = M_fine @ u_fine\n",
    "\n",
    "    # Residual: (K - λM)u ≈ 0\n",
    "    losses['residual'] = torch.mean((Ku - λ * Mu) ** 2)\n",
    "    \n",
    "    # Normalization: u^T M u = 1\n",
    "    losses['normalization'] = ((u_fine.T @ Mu) - 1.0) ** 2\n",
    "\n",
    "    # Orthogonality to previous eigenvectors\n",
    "    losses['orthogonality'] = torch.tensor(0.0, device=u_fine.device, dtype=u_fine.dtype)\n",
    "    if u_previous is not None and len(u_previous) > 0:\n",
    "        for u_prev in u_previous:\n",
    "            overlap = (u_fine.T @ (M_fine @ u_prev))\n",
    "            losses['orthogonality'] += overlap ** 2\n",
    "\n",
    "    # Smoothness regularization\n",
    "    losses['smoothness'] = torch.mean((u_fine[1:] - u_fine[:-1]) ** 2)\n",
    "    \n",
    "    total = sum(weights[k] * losses[k] for k in losses)\n",
    "    return total, losses\n",
    "\n",
    "def adaptive_weights(epoch, losses_history=None):\n",
    "    return {\n",
    "        'residual': 1.0,\n",
    "        'normalization': 10.0 * np.exp(-epoch / 100),\n",
    "        'orthogonality': 5.0,\n",
    "        'smoothness': 0.01\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Upscaler Training\n",
    "# =========================\n",
    "def train_upscaler(level_from, level_to, λ_coarse, u_coarse, K_fine, M_fine, idx_coarse, \n",
    "                   n_eigenpairs=10, n_epochs=1000, lr=1e-3, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    n_coarse = u_coarse.shape[0]\n",
    "    n_fine = K_fine.shape[0]\n",
    "\n",
    "    print(f\"Training upscaler: {n_coarse} → {n_fine} (level {level_from} → {level_to})\")\n",
    "\n",
    "    λ_fine = torch.zeros(n_eigenpairs)\n",
    "    u_fine_all = []\n",
    "\n",
    "    # Move matrices to GPU\n",
    "    K_t = torch.tensor(K_fine, dtype=torch.float32, device=device)\n",
    "    M_t = torch.tensor(M_fine, dtype=torch.float32, device=device)\n",
    "\n",
    "    for i in range(n_eigenpairs):\n",
    "        print(f\"  Eigenpair {i+1}/{n_eigenpairs}\")\n",
    "\n",
    "        # Initialize eigenvalue from coarse solution\n",
    "        λ_init = λ_coarse[i]\n",
    "        \n",
    "        # Convert coarse eigenvector to tensor and move to GPU\n",
    "        u_c = torch.tensor(u_coarse[:, i], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Initialize eigenvalue as trainable parameter on GPU\n",
    "        λ = torch.tensor(λ_init, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "        # Create and move upscaler model to GPU\n",
    "        model = HierarchicalUpscaler(n_coarse, n_fine).to(device)\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + [λ], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.5)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Upscale coarse eigenvector to fine mesh\n",
    "            u_f = model(u_c)\n",
    "            \n",
    "            # Compute loss with adaptive weights\n",
    "            weights = adaptive_weights(epoch)\n",
    "            loss, loss_dict = compute_loss(u_f, λ, K_t, M_t, u_previous=u_fine_all, weights=weights)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "            # Logging\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"    Epoch {epoch}: Loss={loss.item():.2e}, λ={λ.item():.6f}, \"\n",
    "                      f\"residual={loss_dict['residual'].item():.2e}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter > 200:\n",
    "                    print(f\"    Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        # Store results (move back to CPU for storage)\n",
    "        λ_fine[i] = λ.detach().cpu()\n",
    "        u_fine_all.append(u_f.detach())  # Keep on GPU for next iteration\n",
    "\n",
    "    # Convert to numpy (move to CPU)\n",
    "    u_fine_matrix = torch.stack(u_fine_all, dim=1).cpu().numpy()\n",
    "    return λ_fine.numpy(), u_fine_matrix\n",
    "\n",
    "# =========================\n",
    "# Simple Rayleigh–Ritz Refinement\n",
    "# =========================\n",
    "def simple_rayleigh_ritz_refinement(λ_init, u_init, K, M, n_iter=3):\n",
    "    \"\"\"\n",
    "    Refine eigenpairs using Rayleigh quotient iteration and Gram-Schmidt orthogonalization.\n",
    "    \"\"\"\n",
    "    λ_refined = λ_init.copy()\n",
    "    u_refined = u_init.copy()\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i in range(len(λ_init)):\n",
    "            u = u_refined[:, i]\n",
    "            Ku = K @ u\n",
    "            Mu = M @ u\n",
    "            \n",
    "            # Update eigenvalue using Rayleigh quotient\n",
    "            λ_refined[i] = (u.T @ Ku) / (u.T @ Mu)\n",
    "\n",
    "            # Orthogonalize against previous eigenvectors\n",
    "            for j in range(i):\n",
    "                overlap = u.T @ M @ u_refined[:, j]\n",
    "                u -= overlap * u_refined[:, j]\n",
    "\n",
    "            # Normalize\n",
    "            norm = np.sqrt(u.T @ M @ u)\n",
    "            if norm > 1e-10:\n",
    "                u_refined[:, i] = u / norm\n",
    "            else:\n",
    "                print(f\"Warning: Near-zero norm at iteration {it}, eigenvector {i}\")\n",
    "\n",
    "    return λ_refined, u_refined\n",
    "\n",
    "# =========================\n",
    "# Hierarchical Eigen Solve\n",
    "# =========================\n",
    "def hierarchical_eigensolve(K_full, M_full, n_eigenpairs=10, method='uniform', levels=None, device=None, \n",
    "                           skip_null_modes=0, regularization=1e-8):\n",
    "    \"\"\"\n",
    "    Solve generalized eigenvalue problem K u = λ M u using hierarchical neural upscaling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    skip_null_modes : int\n",
    "        Number of near-zero eigenvalues to skip (e.g., rigid body modes)\n",
    "    regularization : float\n",
    "        Small value added to K diagonal for stability\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    # Add small regularization to avoid numerical issues with null modes\n",
    "    K_reg = K_full + regularization * M_full\n",
    "    \n",
    "    # Build hierarchy of coarse meshes\n",
    "    K_h, M_h, idx_h = build_hierarchy(K_reg, M_full, levels=levels, method=method)\n",
    "    level_keys = sorted(K_h.keys())\n",
    "\n",
    "    # Solve coarsest level exactly (on CPU with scipy)\n",
    "    print(\"\\nSolving coarsest level...\")\n",
    "    K_coarse = K_h[level_keys[0]]\n",
    "    M_coarse = M_h[level_keys[0]]\n",
    "    \n",
    "    # Request more modes to account for skipped ones\n",
    "    n_modes_coarse = n_eigenpairs + skip_null_modes\n",
    "    λ_all, u_all = eigh(K_coarse, M_coarse)\n",
    "    \n",
    "    # Skip null/near-zero modes\n",
    "    if skip_null_modes > 0:\n",
    "        print(f\"Skipping first {skip_null_modes} modes (null/rigid body modes)\")\n",
    "        print(f\"Skipped eigenvalues: {λ_all[:skip_null_modes]}\")\n",
    "        λ_current = λ_all[skip_null_modes:skip_null_modes + n_eigenpairs]\n",
    "        u_current = u_all[:, skip_null_modes:skip_null_modes + n_eigenpairs]\n",
    "    else:\n",
    "        λ_current = λ_all[:n_eigenpairs]\n",
    "        u_current = u_all[:, :n_eigenpairs]\n",
    "    \n",
    "    print(f\"Using eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "\n",
    "    # Progressive refinement through hierarchy\n",
    "    for i in range(1, len(level_keys)):\n",
    "        level_from = level_keys[i-1]\n",
    "        level_to = level_keys[i]\n",
    "\n",
    "        print(f\"\\n--- Refining from level {level_from} to level {level_to} ---\")\n",
    "        \n",
    "        # Neural upscaling (on GPU)\n",
    "        λ_current, u_current = train_upscaler(\n",
    "            level_from, level_to,\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            idx_coarse=idx_h[level_from],\n",
    "            n_eigenpairs=n_eigenpairs,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Post-refinement with Rayleigh-Ritz (on CPU)\n",
    "        print(f\"  Applying Rayleigh-Ritz refinement...\")\n",
    "        λ_current, u_current = simple_rayleigh_ritz_refinement(\n",
    "            λ_current, u_current,\n",
    "            K_h[level_to], M_h[level_to],\n",
    "            n_iter=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  Refined eigenvalues: {λ_current[:min(5, n_eigenpairs)]}\")\n",
    "    \n",
    "    # Remove regularization from final eigenvalues\n",
    "    λ_current = λ_current - regularization\n",
    "\n",
    "    return λ_current, u_current\n",
    "\n",
    "# =========================\n",
    "# Test Matrices\n",
    "# =========================\n",
    "def generate_test_matrices(n, matrix_type='laplacian'):\n",
    "    \"\"\"\n",
    "    Generate test matrix pairs (K, M) for eigenvalue problems.\n",
    "    \"\"\"\n",
    "    if matrix_type == 'laplacian':\n",
    "        K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.eye(n)\n",
    "    elif matrix_type == 'tridiagonal':\n",
    "        K = np.diag(2 + 0.1 * np.arange(n)) - np.diag(np.ones(n-1), 1) - np.diag(np.ones(n-1), -1)\n",
    "        M = np.diag(1 + 0.01 * np.arange(n))\n",
    "    elif matrix_type == 'random_spd':\n",
    "        np.random.seed(42)\n",
    "        A = np.random.randn(n, n)\n",
    "        B = np.random.randn(n, n)\n",
    "        K = A @ A.T + n * np.eye(n)\n",
    "        M = B @ B.T + n * np.eye(n)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {matrix_type}\")\n",
    "    return K, M\n",
    "\n",
    "# =========================\n",
    "# Verification\n",
    "# =========================\n",
    "def verify_eigenpairs(λ, u, K, M, name=\"\"):\n",
    "    \"\"\"\n",
    "    Verify quality of computed eigenpairs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name} Verification:\")\n",
    "    n_eig = len(λ)\n",
    "    for i in range(n_eig):\n",
    "        res = np.linalg.norm(K @ u[:, i] - λ[i] * M @ u[:, i])\n",
    "        norm = u[:, i].T @ M @ u[:, i]\n",
    "        print(f\"  λ[{i}]={λ[i]:.6f}, residual={res:.2e}, norm={norm:.6f}\")\n",
    "\n",
    "def diagnose_hierarchy(K_full, M_full, levels=None, method='uniform', n_check=5):\n",
    "    \"\"\"\n",
    "    Diagnose how downsampling affects eigenvalues.\n",
    "    This helps understand if the hierarchy is appropriate.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HIERARCHY DIAGNOSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    K_h, M_h, idx_h = build_hierarchy(K_full, M_full, levels=levels, method=method)\n",
    "    \n",
    "    for level in sorted(K_h.keys()):\n",
    "        K_level = K_h[level]\n",
    "        M_level = M_h[level]\n",
    "        \n",
    "        # Compute first few eigenvalues\n",
    "        λ_level, _ = eigh(K_level, M_level)\n",
    "        λ_level = λ_level[:n_check]\n",
    "        \n",
    "        print(f\"\\nLevel {level} (size={K_level.shape[0]}):\")\n",
    "        print(f\"  Eigenvalues: {λ_level}\")\n",
    "        \n",
    "    # Compare first and last level\n",
    "    λ_first = eigh(K_h[min(K_h.keys())], M_h[min(K_h.keys())])[0][:n_check]\n",
    "    λ_last = eigh(K_h[max(K_h.keys())], M_h[max(K_h.keys())])[0][:n_check]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SPECTRUM SHIFT ANALYSIS:\")\n",
    "    print(f\"Coarsest level: {λ_first}\")\n",
    "    print(f\"Finest level:   {λ_last}\")\n",
    "    print(f\"Ratio (should be ~1): {λ_first / λ_last}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if np.any(λ_first / λ_last > 10) or np.any(λ_first / λ_last < 0.1):\n",
    "        print(\"\\n⚠️  WARNING: Large spectrum shift detected!\")\n",
    "        print(\"   Downsampling dramatically changes eigenvalues.\")\n",
    "        print(\"   Consider:\")\n",
    "        print(\"   1. Using finer coarse meshes\")\n",
    "        print(\"   2. Using different downsampling strategy\")\n",
    "        print(\"   3. Using projection/interpolation operators\")\n",
    "    \n",
    "    return K_h, M_h, idx_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57a123c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BUNNY TEST (n = 2503) =====\n",
      "Computing eigen values\n",
      "Built level 0: 128 DOFs\n",
      "Built level 1: 512 DOFs\n",
      "Built level 2: 1024 DOFs\n",
      "Built level 3: 1500 DOFs\n",
      "Built level 4: 2000 DOFs\n",
      "Built level 5: 2503 DOFs\n",
      "\n",
      "Solving coarsest level...\n",
      "Skipping first 1 modes (null/rigid body modes)\n",
      "Skipped eigenvalues: [141.70204874]\n",
      "Using eigenvalues: [144.07828527 148.95798112 155.23216534 161.85716126 171.19768071]\n",
      "\n",
      "--- Refining from level 0 to level 1 ---\n",
      "Training upscaler: 128 → 512 (level 0 → 1)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.76e+00, λ=144.079300, residual=2.59e-02\n",
      "    Epoch 100: Loss=7.21e-02, λ=144.151138, residual=6.97e-02\n",
      "    Epoch 200: Loss=2.18e-02, λ=144.174194, residual=1.99e-02\n",
      "    Epoch 300: Loss=8.97e-03, λ=144.187927, residual=7.11e-03\n",
      "    Epoch 400: Loss=5.63e-03, λ=144.199051, residual=3.85e-03\n",
      "    Epoch 500: Loss=4.06e-03, λ=144.204391, residual=2.34e-03\n",
      "    Epoch 600: Loss=3.12e-03, λ=144.203674, residual=1.48e-03\n",
      "    Epoch 700: Loss=2.53e-03, λ=144.197464, residual=9.75e-04\n",
      "    Epoch 800: Loss=2.04e-03, λ=144.187119, residual=6.52e-04\n",
      "    Epoch 900: Loss=1.57e-03, λ=144.174911, residual=4.25e-04\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=148.958984, residual=2.84e-02\n",
      "    Epoch 100: Loss=7.83e-02, λ=149.028091, residual=7.65e-02\n",
      "    Epoch 200: Loss=1.88e-02, λ=149.044281, residual=1.70e-02\n",
      "    Epoch 300: Loss=8.12e-03, λ=149.059647, residual=6.16e-03\n",
      "    Epoch 400: Loss=5.77e-03, λ=149.076035, residual=3.80e-03\n",
      "    Epoch 500: Loss=4.80e-03, λ=149.087143, residual=2.84e-03\n",
      "    Epoch 600: Loss=4.19e-03, λ=149.093765, residual=2.27e-03\n",
      "    Epoch 700: Loss=3.64e-03, λ=149.096909, residual=1.80e-03\n",
      "    Epoch 800: Loss=2.94e-03, λ=149.096771, residual=1.33e-03\n",
      "    Epoch 900: Loss=2.09e-03, λ=149.093582, residual=8.86e-04\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.72e+00, λ=155.233170, residual=2.85e-02\n",
      "    Epoch 100: Loss=6.01e-02, λ=155.293518, residual=5.76e-02\n",
      "    Epoch 200: Loss=2.21e-02, λ=155.313751, residual=2.02e-02\n",
      "    Epoch 300: Loss=8.42e-03, λ=155.332123, residual=6.49e-03\n",
      "    Epoch 400: Loss=5.39e-03, λ=155.352081, residual=3.44e-03\n",
      "    Epoch 500: Loss=3.93e-03, λ=155.373444, residual=2.03e-03\n",
      "    Epoch 600: Loss=3.08e-03, λ=155.395660, residual=1.25e-03\n",
      "    Epoch 700: Loss=2.54e-03, λ=155.417725, residual=8.13e-04\n",
      "    Epoch 800: Loss=2.09e-03, λ=155.436356, residual=5.37e-04\n",
      "    Epoch 900: Loss=1.60e-03, λ=155.450211, residual=3.42e-04\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.69e+00, λ=161.858170, residual=3.41e-02\n",
      "    Epoch 100: Loss=5.73e-02, λ=161.921127, residual=5.48e-02\n",
      "    Epoch 200: Loss=2.04e-02, λ=161.936844, residual=1.85e-02\n",
      "    Epoch 300: Loss=9.80e-03, λ=161.944733, residual=7.94e-03\n",
      "    Epoch 400: Loss=5.53e-03, λ=161.954041, residual=3.69e-03\n",
      "    Epoch 500: Loss=3.75e-03, λ=161.963974, residual=1.94e-03\n",
      "    Epoch 600: Loss=2.89e-03, λ=161.968628, residual=1.11e-03\n",
      "    Epoch 700: Loss=2.42e-03, λ=161.968018, residual=7.10e-04\n",
      "    Epoch 800: Loss=2.04e-03, λ=161.964523, residual=4.91e-04\n",
      "    Epoch 900: Loss=1.60e-03, λ=161.959946, residual=3.36e-04\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.73e+00, λ=171.198685, residual=3.19e-02\n",
      "    Epoch 100: Loss=7.44e-02, λ=171.258148, residual=7.18e-02\n",
      "    Epoch 200: Loss=1.41e-02, λ=171.269165, residual=1.18e-02\n",
      "    Epoch 300: Loss=6.48e-03, λ=171.268951, residual=4.26e-03\n",
      "    Epoch 400: Loss=4.20e-03, λ=171.268188, residual=2.08e-03\n",
      "    Epoch 500: Loss=3.31e-03, λ=171.270157, residual=1.29e-03\n",
      "    Epoch 600: Loss=2.88e-03, λ=171.273956, residual=9.64e-04\n",
      "    Epoch 700: Loss=2.54e-03, λ=171.279465, residual=7.43e-04\n",
      "    Epoch 800: Loss=2.14e-03, λ=171.285233, residual=5.41e-04\n",
      "    Epoch 900: Loss=1.60e-03, λ=171.289001, residual=3.52e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.72e+00, λ=173.024261, residual=2.89e-02\n",
      "    Epoch 100: Loss=7.48e-02, λ=173.076553, residual=7.25e-02\n",
      "    Epoch 200: Loss=2.05e-02, λ=173.089355, residual=1.83e-02\n",
      "    Epoch 300: Loss=6.86e-03, λ=173.102722, residual=4.74e-03\n",
      "    Epoch 400: Loss=3.87e-03, λ=173.109283, residual=1.84e-03\n",
      "    Epoch 500: Loss=3.03e-03, λ=173.117111, residual=1.08e-03\n",
      "    Epoch 600: Loss=2.62e-03, λ=173.127884, residual=7.47e-04\n",
      "    Epoch 700: Loss=2.31e-03, λ=173.142838, residual=5.49e-04\n",
      "    Epoch 800: Loss=1.98e-03, λ=173.161667, residual=4.01e-04\n",
      "    Epoch 900: Loss=1.53e-03, λ=173.181503, residual=2.76e-04\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.75e+00, λ=174.476303, residual=2.80e-02\n",
      "    Epoch 100: Loss=7.35e-02, λ=174.529236, residual=7.13e-02\n",
      "    Epoch 200: Loss=2.34e-02, λ=174.543701, residual=2.13e-02\n",
      "    Epoch 300: Loss=1.02e-02, λ=174.542160, residual=8.13e-03\n",
      "    Epoch 400: Loss=5.43e-03, λ=174.519485, residual=3.36e-03\n",
      "    Epoch 500: Loss=3.76e-03, λ=174.494904, residual=1.69e-03\n",
      "    Epoch 600: Loss=3.05e-03, λ=174.479675, residual=1.02e-03\n",
      "    Epoch 700: Loss=2.58e-03, λ=174.474289, residual=6.55e-04\n",
      "    Epoch 800: Loss=2.12e-03, λ=174.476929, residual=4.16e-04\n",
      "    Epoch 900: Loss=1.58e-03, λ=174.484924, residual=2.49e-04\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.75e+00, λ=179.037338, residual=1.94e-02\n",
      "    Epoch 100: Loss=7.60e-02, λ=179.104538, residual=7.37e-02\n",
      "    Epoch 200: Loss=2.23e-02, λ=179.150162, residual=2.02e-02\n",
      "    Epoch 300: Loss=9.01e-03, λ=179.169403, residual=7.06e-03\n",
      "    Epoch 400: Loss=4.72e-03, λ=179.166672, residual=2.80e-03\n",
      "    Epoch 500: Loss=3.11e-03, λ=179.151474, residual=1.19e-03\n",
      "    Epoch 600: Loss=2.55e-03, λ=179.130997, residual=6.51e-04\n",
      "    Epoch 700: Loss=2.26e-03, λ=179.109634, residual=4.45e-04\n",
      "    Epoch 800: Loss=1.96e-03, λ=179.088699, residual=3.19e-04\n",
      "    Epoch 900: Loss=1.53e-03, λ=179.070419, residual=2.17e-04\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.72e+00, λ=180.084442, residual=2.65e-02\n",
      "    Epoch 100: Loss=6.87e-02, λ=180.141937, residual=6.66e-02\n",
      "    Epoch 200: Loss=1.90e-02, λ=180.156448, residual=1.69e-02\n",
      "    Epoch 300: Loss=8.31e-03, λ=180.150375, residual=6.33e-03\n",
      "    Epoch 400: Loss=4.61e-03, λ=180.136307, residual=2.66e-03\n",
      "    Epoch 500: Loss=3.30e-03, λ=180.119125, residual=1.37e-03\n",
      "    Epoch 600: Loss=2.81e-03, λ=180.100296, residual=9.12e-04\n",
      "    Epoch 700: Loss=2.53e-03, λ=180.080460, residual=7.06e-04\n",
      "    Epoch 800: Loss=2.17e-03, λ=180.060791, residual=5.36e-04\n",
      "    Epoch 900: Loss=1.66e-03, λ=180.043762, residual=3.68e-04\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.75e+00, λ=187.563400, residual=2.80e-02\n",
      "    Epoch 100: Loss=7.58e-02, λ=187.601624, residual=7.35e-02\n",
      "    Epoch 200: Loss=2.14e-02, λ=187.618271, residual=1.92e-02\n",
      "    Epoch 300: Loss=9.82e-03, λ=187.644638, residual=7.79e-03\n",
      "    Epoch 400: Loss=6.42e-03, λ=187.666092, residual=4.47e-03\n",
      "    Epoch 500: Loss=4.74e-03, λ=187.680893, residual=2.80e-03\n",
      "    Epoch 600: Loss=3.68e-03, λ=187.692490, residual=1.72e-03\n",
      "    Epoch 700: Loss=2.91e-03, λ=187.705673, residual=9.72e-04\n",
      "    Epoch 800: Loss=2.28e-03, λ=187.719452, residual=5.09e-04\n",
      "    Epoch 900: Loss=1.62e-03, λ=187.731140, residual=2.47e-04\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [142.74724 148.09767 156.5279  161.46767 171.25171]\n",
      "\n",
      "--- Refining from level 1 to level 2 ---\n",
      "Training upscaler: 512 → 1024 (level 1 → 2)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=142.748245, residual=1.33e-02\n",
      "    Epoch 100: Loss=5.89e-02, λ=142.799683, residual=5.69e-02\n",
      "    Epoch 200: Loss=2.62e-02, λ=142.797256, residual=2.50e-02\n",
      "    Epoch 300: Loss=1.49e-02, λ=142.786804, residual=1.38e-02\n",
      "    Epoch 400: Loss=9.86e-03, λ=142.778366, residual=8.74e-03\n",
      "    Epoch 500: Loss=7.13e-03, λ=142.770737, residual=6.00e-03\n",
      "    Epoch 600: Loss=5.34e-03, λ=142.762497, residual=4.21e-03\n",
      "    Epoch 700: Loss=4.02e-03, λ=142.753342, residual=2.93e-03\n",
      "    Epoch 800: Loss=2.98e-03, λ=142.744186, residual=2.01e-03\n",
      "    Epoch 900: Loss=2.16e-03, λ=142.735306, residual=1.36e-03\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=148.098679, residual=1.38e-02\n",
      "    Epoch 100: Loss=3.61e-02, λ=148.110214, residual=3.48e-02\n",
      "    Epoch 200: Loss=1.04e-02, λ=148.096817, residual=9.34e-03\n",
      "    Epoch 300: Loss=4.86e-03, λ=148.090500, residual=3.78e-03\n",
      "    Epoch 400: Loss=2.78e-03, λ=148.078949, residual=1.69e-03\n",
      "    Epoch 500: Loss=1.88e-03, λ=148.077759, residual=8.04e-04\n",
      "    Epoch 600: Loss=1.48e-03, λ=148.089615, residual=4.38e-04\n",
      "    Epoch 700: Loss=1.27e-03, λ=148.104874, residual=2.70e-04\n",
      "    Epoch 800: Loss=1.10e-03, λ=148.119705, residual=1.79e-04\n",
      "    Epoch 900: Loss=9.21e-04, λ=148.132706, residual=1.19e-04\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.76e+00, λ=156.528900, residual=1.37e-02\n",
      "    Epoch 100: Loss=2.13e-02, λ=156.572479, residual=2.02e-02\n",
      "    Epoch 200: Loss=5.81e-03, λ=156.576370, residual=4.74e-03\n",
      "    Epoch 300: Loss=2.69e-03, λ=156.581131, residual=1.64e-03\n",
      "    Epoch 400: Loss=1.84e-03, λ=156.584854, residual=7.97e-04\n",
      "    Epoch 500: Loss=1.53e-03, λ=156.584686, residual=5.19e-04\n",
      "    Epoch 600: Loss=1.37e-03, λ=156.582214, residual=3.90e-04\n",
      "    Epoch 700: Loss=1.23e-03, λ=156.579163, residual=3.03e-04\n",
      "    Epoch 800: Loss=1.09e-03, λ=156.576157, residual=2.31e-04\n",
      "    Epoch 900: Loss=9.02e-04, λ=156.575195, residual=1.65e-04\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.75e+00, λ=161.468674, residual=1.50e-02\n",
      "    Epoch 100: Loss=3.08e-02, λ=161.470551, residual=2.97e-02\n",
      "    Epoch 200: Loss=7.96e-03, λ=161.455917, residual=6.94e-03\n",
      "    Epoch 300: Loss=3.62e-03, λ=161.456818, residual=2.59e-03\n",
      "    Epoch 400: Loss=2.21e-03, λ=161.454117, residual=1.20e-03\n",
      "    Epoch 500: Loss=1.60e-03, λ=161.448013, residual=6.18e-04\n",
      "    Epoch 600: Loss=1.29e-03, λ=161.444504, residual=3.38e-04\n",
      "    Epoch 700: Loss=1.12e-03, λ=161.445389, residual=1.99e-04\n",
      "    Epoch 800: Loss=9.89e-04, λ=161.450348, residual=1.26e-04\n",
      "    Epoch 900: Loss=8.39e-04, λ=161.457794, residual=8.09e-05\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=171.252716, residual=1.53e-02\n",
      "    Epoch 100: Loss=2.23e-02, λ=171.203232, residual=2.11e-02\n",
      "    Epoch 200: Loss=6.55e-03, λ=171.150772, residual=5.44e-03\n",
      "    Epoch 300: Loss=3.27e-03, λ=171.122040, residual=2.16e-03\n",
      "    Epoch 400: Loss=2.14e-03, λ=171.117279, residual=1.03e-03\n",
      "    Epoch 500: Loss=1.74e-03, λ=171.116837, residual=6.42e-04\n",
      "    Epoch 600: Loss=1.53e-03, λ=171.114502, residual=4.58e-04\n",
      "    Epoch 700: Loss=1.37e-03, λ=171.109482, residual=3.35e-04\n",
      "    Epoch 800: Loss=1.21e-03, λ=171.101074, residual=2.42e-04\n",
      "    Epoch 900: Loss=9.84e-04, λ=171.090393, residual=1.62e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=175.242065, residual=1.48e-02\n",
      "    Epoch 100: Loss=4.28e-02, λ=175.219025, residual=4.16e-02\n",
      "    Epoch 200: Loss=1.47e-02, λ=175.169037, residual=1.36e-02\n",
      "    Epoch 300: Loss=6.93e-03, λ=175.135330, residual=5.82e-03\n",
      "    Epoch 400: Loss=4.18e-03, λ=175.112228, residual=3.08e-03\n",
      "    Epoch 500: Loss=2.83e-03, λ=175.098541, residual=1.74e-03\n",
      "    Epoch 600: Loss=2.13e-03, λ=175.095505, residual=1.05e-03\n",
      "    Epoch 700: Loss=1.72e-03, λ=175.102005, residual=6.81e-04\n",
      "    Epoch 800: Loss=1.41e-03, λ=175.116379, residual=4.46e-04\n",
      "    Epoch 900: Loss=1.13e-03, λ=175.136185, residual=2.89e-04\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=175.942505, residual=1.49e-02\n",
      "    Epoch 100: Loss=2.37e-02, λ=175.909088, residual=2.26e-02\n",
      "    Epoch 200: Loss=6.31e-03, λ=175.872574, residual=5.19e-03\n",
      "    Epoch 300: Loss=3.23e-03, λ=175.863708, residual=2.08e-03\n",
      "    Epoch 400: Loss=2.20e-03, λ=175.868073, residual=1.08e-03\n",
      "    Epoch 500: Loss=1.69e-03, λ=175.883469, residual=6.01e-04\n",
      "    Epoch 600: Loss=1.40e-03, λ=175.906845, residual=3.55e-04\n",
      "    Epoch 700: Loss=1.23e-03, λ=175.935059, residual=2.40e-04\n",
      "    Epoch 800: Loss=1.09e-03, λ=175.964050, residual=1.70e-04\n",
      "    Epoch 900: Loss=9.15e-04, λ=175.990845, residual=1.24e-04\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=177.884705, residual=1.25e-02\n",
      "    Epoch 100: Loss=2.56e-02, λ=177.828201, residual=2.43e-02\n",
      "    Epoch 200: Loss=6.64e-03, λ=177.758423, residual=5.51e-03\n",
      "    Epoch 300: Loss=3.04e-03, λ=177.735748, residual=1.89e-03\n",
      "    Epoch 400: Loss=2.01e-03, λ=177.764908, residual=8.82e-04\n",
      "    Epoch 500: Loss=1.57e-03, λ=177.807068, residual=4.69e-04\n",
      "    Epoch 600: Loss=1.36e-03, λ=177.845718, residual=2.93e-04\n",
      "    Epoch 700: Loss=1.23e-03, λ=177.880402, residual=2.14e-04\n",
      "    Epoch 800: Loss=1.10e-03, λ=177.913971, residual=1.68e-04\n",
      "    Epoch 900: Loss=9.21e-04, λ=177.946976, residual=1.29e-04\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=178.755508, residual=1.81e-02\n",
      "    Epoch 100: Loss=2.58e-02, λ=178.716080, residual=2.45e-02\n",
      "    Epoch 200: Loss=5.93e-03, λ=178.678482, residual=4.77e-03\n",
      "    Epoch 300: Loss=3.00e-03, λ=178.666626, residual=1.83e-03\n",
      "    Epoch 400: Loss=2.07e-03, λ=178.674316, residual=9.15e-04\n",
      "    Epoch 500: Loss=1.61e-03, λ=178.691635, residual=4.74e-04\n",
      "    Epoch 600: Loss=1.36e-03, λ=178.709946, residual=2.60e-04\n",
      "    Epoch 700: Loss=1.21e-03, λ=178.729782, residual=1.59e-04\n",
      "    Epoch 800: Loss=1.08e-03, λ=178.751846, residual=1.09e-04\n",
      "    Epoch 900: Loss=8.98e-04, λ=178.774323, residual=7.47e-05\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=188.382065, residual=2.14e-02\n",
      "    Epoch 100: Loss=3.78e-02, λ=188.326385, residual=3.64e-02\n",
      "    Epoch 200: Loss=1.14e-02, λ=188.317078, residual=1.01e-02\n",
      "    Epoch 300: Loss=4.68e-03, λ=188.320221, residual=3.45e-03\n",
      "    Epoch 400: Loss=2.82e-03, λ=188.314072, residual=1.64e-03\n",
      "    Epoch 500: Loss=2.10e-03, λ=188.302765, residual=9.60e-04\n",
      "    Epoch 600: Loss=1.72e-03, λ=188.289444, residual=6.16e-04\n",
      "    Epoch 700: Loss=1.47e-03, λ=188.279022, residual=4.20e-04\n",
      "    Epoch 800: Loss=1.26e-03, λ=188.273544, residual=2.89e-04\n",
      "    Epoch 900: Loss=1.02e-03, λ=188.272964, residual=1.89e-04\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [141.98354 148.52168 156.58829 161.7081  170.57524]\n",
      "\n",
      "--- Refining from level 2 to level 3 ---\n",
      "Training upscaler: 1024 → 1500 (level 2 → 3)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.80e+00, λ=141.984543, residual=1.25e-02\n",
      "    Epoch 100: Loss=3.87e-02, λ=142.019821, residual=3.78e-02\n",
      "    Epoch 200: Loss=1.57e-02, λ=141.998520, residual=1.49e-02\n",
      "    Epoch 300: Loss=8.89e-03, λ=141.977158, residual=8.08e-03\n",
      "    Epoch 400: Loss=5.99e-03, λ=141.954193, residual=5.19e-03\n",
      "    Epoch 500: Loss=4.51e-03, λ=141.927521, residual=3.71e-03\n",
      "    Epoch 600: Loss=3.60e-03, λ=141.897903, residual=2.79e-03\n",
      "    Epoch 700: Loss=2.92e-03, λ=141.867386, residual=2.12e-03\n",
      "    Epoch 800: Loss=2.34e-03, λ=141.838898, residual=1.60e-03\n",
      "    Epoch 900: Loss=1.82e-03, λ=141.813461, residual=1.19e-03\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=148.522690, residual=1.24e-02\n",
      "    Epoch 100: Loss=4.21e-02, λ=148.521332, residual=4.06e-02\n",
      "    Epoch 200: Loss=1.65e-02, λ=148.456192, residual=1.57e-02\n",
      "    Epoch 300: Loss=8.94e-03, λ=148.408020, residual=8.15e-03\n",
      "    Epoch 400: Loss=5.85e-03, λ=148.364136, residual=5.06e-03\n",
      "    Epoch 500: Loss=4.30e-03, λ=148.318726, residual=3.52e-03\n",
      "    Epoch 600: Loss=3.30e-03, λ=148.272949, residual=2.53e-03\n",
      "    Epoch 700: Loss=2.55e-03, λ=148.229843, residual=1.79e-03\n",
      "    Epoch 800: Loss=1.93e-03, λ=148.192078, residual=1.22e-03\n",
      "    Epoch 900: Loss=1.41e-03, λ=148.161026, residual=7.92e-04\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=156.589294, residual=1.38e-02\n",
      "    Epoch 100: Loss=4.00e-02, λ=156.542160, residual=3.92e-02\n",
      "    Epoch 200: Loss=1.66e-02, λ=156.465179, residual=1.58e-02\n",
      "    Epoch 300: Loss=9.24e-03, λ=156.404037, residual=8.42e-03\n",
      "    Epoch 400: Loss=5.79e-03, λ=156.349701, residual=4.98e-03\n",
      "    Epoch 500: Loss=3.99e-03, λ=156.300339, residual=3.18e-03\n",
      "    Epoch 600: Loss=2.96e-03, λ=156.255981, residual=2.15e-03\n",
      "    Epoch 700: Loss=2.31e-03, λ=156.217804, residual=1.52e-03\n",
      "    Epoch 800: Loss=1.84e-03, λ=156.186676, residual=1.09e-03\n",
      "    Epoch 900: Loss=1.43e-03, λ=156.162750, residual=7.72e-04\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=161.709106, residual=1.02e-02\n",
      "    Epoch 100: Loss=3.90e-02, λ=161.638382, residual=3.83e-02\n",
      "    Epoch 200: Loss=1.42e-02, λ=161.584183, residual=1.35e-02\n",
      "    Epoch 300: Loss=7.29e-03, λ=161.556259, residual=6.53e-03\n",
      "    Epoch 400: Loss=4.37e-03, λ=161.530670, residual=3.61e-03\n",
      "    Epoch 500: Loss=2.81e-03, λ=161.504730, residual=2.03e-03\n",
      "    Epoch 600: Loss=1.98e-03, λ=161.479324, residual=1.19e-03\n",
      "    Epoch 700: Loss=1.56e-03, λ=161.456818, residual=7.76e-04\n",
      "    Epoch 800: Loss=1.29e-03, λ=161.438080, residual=5.30e-04\n",
      "    Epoch 900: Loss=1.03e-03, λ=161.423111, residual=3.57e-04\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=170.576248, residual=1.17e-02\n",
      "    Epoch 100: Loss=4.62e-02, λ=170.564621, residual=4.53e-02\n",
      "    Epoch 200: Loss=1.72e-02, λ=170.492996, residual=1.64e-02\n",
      "    Epoch 300: Loss=8.84e-03, λ=170.481033, residual=8.04e-03\n",
      "    Epoch 400: Loss=4.40e-03, λ=170.498398, residual=3.61e-03\n",
      "    Epoch 500: Loss=2.33e-03, λ=170.506195, residual=1.55e-03\n",
      "    Epoch 600: Loss=1.59e-03, λ=170.497406, residual=8.24e-04\n",
      "    Epoch 700: Loss=1.29e-03, λ=170.483719, residual=5.40e-04\n",
      "    Epoch 800: Loss=1.10e-03, λ=170.472214, residual=3.77e-04\n",
      "    Epoch 900: Loss=9.08e-04, λ=170.464462, residual=2.60e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=176.375992, residual=1.08e-02\n",
      "    Epoch 100: Loss=5.22e-02, λ=176.313095, residual=5.07e-02\n",
      "    Epoch 200: Loss=2.02e-02, λ=176.239380, residual=1.94e-02\n",
      "    Epoch 300: Loss=1.04e-02, λ=176.194595, residual=9.57e-03\n",
      "    Epoch 400: Loss=5.26e-03, λ=176.175201, residual=4.44e-03\n",
      "    Epoch 500: Loss=2.77e-03, λ=176.177170, residual=1.96e-03\n",
      "    Epoch 600: Loss=1.77e-03, λ=176.179031, residual=9.80e-04\n",
      "    Epoch 700: Loss=1.34e-03, λ=176.171280, residual=5.64e-04\n",
      "    Epoch 800: Loss=1.09e-03, λ=176.159195, residual=3.63e-04\n",
      "    Epoch 900: Loss=8.95e-04, λ=176.147354, residual=2.43e-04\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=176.683899, residual=1.11e-02\n",
      "    Epoch 100: Loss=5.19e-02, λ=176.650314, residual=5.04e-02\n",
      "    Epoch 200: Loss=1.95e-02, λ=176.587112, residual=1.86e-02\n",
      "    Epoch 300: Loss=1.11e-02, λ=176.571182, residual=1.02e-02\n",
      "    Epoch 400: Loss=7.58e-03, λ=176.573135, residual=6.72e-03\n",
      "    Epoch 500: Loss=5.59e-03, λ=176.583054, residual=4.72e-03\n",
      "    Epoch 600: Loss=4.29e-03, λ=176.596130, residual=3.41e-03\n",
      "    Epoch 700: Loss=3.33e-03, λ=176.609863, residual=2.46e-03\n",
      "    Epoch 800: Loss=2.55e-03, λ=176.622208, residual=1.74e-03\n",
      "    Epoch 900: Loss=1.88e-03, λ=176.633041, residual=1.21e-03\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=178.913223, residual=1.15e-02\n",
      "    Epoch 100: Loss=4.28e-02, λ=178.847183, residual=4.16e-02\n",
      "    Epoch 200: Loss=1.78e-02, λ=178.764099, residual=1.69e-02\n",
      "    Epoch 300: Loss=1.02e-02, λ=178.710602, residual=9.34e-03\n",
      "    Epoch 400: Loss=6.77e-03, λ=178.675735, residual=5.91e-03\n",
      "    Epoch 500: Loss=4.75e-03, λ=178.655090, residual=3.88e-03\n",
      "    Epoch 600: Loss=3.35e-03, λ=178.644821, residual=2.49e-03\n",
      "    Epoch 700: Loss=2.37e-03, λ=178.640564, residual=1.54e-03\n",
      "    Epoch 800: Loss=1.72e-03, λ=178.638550, residual=9.33e-04\n",
      "    Epoch 900: Loss=1.26e-03, λ=178.636856, residual=5.63e-04\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=179.424316, residual=1.16e-02\n",
      "    Epoch 100: Loss=4.28e-02, λ=179.369034, residual=4.20e-02\n",
      "    Epoch 200: Loss=1.56e-02, λ=179.322968, residual=1.48e-02\n",
      "    Epoch 300: Loss=9.03e-03, λ=179.298904, residual=8.17e-03\n",
      "    Epoch 400: Loss=6.16e-03, λ=179.278900, residual=5.31e-03\n",
      "    Epoch 500: Loss=4.42e-03, λ=179.259064, residual=3.57e-03\n",
      "    Epoch 600: Loss=3.20e-03, λ=179.236725, residual=2.37e-03\n",
      "    Epoch 700: Loss=2.34e-03, λ=179.209595, residual=1.53e-03\n",
      "    Epoch 800: Loss=1.72e-03, λ=179.177002, residual=9.82e-04\n",
      "    Epoch 900: Loss=1.27e-03, λ=179.142410, residual=6.27e-04\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=188.437469, residual=1.11e-02\n",
      "    Epoch 100: Loss=4.77e-02, λ=188.352982, residual=4.68e-02\n",
      "    Epoch 200: Loss=1.66e-02, λ=188.285446, residual=1.57e-02\n",
      "    Epoch 300: Loss=8.64e-03, λ=188.258667, residual=7.78e-03\n",
      "    Epoch 400: Loss=5.68e-03, λ=188.248383, residual=4.82e-03\n",
      "    Epoch 500: Loss=4.22e-03, λ=188.245850, residual=3.36e-03\n",
      "    Epoch 600: Loss=3.35e-03, λ=188.249466, residual=2.49e-03\n",
      "    Epoch 700: Loss=2.75e-03, λ=188.259567, residual=1.90e-03\n",
      "    Epoch 800: Loss=2.24e-03, λ=188.275772, residual=1.44e-03\n",
      "    Epoch 900: Loss=1.75e-03, λ=188.296341, residual=1.08e-03\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [140.3773  146.34518 155.13693 160.94116 170.26366]\n",
      "\n",
      "--- Refining from level 3 to level 4 ---\n",
      "Training upscaler: 1500 → 2000 (level 3 → 4)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=140.378311, residual=9.72e-03\n",
      "    Epoch 100: Loss=4.37e-02, λ=140.326721, residual=4.32e-02\n",
      "    Epoch 200: Loss=1.59e-02, λ=140.259293, residual=1.53e-02\n",
      "    Epoch 300: Loss=8.06e-03, λ=140.220428, residual=7.47e-03\n",
      "    Epoch 400: Loss=4.96e-03, λ=140.196457, residual=4.37e-03\n",
      "    Epoch 500: Loss=3.47e-03, λ=140.179871, residual=2.88e-03\n",
      "    Epoch 600: Loss=2.65e-03, λ=140.166840, residual=2.05e-03\n",
      "    Epoch 700: Loss=2.08e-03, λ=140.155823, residual=1.48e-03\n",
      "    Epoch 800: Loss=1.63e-03, λ=140.147110, residual=1.05e-03\n",
      "    Epoch 900: Loss=1.24e-03, λ=140.140320, residual=7.28e-04\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=146.346191, residual=9.97e-03\n",
      "    Epoch 100: Loss=4.61e-02, λ=146.300171, residual=4.56e-02\n",
      "    Epoch 200: Loss=1.59e-02, λ=146.238983, residual=1.53e-02\n",
      "    Epoch 300: Loss=8.30e-03, λ=146.207428, residual=7.70e-03\n",
      "    Epoch 400: Loss=5.28e-03, λ=146.185684, residual=4.68e-03\n",
      "    Epoch 500: Loss=3.79e-03, λ=146.168549, residual=3.18e-03\n",
      "    Epoch 600: Loss=2.92e-03, λ=146.155060, residual=2.29e-03\n",
      "    Epoch 700: Loss=2.31e-03, λ=146.145370, residual=1.69e-03\n",
      "    Epoch 800: Loss=1.82e-03, λ=146.138916, residual=1.22e-03\n",
      "    Epoch 900: Loss=1.38e-03, λ=146.135086, residual=8.57e-04\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=155.137924, residual=9.49e-03\n",
      "    Epoch 100: Loss=2.95e-02, λ=155.095169, residual=2.88e-02\n",
      "    Epoch 200: Loss=1.05e-02, λ=155.046631, residual=9.85e-03\n",
      "    Epoch 300: Loss=5.31e-03, λ=155.020401, residual=4.70e-03\n",
      "    Epoch 400: Loss=3.25e-03, λ=155.005447, residual=2.64e-03\n",
      "    Epoch 500: Loss=2.29e-03, λ=154.996811, residual=1.69e-03\n",
      "    Epoch 600: Loss=1.77e-03, λ=154.991730, residual=1.17e-03\n",
      "    Epoch 700: Loss=1.44e-03, λ=154.989120, residual=8.50e-04\n",
      "    Epoch 800: Loss=1.19e-03, λ=154.988098, residual=6.22e-04\n",
      "    Epoch 900: Loss=9.60e-04, λ=154.988098, residual=4.45e-04\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=160.942169, residual=1.32e-02\n",
      "    Epoch 100: Loss=3.63e-02, λ=160.884491, residual=3.52e-02\n",
      "    Epoch 200: Loss=1.31e-02, λ=160.814102, residual=1.25e-02\n",
      "    Epoch 300: Loss=6.94e-03, λ=160.762726, residual=6.33e-03\n",
      "    Epoch 400: Loss=4.17e-03, λ=160.723343, residual=3.57e-03\n",
      "    Epoch 500: Loss=2.80e-03, λ=160.690155, residual=2.21e-03\n",
      "    Epoch 600: Loss=2.01e-03, λ=160.660660, residual=1.42e-03\n",
      "    Epoch 700: Loss=1.50e-03, λ=160.634720, residual=9.22e-04\n",
      "    Epoch 800: Loss=1.16e-03, λ=160.612778, residual=6.09e-04\n",
      "    Epoch 900: Loss=9.15e-04, λ=160.594696, residual=4.12e-04\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=170.264648, residual=8.81e-03\n",
      "    Epoch 100: Loss=3.84e-02, λ=170.184464, residual=3.76e-02\n",
      "    Epoch 200: Loss=1.32e-02, λ=170.132812, residual=1.25e-02\n",
      "    Epoch 300: Loss=6.23e-03, λ=170.110703, residual=5.61e-03\n",
      "    Epoch 400: Loss=3.63e-03, λ=170.103210, residual=3.00e-03\n",
      "    Epoch 500: Loss=2.47e-03, λ=170.101761, residual=1.84e-03\n",
      "    Epoch 600: Loss=1.78e-03, λ=170.102951, residual=1.16e-03\n",
      "    Epoch 700: Loss=1.35e-03, λ=170.106354, residual=7.29e-04\n",
      "    Epoch 800: Loss=1.07e-03, λ=170.110931, residual=4.72e-04\n",
      "    Epoch 900: Loss=8.61e-04, λ=170.115753, residual=3.14e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=175.525513, residual=9.77e-03\n",
      "    Epoch 100: Loss=3.17e-02, λ=175.456039, residual=3.11e-02\n",
      "    Epoch 200: Loss=1.10e-02, λ=175.419556, residual=1.04e-02\n",
      "    Epoch 300: Loss=6.05e-03, λ=175.392639, residual=5.41e-03\n",
      "    Epoch 400: Loss=4.02e-03, λ=175.371155, residual=3.38e-03\n",
      "    Epoch 500: Loss=3.01e-03, λ=175.353912, residual=2.36e-03\n",
      "    Epoch 600: Loss=2.38e-03, λ=175.340302, residual=1.74e-03\n",
      "    Epoch 700: Loss=1.93e-03, λ=175.330215, residual=1.29e-03\n",
      "    Epoch 800: Loss=1.56e-03, λ=175.323547, residual=9.52e-04\n",
      "    Epoch 900: Loss=1.23e-03, λ=175.319778, residual=6.85e-04\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=176.966660, residual=9.60e-03\n",
      "    Epoch 100: Loss=3.30e-02, λ=176.901062, residual=3.23e-02\n",
      "    Epoch 200: Loss=6.80e-03, λ=176.867523, residual=6.20e-03\n",
      "    Epoch 300: Loss=2.90e-03, λ=176.852356, residual=2.28e-03\n",
      "    Epoch 400: Loss=1.80e-03, λ=176.843384, residual=1.18e-03\n",
      "    Epoch 500: Loss=1.39e-03, λ=176.834579, residual=7.62e-04\n",
      "    Epoch 600: Loss=1.17e-03, λ=176.823090, residual=5.44e-04\n",
      "    Epoch 700: Loss=1.03e-03, λ=176.808548, residual=4.14e-04\n",
      "    Epoch 800: Loss=9.08e-04, λ=176.791901, residual=3.18e-04\n",
      "    Epoch 900: Loss=7.68e-04, λ=176.775116, residual=2.32e-04\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=178.484360, residual=9.88e-03\n",
      "    Epoch 100: Loss=2.62e-02, λ=178.414246, residual=2.55e-02\n",
      "    Epoch 200: Loss=6.60e-03, λ=178.375870, residual=6.01e-03\n",
      "    Epoch 300: Loss=3.14e-03, λ=178.356552, residual=2.54e-03\n",
      "    Epoch 400: Loss=2.07e-03, λ=178.341629, residual=1.47e-03\n",
      "    Epoch 500: Loss=1.59e-03, λ=178.328766, residual=9.86e-04\n",
      "    Epoch 600: Loss=1.31e-03, λ=178.318085, residual=7.03e-04\n",
      "    Epoch 700: Loss=1.12e-03, λ=178.309601, residual=5.13e-04\n",
      "    Epoch 800: Loss=9.60e-04, λ=178.303070, residual=3.75e-04\n",
      "    Epoch 900: Loss=8.02e-04, λ=178.298889, residual=2.66e-04\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=177.433746, residual=9.50e-03\n",
      "    Epoch 100: Loss=3.14e-02, λ=177.359818, residual=3.03e-02\n",
      "    Epoch 200: Loss=9.62e-03, λ=177.331360, residual=8.96e-03\n",
      "    Epoch 300: Loss=4.13e-03, λ=177.318909, residual=3.49e-03\n",
      "    Epoch 400: Loss=2.34e-03, λ=177.306702, residual=1.70e-03\n",
      "    Epoch 500: Loss=1.55e-03, λ=177.296188, residual=9.18e-04\n",
      "    Epoch 600: Loss=1.16e-03, λ=177.288559, residual=5.44e-04\n",
      "    Epoch 700: Loss=9.43e-04, λ=177.283951, residual=3.47e-04\n",
      "    Epoch 800: Loss=8.02e-04, λ=177.281250, residual=2.34e-04\n",
      "    Epoch 900: Loss=6.74e-04, λ=177.279724, residual=1.57e-04\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=189.902374, residual=1.06e-02\n",
      "    Epoch 100: Loss=3.61e-02, λ=189.829071, residual=3.51e-02\n",
      "    Epoch 200: Loss=1.02e-02, λ=189.791672, residual=9.55e-03\n",
      "    Epoch 300: Loss=4.34e-03, λ=189.776169, residual=3.67e-03\n",
      "    Epoch 400: Loss=2.72e-03, λ=189.770554, residual=2.04e-03\n",
      "    Epoch 500: Loss=2.04e-03, λ=189.769363, residual=1.38e-03\n",
      "    Epoch 600: Loss=1.66e-03, λ=189.770233, residual=1.01e-03\n",
      "    Epoch 700: Loss=1.38e-03, λ=189.772064, residual=7.40e-04\n",
      "    Epoch 800: Loss=1.14e-03, λ=189.775116, residual=5.24e-04\n",
      "    Epoch 900: Loss=9.01e-04, λ=189.778168, residual=3.52e-04\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [139.60733 146.41808 155.04012 158.65987 171.10239]\n",
      "\n",
      "--- Refining from level 4 to level 5 ---\n",
      "Training upscaler: 2000 → 2503 (level 4 → 5)\n",
      "  Eigenpair 1/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=139.608322, residual=8.26e-03\n",
      "    Epoch 100: Loss=5.60e-02, λ=139.582718, residual=5.52e-02\n",
      "    Epoch 200: Loss=1.88e-02, λ=139.532150, residual=1.82e-02\n",
      "    Epoch 300: Loss=1.04e-02, λ=139.506729, residual=9.84e-03\n",
      "    Epoch 400: Loss=6.94e-03, λ=139.491714, residual=6.40e-03\n",
      "    Epoch 500: Loss=5.03e-03, λ=139.481400, residual=4.48e-03\n",
      "    Epoch 600: Loss=3.79e-03, λ=139.473068, residual=3.24e-03\n",
      "    Epoch 700: Loss=2.90e-03, λ=139.465439, residual=2.37e-03\n",
      "    Epoch 800: Loss=2.24e-03, λ=139.456818, residual=1.75e-03\n",
      "    Epoch 900: Loss=1.73e-03, λ=139.447662, residual=1.30e-03\n",
      "  Eigenpair 2/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=146.419067, residual=8.19e-03\n",
      "    Epoch 100: Loss=1.54e-01, λ=146.395432, residual=1.53e-01\n",
      "    Epoch 200: Loss=7.93e-02, λ=146.329315, residual=7.73e-02\n",
      "    Epoch 300: Loss=5.07e-02, λ=146.264389, residual=4.86e-02\n",
      "    Epoch 400: Loss=3.38e-02, λ=146.212387, residual=3.15e-02\n",
      "    Epoch 500: Loss=2.30e-02, λ=146.172150, residual=2.07e-02\n",
      "    Epoch 600: Loss=1.58e-02, λ=146.141174, residual=1.38e-02\n",
      "    Epoch 700: Loss=1.09e-02, λ=146.117264, residual=9.44e-03\n",
      "    Epoch 800: Loss=7.51e-03, λ=146.098724, residual=6.61e-03\n",
      "    Epoch 900: Loss=5.31e-03, λ=146.084122, residual=4.76e-03\n",
      "  Eigenpair 3/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=155.041107, residual=8.84e-03\n",
      "    Epoch 100: Loss=8.21e-02, λ=155.006897, residual=7.85e-02\n",
      "    Epoch 200: Loss=2.74e-02, λ=154.963150, residual=2.68e-02\n",
      "    Epoch 300: Loss=1.46e-02, λ=154.936386, residual=1.40e-02\n",
      "    Epoch 400: Loss=9.44e-03, λ=154.916855, residual=8.82e-03\n",
      "    Epoch 500: Loss=6.64e-03, λ=154.900238, residual=6.02e-03\n",
      "    Epoch 600: Loss=4.90e-03, λ=154.885513, residual=4.28e-03\n",
      "    Epoch 700: Loss=3.70e-03, λ=154.872437, residual=3.11e-03\n",
      "    Epoch 800: Loss=2.81e-03, λ=154.861008, residual=2.29e-03\n",
      "    Epoch 900: Loss=2.15e-03, λ=154.851227, residual=1.69e-03\n",
      "  Eigenpair 4/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=158.660858, residual=8.69e-03\n",
      "    Epoch 100: Loss=1.68e-01, λ=158.626282, residual=1.64e-01\n",
      "    Epoch 200: Loss=6.61e-02, λ=158.576202, residual=6.46e-02\n",
      "    Epoch 300: Loss=3.45e-02, λ=158.539017, residual=3.34e-02\n",
      "    Epoch 400: Loss=2.16e-02, λ=158.510605, residual=2.05e-02\n",
      "    Epoch 500: Loss=1.48e-02, λ=158.487381, residual=1.37e-02\n",
      "    Epoch 600: Loss=1.06e-02, λ=158.468460, residual=9.59e-03\n",
      "    Epoch 700: Loss=7.73e-03, λ=158.453430, residual=6.85e-03\n",
      "    Epoch 800: Loss=5.65e-03, λ=158.441895, residual=4.98e-03\n",
      "    Epoch 900: Loss=4.17e-03, λ=158.433350, residual=3.68e-03\n",
      "  Eigenpair 5/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=171.103378, residual=8.40e-03\n",
      "    Epoch 100: Loss=2.10e-02, λ=171.092285, residual=2.05e-02\n",
      "    Epoch 200: Loss=7.15e-03, λ=171.090744, residual=6.63e-03\n",
      "    Epoch 300: Loss=3.90e-03, λ=171.092819, residual=3.38e-03\n",
      "    Epoch 400: Loss=2.57e-03, λ=171.095871, residual=2.05e-03\n",
      "    Epoch 500: Loss=1.91e-03, λ=171.098923, residual=1.38e-03\n",
      "    Epoch 600: Loss=1.52e-03, λ=171.101974, residual=9.99e-04\n",
      "    Epoch 700: Loss=1.26e-03, λ=171.105026, residual=7.40e-04\n",
      "    Epoch 800: Loss=1.05e-03, λ=171.108078, residual=5.48e-04\n",
      "    Epoch 900: Loss=8.62e-04, λ=171.111130, residual=4.01e-04\n",
      "  Eigenpair 6/10\n",
      "    Epoch 0: Loss=9.78e+00, λ=175.507126, residual=8.11e-03\n",
      "    Epoch 100: Loss=7.76e-02, λ=175.478287, residual=7.67e-02\n",
      "    Epoch 200: Loss=2.69e-02, λ=175.449570, residual=2.63e-02\n",
      "    Epoch 300: Loss=1.44e-02, λ=175.430252, residual=1.38e-02\n",
      "    Epoch 400: Loss=9.05e-03, λ=175.417038, residual=8.44e-03\n",
      "    Epoch 500: Loss=6.35e-03, λ=175.407333, residual=5.74e-03\n",
      "    Epoch 600: Loss=4.77e-03, λ=175.399612, residual=4.16e-03\n",
      "    Epoch 700: Loss=3.70e-03, λ=175.392776, residual=3.11e-03\n",
      "    Epoch 800: Loss=2.90e-03, λ=175.386673, residual=2.36e-03\n",
      "    Epoch 900: Loss=2.27e-03, λ=175.380569, residual=1.80e-03\n",
      "  Eigenpair 7/10\n",
      "    Epoch 0: Loss=9.79e+00, λ=174.538528, residual=8.16e-03\n",
      "    Epoch 100: Loss=2.09e-01, λ=174.499435, residual=1.87e-01\n",
      "    Epoch 200: Loss=9.60e-02, λ=174.453339, residual=9.37e-02\n",
      "    Epoch 300: Loss=5.42e-02, λ=174.414703, residual=5.21e-02\n",
      "    Epoch 400: Loss=3.42e-02, λ=174.386292, residual=3.22e-02\n",
      "    Epoch 500: Loss=2.33e-02, λ=174.364426, residual=2.14e-02\n",
      "    Epoch 600: Loss=1.65e-02, λ=174.347046, residual=1.48e-02\n",
      "    Epoch 700: Loss=1.17e-02, λ=174.333054, residual=1.04e-02\n",
      "    Epoch 800: Loss=8.31e-03, λ=174.321732, residual=7.47e-03\n",
      "    Epoch 900: Loss=6.00e-03, λ=174.312592, residual=5.47e-03\n",
      "  Eigenpair 8/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=178.214600, residual=7.76e-03\n",
      "    Epoch 100: Loss=2.28e-01, λ=178.176880, residual=2.27e-01\n",
      "    Epoch 200: Loss=1.10e-01, λ=178.134171, residual=1.07e-01\n",
      "    Epoch 300: Loss=6.22e-02, λ=178.100266, residual=5.98e-02\n",
      "    Epoch 400: Loss=3.88e-02, λ=178.074829, residual=3.65e-02\n",
      "    Epoch 500: Loss=2.57e-02, λ=178.054688, residual=2.35e-02\n",
      "    Epoch 600: Loss=1.74e-02, λ=178.038361, residual=1.55e-02\n",
      "    Epoch 700: Loss=1.18e-02, λ=178.025513, residual=1.04e-02\n",
      "    Epoch 800: Loss=8.07e-03, λ=178.015579, residual=7.17e-03\n",
      "    Epoch 900: Loss=5.68e-03, λ=178.007751, residual=5.12e-03\n",
      "  Eigenpair 9/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=177.236053, residual=7.99e-03\n",
      "    Epoch 100: Loss=1.03e-01, λ=177.195984, residual=1.02e-01\n",
      "    Epoch 200: Loss=3.70e-02, λ=177.163345, residual=3.63e-02\n",
      "    Epoch 300: Loss=1.92e-02, λ=177.141006, residual=1.85e-02\n",
      "    Epoch 400: Loss=1.21e-02, λ=177.122681, residual=1.14e-02\n",
      "    Epoch 500: Loss=8.48e-03, λ=177.106659, residual=7.79e-03\n",
      "    Epoch 600: Loss=6.19e-03, λ=177.092697, residual=5.51e-03\n",
      "    Epoch 700: Loss=4.59e-03, λ=177.081024, residual=3.95e-03\n",
      "    Epoch 800: Loss=3.41e-03, λ=177.071259, residual=2.86e-03\n",
      "    Epoch 900: Loss=2.56e-03, λ=177.063293, residual=2.10e-03\n",
      "  Eigenpair 10/10\n",
      "    Epoch 0: Loss=9.77e+00, λ=190.723892, residual=9.16e-03\n",
      "    Epoch 100: Loss=2.24e-01, λ=190.689362, residual=2.06e-01\n",
      "    Epoch 200: Loss=9.49e-02, λ=190.658768, residual=9.27e-02\n",
      "    Epoch 300: Loss=5.42e-02, λ=190.633255, residual=5.23e-02\n",
      "    Epoch 400: Loss=3.43e-02, λ=190.613174, residual=3.23e-02\n",
      "    Epoch 500: Loss=2.30e-02, λ=190.597519, residual=2.11e-02\n",
      "    Epoch 600: Loss=1.61e-02, λ=190.585022, residual=1.44e-02\n",
      "    Epoch 700: Loss=1.14e-02, λ=190.575027, residual=1.02e-02\n",
      "    Epoch 800: Loss=8.23e-03, λ=190.566910, residual=7.40e-03\n",
      "    Epoch 900: Loss=6.05e-03, λ=190.560425, residual=5.50e-03\n",
      "  Applying Rayleigh-Ritz refinement...\n",
      "  Refined eigenvalues: [139.21329 142.62805 153.84276 156.14365 171.84094]\n",
      "\n",
      "Bunny test Verification:\n",
      "  λ[0]=139.213287, residual=1.88e+00, norm=1.000000\n",
      "  λ[1]=142.628052, residual=4.46e+00, norm=1.000000\n",
      "  λ[2]=153.842758, residual=2.16e+00, norm=1.000000\n",
      "  λ[3]=156.143646, residual=3.56e+00, norm=1.000000\n",
      "  λ[4]=171.840942, residual=1.00e+00, norm=1.000000\n",
      "  λ[5]=174.623383, residual=2.28e+00, norm=1.000000\n",
      "  λ[6]=168.999023, residual=4.63e+00, norm=1.000000\n",
      "  λ[7]=173.474884, residual=4.57e+00, norm=1.000000\n",
      "  λ[8]=175.121841, residual=2.44e+00, norm=1.000000\n",
      "  λ[9]=187.112289, residual=4.66e+00, norm=1.000000\n",
      "✓ Bunny test done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== BUNNY TEST (n = 2503) =====\")\n",
    "from Mesh import Mesh\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "K, M = m.computeLaplacian()\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = eigh(K,M)\n",
    "\n",
    "levels = {0: 128, 1: 512, 2: 1024, 3: 1500, 4: 2000, 5: 2503}\n",
    "λ_hier, u_hier = hierarchical_eigensolve(\n",
    "    K, M,\n",
    "    n_eigenpairs=10,\n",
    "    levels=levels,\n",
    "    skip_null_modes=1,      # Skip the ~0 eigenvalue\n",
    "    regularization=1e-6,     # Small stabilizing shift\n",
    "    method='uniform',        # or 'maxdist' for better sampling\n",
    "    device=device\n",
    ")\n",
    "verify_eigenpairs(λ_hier, u_hier, K, M, \"Bunny test\")\n",
    "print(\"✓ Bunny test done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13212f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.510357, 40.49331 , 43.84867 , 45.69833 , 46.548016, 45.92576 ,\n",
       "       50.122097, 51.026657, 51.08666 , 53.15215 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ_hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b66ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
