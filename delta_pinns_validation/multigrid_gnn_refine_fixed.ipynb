{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaae7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multigrid_gnn_refine_fixed.py\n",
    "\"\"\"\n",
    "Complete working rewrite of your multigrid + GNN eigen-refinement pipeline.\n",
    "\n",
    "Assumptions:\n",
    " - `Mesh` class exists and Mesh('bunny.obj') loads .verts (n x 3) and .connectivity (triangles).\n",
    " - `robust_laplacian.point_cloud_laplacian(X)` returns (L, M) as scipy sparse matrices where L and M are compatible with eigsh.\n",
    " - scikit-learn, scipy, numpy, matplotlib, torch are installed.\n",
    "\n",
    "Key features:\n",
    " - All classes and functions defined in one file (no missing names).\n",
    " - Stable training: column normalization, small correction scale, normalized losses, grad clipping, configurable weights.\n",
    " - Auto-detect input feature dimension to avoid matmul mismatches.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports (must be available in your environment)\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utility helpers\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    \"\"\"Convert scipy sparse matrix to torch.sparse_coo_tensor (CPU or GPU depending on .to(device)).\"\"\"\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    \"\"\"Normalize numpy matrix columns to have unit L2 norm. Returns normalized U and norms.\"\"\"\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    \"\"\"Normalize torch tensor columns to have unit L2 norm. Returns normalized U and norms (torch).\"\"\"\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Simple per-node corrector (message-passing via neighbor mean + MLP)\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 64, 32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2  # because we will concat self + neighbor-mean in forward\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [n, in_dim]\n",
    "        edge_index: LongTensor shape [2, n_edges] with (row=target, col=source) semantics\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # both LongTensor\n",
    "        n = x.shape[0]\n",
    "        # aggregate neighbor features: mean aggregator\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device)\n",
    "        deg = deg.clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)  # shape [n, 2*in_dim]\n",
    "        return self.net(h)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid eigensolver with GNN corrector\n",
    "# ------------------------\n",
    "class MultigridEigensolver:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        # use eigsh with M as mass\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Core training routine\n",
    "    # ------------------------\n",
    "    def train_gnn(self, model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                  n_modes,\n",
    "                  epochs=200,\n",
    "                  lr=1e-3,\n",
    "                  corr_scale=1e-2,\n",
    "                  w_res=10.0,\n",
    "                  w_orth=1.0,\n",
    "                  w_proj=1e-3,\n",
    "                  grad_clip=1.0,\n",
    "                  weight_decay=1e-6,\n",
    "                  log_every=200):\n",
    "        \"\"\"\n",
    "        Train corrector model:\n",
    "          - x_feats: torch.FloatTensor [n_fine, in_dim] on device\n",
    "          - edge_index: torch.LongTensor [2, n_edges] on device\n",
    "          - U_init: numpy array [n_fine, n_modes] (will be normalized inside)\n",
    "          - L_fine, M_fine: scipy sparse matrices\n",
    "          - U_coarse: numpy array [n_coarse, n_modes]\n",
    "          - P: scipy sparse prolongation (n_fine x n_coarse)\n",
    "        Returns U_pred (numpy array [n_fine, n_modes]) - denormalized to original U_init scale.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Convert sparse matrices to torch sparse on device\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(device)\n",
    "        R_t = sp_to_torch_sparse(P.T).to(device)\n",
    "\n",
    "        # Normalize columns of U_init and U_coarse (keep original norms for rescaling)\n",
    "        U_init_normed, uinit_norms = normalize_columns_np(U_init)\n",
    "        U_coarse_normed, ucoarse_norms = normalize_columns_np(U_coarse)\n",
    "\n",
    "        U_init_t = torch.FloatTensor(U_init_normed).to(device)   # [n_fine, n_modes]\n",
    "        U_coarse_t = torch.FloatTensor(U_coarse_normed).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        n_fine = U_init_t.shape[0]\n",
    "        n_coarse = U_coarse_t.shape[0]\n",
    "        denom_res = float(max(1, n_fine * n_modes))\n",
    "        denom_proj = float(max(1, n_coarse * n_modes))\n",
    "        I = torch.eye(n_modes, device=device)\n",
    "\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            corr_raw = model(x_feats, edge_index)  # [n_fine, n_modes]\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_init_t + corr\n",
    "\n",
    "            # Rayleigh-related tensors\n",
    "            Lu = torch.sparse.mm(L_t, U_pred)\n",
    "            Mu = torch.sparse.mm(M_t, U_pred)\n",
    "            num = torch.sum(U_pred * Lu, dim=0)\n",
    "            den = torch.sum(U_pred * Mu, dim=0) + 1e-12\n",
    "            lambdas = num / den\n",
    "\n",
    "            # Residual loss (normalized)\n",
    "            res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "            L_res = torch.sum(res**2) / denom_res\n",
    "\n",
    "            # Orthonormality loss (M-weighted Gram)\n",
    "            MUt = torch.sparse.mm(M_t, U_pred)\n",
    "            Gram = U_pred.t() @ MUt\n",
    "            L_orth = torch.sum((Gram - I)**2) / (n_modes * n_modes)\n",
    "\n",
    "            # Projection loss\n",
    "            proj = torch.sparse.mm(R_t, U_pred)\n",
    "            L_proj = torch.sum((proj - U_coarse_t)**2) / denom_proj\n",
    "\n",
    "            loss = w_res * L_res + w_orth * L_orth + w_proj * L_proj\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (ep % log_every == 0) or (ep == epochs - 1):\n",
    "                with torch.no_grad():\n",
    "                    u_norm = float(U_pred.norm().cpu().item())\n",
    "                    corr_std = float(corr.std().cpu().item())\n",
    "                print(f\"    Epoch {ep:4d}: Loss={loss.item():.6f} (Res={L_res.item():.6f}, Orth={L_orth.item():.6f}, Proj={L_proj.item():.6f}) U_norm={u_norm:.4f} corr_std={corr_std:.6f}\")\n",
    "\n",
    "        # Denormalize: multiply columns by original column norms of U_init\n",
    "        U_pred_np = U_pred.detach().cpu().numpy() * uinit_norms.reshape(1, -1)\n",
    "        return U_pred_np\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L_fine, M_fine):\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(self.device)\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "\n",
    "    # ------------------------\n",
    "    # Refine one level (coarse -> fine)\n",
    "    # ------------------------\n",
    "    def refine_level(self, X_coarse, U_coarse, X_fine, n_modes,\n",
    "                     hidden_sizes=(128, 64, 32),\n",
    "                     dropout=0.0,\n",
    "                     k_neighbors=4,\n",
    "                     epochs=200,\n",
    "                     lr=1e-3,\n",
    "                     corr_scale=1e-2,\n",
    "                     w_res=10.0,\n",
    "                     w_orth=1.0,\n",
    "                     w_proj=1e-3,\n",
    "                     freeze_layers=0,\n",
    "                     checkpoint_name=None):\n",
    "        \"\"\"\n",
    "        Single-level refinement.\n",
    "        - Automatically sets input dim from features.\n",
    "        - Creates model if not existing; reuses and optionally freezes layers if existing.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        print(f\"  Computing Laplacian for {X_fine.shape[0]} points...\")\n",
    "        L_fine, M_fine = robust_laplacian.point_cloud_laplacian(X_fine)\n",
    "\n",
    "        print(\"  Building prolongation operator...\")\n",
    "        P = self.build_prolongation(X_coarse, X_fine, k=1)\n",
    "\n",
    "        print(\"  Building kNN graph...\")\n",
    "        edge_index = self.build_knn_graph(X_fine, k=k_neighbors).to(device)\n",
    "\n",
    "        # Build U_init on fine grid\n",
    "        U_init = P @ U_coarse  # shape [n_fine, n_modes]\n",
    "\n",
    "        # Build features: coords + U_init (we pass raw U_init; normalization happens inside train_gnn)\n",
    "        x_feats = torch.FloatTensor(np.hstack([X_fine, U_init])).to(device)\n",
    "\n",
    "        in_dim = x_feats.shape[1]\n",
    "        out_dim = n_modes\n",
    "\n",
    "        if self.model is None:\n",
    "            print(f\"  Creating new corrector model (in_dim={in_dim}, out_dim={out_dim})...\")\n",
    "            self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "        else:\n",
    "            # If model exists but input dimension changed, re-create model to match new in_dim\n",
    "            # (safer than trying to partially load weights with mismatched shapes)\n",
    "            existing_in_dim = None\n",
    "            # try to infer existing in_dim by checking first Linear in model.net if present\n",
    "            for m in self.model.net:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    existing_in_dim = m.in_features\n",
    "                    break\n",
    "            if existing_in_dim != in_dim:\n",
    "                print(f\"  Recreating model to match new input dim (was {existing_in_dim}, now {in_dim})...\")\n",
    "                # Optionally copy weights for layers that match by size\n",
    "                old_state = self.model.state_dict()\n",
    "                self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "                # attempt to copy subset of weights where shapes match\n",
    "                new_state = self.model.state_dict()\n",
    "                for k, v in old_state.items():\n",
    "                    if k in new_state and old_state[k].shape == new_state[k].shape:\n",
    "                        new_state[k] = old_state[k]\n",
    "                self.model.load_state_dict(new_state)\n",
    "\n",
    "        # Optionally freeze first few linear layers (count of Linear modules)\n",
    "        if freeze_layers > 0:\n",
    "            linear_count = 0\n",
    "            for module in self.model.net:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    linear_count += 1\n",
    "                    if linear_count <= freeze_layers:\n",
    "                        for p in module.parameters():\n",
    "                            p.requires_grad = False\n",
    "            print(f\"  Frozen first {freeze_layers} linear layers.\")\n",
    "\n",
    "        print(f\"  Training corrector: epochs={epochs}, lr={lr}, corr_scale={corr_scale}\")\n",
    "        U_pred = self.train_gnn(self.model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                                n_modes,\n",
    "                                epochs=epochs,\n",
    "                                lr=lr,\n",
    "                                corr_scale=corr_scale,\n",
    "                                w_res=w_res,\n",
    "                                w_orth=w_orth,\n",
    "                                w_proj=w_proj)\n",
    "\n",
    "        print(\"  Rayleigh-Ritz refinement...\")\n",
    "        lambda_refined, U_refined = self.refine_eigenvectors(U_pred, L_fine, M_fine)\n",
    "\n",
    "        if checkpoint_name is not None:\n",
    "            ckpt = {\"model_state\": self.model.state_dict(), \"lambda_refined\": lambda_refined}\n",
    "            torch.save(ckpt, os.path.join(self.checkpoint_dir, checkpoint_name))\n",
    "            print(f\"  Saved checkpoint: {os.path.join(self.checkpoint_dir, checkpoint_name)}\")\n",
    "\n",
    "        return lambda_refined, U_refined, L_fine, M_fine\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Visualization helper\n",
    "# ------------------------\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None, show=True):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2],\n",
    "                    triangles=mesh.connectivity, alpha=0.35)\n",
    "    if highlight_indices is not None:\n",
    "        hv = mesh.verts[highlight_indices]\n",
    "        ax.scatter(hv[:, 0], hv[:, 1], hv[:, 2], s=6, label=f\"{len(highlight_indices)} pts\")\n",
    "        ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=120, azim=-90)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main script\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hidden_sizes = (128, 128, 128)\n",
    "    dropout = 0.0\n",
    "\n",
    "    # schedule and hyperparams\n",
    "    epochs_schedule = {0: 0, 1: 1500, 2: 1000, 3: 800, 4: 800}\n",
    "    #hierarchy = [128, 512, 1024]  # final level will append full\n",
    "    hierarchy = [1024, 2048, 4096]  # final level will append full\n",
    "    k_neighbors = 4\n",
    "    lr_start = 1e-3\n",
    "    lr_min = 5e-4\n",
    "    corr_scale = 1e-2\n",
    "    w_res = 10.0\n",
    "    w_orth = 10.0\n",
    "    w_proj = 1e-3\n",
    "    freeze_schedule = {1: 0, 2: 1, 3: 1, 4: 2}\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridEigensolver.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    print(f\"Mesh loaded: {n_total} vertices\")\n",
    "\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    all_idx = np.arange(n_total)\n",
    "    rng.shuffle(all_idx)\n",
    "    indices_per_level = {}\n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        indices_per_level[i] = all_idx[:n_points].copy()\n",
    "        print(f\"  Level {i}: {n_points} points (nested)\")\n",
    "\n",
    "    solver = MultigridEigensolver()\n",
    "\n",
    "    # Level 0 coarse solve\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(\"\\nLEVEL 0: coarse solving...\")\n",
    "    lambda_cur, U_cur, L_cur, M_cur = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "    # iterative refinement\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level - 1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "        epochs = epochs_schedule.get(level, 1000)\n",
    "\n",
    "        print(f\"\\nLEVEL {level}: refine {Xc.shape[0]} -> {Xf.shape[0]}, epochs={epochs}\")\n",
    "        freeze_layers = freeze_schedule.get(level, 0)\n",
    "\n",
    "        total_levels = len(hierarchy)\n",
    "        decay = (level - 1) / max(1, total_levels - 1)\n",
    "        lr = lr_start * ((lr_min / lr_start) ** decay)\n",
    "\n",
    "        lambda_cur, U_cur, L_cur, M_cur = solver.refine_level(\n",
    "            Xc, U_cur, Xf, n_modes,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            k_neighbors=k_neighbors,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            corr_scale=corr_scale,\n",
    "            w_res=w_res,\n",
    "            w_orth=w_orth,\n",
    "            w_proj=w_proj,\n",
    "            freeze_layers=freeze_layers,\n",
    "            checkpoint_name=f\"level_{level}_ckpt.pt\"\n",
    "        )\n",
    "\n",
    "        print(\"GNN-refined eigenvalues:\", np.round(lambda_cur, 3))\n",
    "\n",
    "        # exact eigenvalues for verification\n",
    "        #print(\"  computing exact eigenvalues for verification...\")\n",
    "        #lambda_exact, _, _, _ = solver.solve_eigenvalue_problem(Xf, n_modes)\n",
    "        #rel_err = np.abs(lambda_cur - lambda_exact) / (np.abs(lambda_exact) + 1e-12)\n",
    "        #print(\"  Exact eigenvalues:\", np.round(lambda_exact, 6))\n",
    "        #print(\"  Relative errors:  \", np.round(rel_err, 6))\n",
    "\n",
    "    print(\"\\nDone. Final eigenvalues:\", np.round(lambda_cur, 3))\n",
    "\n",
    "    return U_cur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdc77de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Mesh loaded: 2503 vertices\n",
      "Hierarchy: [1024, 2048, 2503]\n",
      "  Level 0: 1024 points (nested)\n",
      "  Level 1: 2048 points (nested)\n",
      "  Level 2: 2503 points (nested)\n",
      "\n",
      "LEVEL 0: coarse solving...\n",
      "Coarse eigenvalues: [0.       0.333353 0.765464 0.8359   1.064607 1.2346   1.738815 2.626301\n",
      " 2.899525 3.119024]\n",
      "\n",
      "LEVEL 1: refine 1024 -> 2048, epochs=1500\n",
      "  Computing Laplacian for 2048 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Creating new corrector model (in_dim=13, out_dim=10)...\n",
      "  Training corrector: epochs=1500, lr=0.001, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.970057 (Res=0.000054, Orth=0.096952, Proj=0.000696) U_norm=3.1571 corr_std=0.000546\n",
      "    Epoch  200: Loss=0.404786 (Res=0.000198, Orth=0.040273, Proj=0.077514) U_norm=18.9547 corr_std=0.112880\n",
      "    Epoch  400: Loss=0.004728 (Res=0.000458, Orth=0.000001, Proj=0.141191) U_norm=25.3831 corr_std=0.156705\n",
      "    Epoch  600: Loss=0.004114 (Res=0.000397, Orth=0.000001, Proj=0.141196) U_norm=25.3861 corr_std=0.156705\n",
      "    Epoch  800: Loss=0.003605 (Res=0.000346, Orth=0.000000, Proj=0.141160) U_norm=25.3859 corr_std=0.156708\n",
      "    Epoch 1000: Loss=0.003191 (Res=0.000305, Orth=0.000000, Proj=0.141158) U_norm=25.3876 corr_std=0.156704\n",
      "    Epoch 1200: Loss=0.002810 (Res=0.000267, Orth=0.000000, Proj=0.141200) U_norm=25.3916 corr_std=0.156716\n",
      "    Epoch 1400: Loss=0.002460 (Res=0.000231, Orth=0.000000, Proj=0.141237) U_norm=25.3947 corr_std=0.156725\n",
      "    Epoch 1499: Loss=0.002298 (Res=0.000215, Orth=0.000000, Proj=0.141280) U_norm=25.3978 corr_std=0.156731\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_1_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.071 0.394 0.962 1.226 1.372 1.575 2.076 3.15  3.326 3.528]\n",
      "\n",
      "LEVEL 2: refine 2048 -> 2503, epochs=1000\n",
      "  Computing Laplacian for 2503 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=1000, lr=0.0007071067811865476, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.246843 (Res=0.000125, Orth=0.024556, Proj=0.031930) U_norm=21.7881 corr_std=0.135258\n",
      "    Epoch  200: Loss=0.002297 (Res=0.000224, Orth=0.000000, Proj=0.053930) U_norm=27.7713 corr_std=0.176902\n",
      "    Epoch  400: Loss=0.002122 (Res=0.000207, Orth=0.000000, Proj=0.053951) U_norm=27.7811 corr_std=0.176951\n",
      "    Epoch  600: Loss=0.001994 (Res=0.000194, Orth=0.000000, Proj=0.053975) U_norm=27.7885 corr_std=0.176983\n",
      "    Epoch  800: Loss=0.001891 (Res=0.000184, Orth=0.000000, Proj=0.053983) U_norm=27.7920 corr_std=0.176992\n",
      "    Epoch  999: Loss=0.001805 (Res=0.000175, Orth=0.000000, Proj=0.054010) U_norm=27.8015 corr_std=0.177031\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_2_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.09  0.456 0.979 1.349 1.777 1.929 2.624 3.389 3.618 4.24 ]\n",
      "\n",
      "Done. Final eigenvalues: [0.09  0.456 0.979 1.349 1.777 1.929 2.624 3.389 3.618 4.24 ]\n"
     ]
    }
   ],
   "source": [
    "U_pred = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ed67aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.5931e+01, -1.1700e+00, -2.3900e-01,  4.1250e+00, -1.6180e+00,\n",
       "         2.7710e+00, -1.7580e+00, -5.0000e-02,  4.3860e+00,  3.0300e+00],\n",
       "       [-1.1700e+00,  7.4879e+01, -2.5010e+00,  1.5660e+00, -2.0390e+00,\n",
       "        -1.5900e-01, -2.1210e+00,  2.8400e-01,  7.9900e-01,  1.0640e+00],\n",
       "       [-2.3900e-01, -2.5010e+00,  7.5439e+01,  4.6060e+00,  4.3600e-01,\n",
       "         3.0010e+00, -2.2270e+00, -1.6500e+00,  1.9160e+00,  6.4100e-01],\n",
       "       [ 4.1250e+00,  1.5660e+00,  4.6060e+00,  8.0014e+01, -1.3900e+00,\n",
       "         2.5500e-01,  1.1350e+00,  3.4430e+00,  3.7500e-01, -3.9500e-01],\n",
       "       [-1.6180e+00, -2.0390e+00,  4.3600e-01, -1.3900e+00,  7.6387e+01,\n",
       "        -5.9100e-01,  2.8290e+00, -2.2240e+00,  2.0910e+00,  2.1900e+00],\n",
       "       [ 2.7710e+00, -1.5900e-01,  3.0010e+00,  2.5500e-01, -5.9100e-01,\n",
       "         8.0063e+01,  3.8430e+00, -1.4230e+00,  1.0540e+00, -1.7680e+00],\n",
       "       [-1.7580e+00, -2.1210e+00, -2.2270e+00,  1.1350e+00,  2.8290e+00,\n",
       "         3.8430e+00,  7.2624e+01, -1.8720e+00, -3.8900e-01, -1.6700e+00],\n",
       "       [-5.0000e-02,  2.8400e-01, -1.6500e+00,  3.4430e+00, -2.2240e+00,\n",
       "        -1.4230e+00, -1.8720e+00,  7.9591e+01,  3.2850e+00,  1.5920e+00],\n",
       "       [ 4.3860e+00,  7.9900e-01,  1.9160e+00,  3.7500e-01,  2.0910e+00,\n",
       "         1.0540e+00, -3.8900e-01,  3.2850e+00,  8.0622e+01, -1.7600e+00],\n",
       "       [ 3.0300e+00,  1.0640e+00,  6.4100e-01, -3.9500e-01,  2.1900e+00,\n",
       "        -1.7680e+00, -1.6700e+00,  1.5920e+00, -1.7600e+00,  7.7740e+01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(U_pred.T @ U_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a5b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "cells = [('triangle', m.connectivity)]\n",
    "m_out = meshio.Mesh(m.verts, cells, point_data={f'v{i}': U_pred[:, i] for i in range(1, 10)})\n",
    "\n",
    "m_out.write('bunny_eigfuncs_pred_seq.vtu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9342ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete fixed version of multigrid + GNN eigen-refinement pipeline.\n",
    "\n",
    "Key improvements:\n",
    " - Fixed dimension consistency (U_coarse subsetting)\n",
    " - Adaptive per-mode correction scaling\n",
    " - Best model checkpointing during training\n",
    " - Enhanced monitoring and validation\n",
    " - Eigenvalue quality metrics\n",
    " - SMOOTHNESS REGULARIZATION to prevent high-frequency noise\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh, spsolve\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports (must be available in your environment)\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utility helpers\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    \"\"\"Convert scipy sparse matrix to torch.sparse_coo_tensor.\"\"\"\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    \"\"\"Normalize numpy matrix columns to have unit L2 norm. Returns normalized U and norms.\"\"\"\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    \"\"\"Normalize torch tensor columns to have unit L2 norm. Returns normalized U and norms (torch).\"\"\"\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "def validate_eigenvalues(U, L, M, lambda_vals):\n",
    "    \"\"\"\n",
    "    Validate that U, lambda satisfy L*U = lambda*M*U\n",
    "    Returns max relative residual norm\n",
    "    \"\"\"\n",
    "    from scipy.sparse import issparse\n",
    "    n_modes = U.shape[1]\n",
    "    residuals = []\n",
    "    \n",
    "    for i in range(n_modes):\n",
    "        u = U[:, i].reshape(-1, 1)\n",
    "        lam = lambda_vals[i]\n",
    "        \n",
    "        Lu = L @ u if issparse(L) else L.dot(u)\n",
    "        Mu = M @ u if issparse(M) else M.dot(u)\n",
    "        residual = Lu - lam * Mu\n",
    "        \n",
    "        res_norm = np.linalg.norm(residual)\n",
    "        u_norm = np.linalg.norm(Mu)\n",
    "        \n",
    "        rel_residual = res_norm / (abs(lam) * u_norm + 1e-12)\n",
    "        residuals.append(rel_residual)\n",
    "    \n",
    "    return max(residuals), np.array(residuals)\n",
    "\n",
    "\n",
    "def smooth_eigenfunctions(U, L, M, n_iters=3, tau=0.01):\n",
    "    \"\"\"\n",
    "    Smooth eigenfunctions using implicit smoothing: (M + tau*L) * U_new = M * U_old\n",
    "    This reduces high-frequency oscillations while preserving eigenspace.\n",
    "    \n",
    "    AGGRESSIVE VERSION: Uses larger tau and more iterations.\n",
    "    \"\"\"\n",
    "    U_smooth = U.copy()\n",
    "    \n",
    "    # Use progressively larger tau for more smoothing\n",
    "    for it in range(n_iters):\n",
    "        # Increase tau over iterations for stronger smoothing\n",
    "        current_tau = tau * (1.0 + 0.5 * it / max(1, n_iters))\n",
    "        A = M + current_tau * L\n",
    "        \n",
    "        for i in range(U.shape[1]):\n",
    "            rhs = M @ U_smooth[:, i]\n",
    "            U_smooth[:, i] = spsolve(A, rhs)\n",
    "    \n",
    "    return U_smooth\n",
    "\n",
    "\n",
    "def m_orthonormalize(U, M):\n",
    "    \"\"\"Explicit M-orthonormalization using Cholesky.\"\"\"\n",
    "    from scipy.linalg import cholesky, solve_triangular\n",
    "    \n",
    "    M_dense = M.toarray() if hasattr(M, 'toarray') else M\n",
    "    \n",
    "    # Compute M-weighted Gram matrix\n",
    "    MU = M_dense @ U\n",
    "    G = U.T @ MU\n",
    "    \n",
    "    # Cholesky factorization: G = L @ L.T\n",
    "    try:\n",
    "        L_chol = cholesky(G, lower=True)\n",
    "        # M-orthonormalize: U_orth = U @ inv(L)\n",
    "        U_orth = solve_triangular(L_chol.T, U.T, lower=False).T\n",
    "        return U_orth\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"    Warning: Cholesky failed, Gram matrix not positive definite. Skipping orthonormalization.\")\n",
    "        return U\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Adaptive corrector with per-mode scaling\n",
    "# ------------------------\n",
    "class AdaptiveCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 64, 32), dropout=0.0, init_scale=0.01):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2  # because we concat self + neighbor-mean\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Learnable per-mode scaling factors\n",
    "        self.mode_scales = nn.Parameter(torch.ones(out_dim) * init_scale)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [n, in_dim]\n",
    "        edge_index: LongTensor shape [2, n_edges]\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        n = x.shape[0]\n",
    "        # aggregate neighbor features: mean aggregator\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device)\n",
    "        deg = deg.clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)\n",
    "        correction = self.net(h)\n",
    "        \n",
    "        # Apply per-mode adaptive scaling\n",
    "        return correction * self.mode_scales.unsqueeze(0)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid eigensolver with GNN corrector\n",
    "# ------------------------\n",
    "class MultigridEigensolver:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Enhanced training routine WITH SMOOTHNESS REGULARIZATION\n",
    "    # ------------------------\n",
    "    def train_gnn(self, model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                  n_modes,\n",
    "                  epochs=200,\n",
    "                  lr=1e-3,\n",
    "                  w_res=10.0,\n",
    "                  w_orth=1.0,\n",
    "                  w_proj=1e-3,\n",
    "                  w_smooth=1.0,  # NEW: smoothness regularization weight\n",
    "                  grad_clip=1.0,\n",
    "                  weight_decay=1e-6,\n",
    "                  log_every=200):\n",
    "        \"\"\"\n",
    "        Train corrector model with smoothness regularization to prevent high-frequency noise.\n",
    "        \n",
    "        NEW: w_smooth controls Laplacian smoothness penalty on corrections.\n",
    "        Higher w_smooth = smoother corrections = less speckled eigenfunctions.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Convert sparse matrices to torch sparse on device\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(device)\n",
    "        R_t = sp_to_torch_sparse(P.T).to(device)\n",
    "\n",
    "        # Normalize columns of U_init and U_coarse\n",
    "        U_init_normed, uinit_norms = normalize_columns_np(U_init)\n",
    "        U_coarse_normed, ucoarse_norms = normalize_columns_np(U_coarse)\n",
    "\n",
    "        U_init_t = torch.FloatTensor(U_init_normed).to(device)\n",
    "        U_coarse_t = torch.FloatTensor(U_coarse_normed).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        n_fine = U_init_t.shape[0]\n",
    "        n_coarse = U_coarse_t.shape[0]\n",
    "        denom_res = float(max(1, n_fine * n_modes))\n",
    "        denom_proj = float(max(1, n_coarse * n_modes))\n",
    "        I = torch.eye(n_modes, device=device)\n",
    "\n",
    "        # Track best model\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        best_epoch = 0\n",
    "\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            corr = model(x_feats, edge_index)  # Already scaled by learnable mode_scales\n",
    "            U_pred = U_init_t + corr\n",
    "\n",
    "            # Rayleigh-related tensors\n",
    "            Lu = torch.sparse.mm(L_t, U_pred)\n",
    "            Mu = torch.sparse.mm(M_t, U_pred)\n",
    "            num = torch.sum(U_pred * Lu, dim=0)\n",
    "            den = torch.sum(U_pred * Mu, dim=0) + 1e-12\n",
    "            lambdas = num / den\n",
    "\n",
    "            # Residual loss (normalized)\n",
    "            res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "            L_res = torch.sum(res**2) / denom_res\n",
    "\n",
    "            # Orthonormality loss (M-weighted Gram)\n",
    "            MUt = torch.sparse.mm(M_t, U_pred)\n",
    "            Gram = U_pred.t() @ MUt\n",
    "            L_orth = torch.sum((Gram - I)**2) / (n_modes * n_modes)\n",
    "\n",
    "            # Projection loss\n",
    "            proj = torch.sparse.mm(R_t, U_pred)\n",
    "            L_proj = torch.sum((proj - U_coarse_t)**2) / denom_proj\n",
    "\n",
    "            # NEW: Smoothness loss - penalize high Laplacian energy of CORRECTION\n",
    "            # This prevents high-frequency oscillations/speckles\n",
    "            # We penalize corr.T @ L @ corr, which measures \"roughness\"\n",
    "            L_corr = torch.sparse.mm(L_t, corr)\n",
    "            L_smooth_corr = torch.sum(corr * L_corr) / denom_res\n",
    "            \n",
    "            # ADDITIONAL: Also penalize total Laplacian energy of U_pred\n",
    "            # This enforces that final eigenfunctions are smooth\n",
    "            L_smooth_total = torch.sum(U_pred * Lu) / denom_res\n",
    "\n",
    "            # Combined loss with BOTH smoothness terms\n",
    "            loss = w_res * L_res + w_orth * L_orth + w_proj * L_proj + w_smooth * (L_smooth_corr + L_smooth_total)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track best model\n",
    "            current_loss = loss.item()\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                best_epoch = ep\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "            if (ep % log_every == 0) or (ep == epochs - 1):\n",
    "                with torch.no_grad():\n",
    "                    u_norm = float(U_pred.norm().cpu().item())\n",
    "                    corr_norm = float(corr.norm().cpu().item())\n",
    "                    corr_std = float(corr.std().cpu().item())\n",
    "                    corr_mean = float(corr.mean().cpu().item())\n",
    "                    lambdas_np = lambdas.cpu().numpy()\n",
    "                    \n",
    "                    # Get mode scales\n",
    "                    scales = model.mode_scales.detach().cpu().numpy()\n",
    "                    \n",
    "                    print(f\"    Epoch {ep:4d}: Loss={current_loss:.6f} \"\n",
    "                          f\"(Res={L_res.item():.6f}, Orth={L_orth.item():.6f}, \"\n",
    "                          f\"Proj={L_proj.item():.6f}, Smooth={L_smooth_corr.item():.4f}+{L_smooth_total.item():.4f})\")\n",
    "                    print(f\"              U_norm={u_norm:.4f} corr_norm={corr_norm:.6f} \"\n",
    "                          f\"corr_std={corr_std:.6f} corr_mean={corr_mean:.6f}\")\n",
    "                    print(f\"              Lambdas: {np.round(lambdas_np[:5], 4)}\")\n",
    "                    print(f\"              Scales:  {np.round(scales[:5], 6)}\")\n",
    "\n",
    "        # Restore best model\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "            print(f\"    --> Restored best model from epoch {best_epoch} (loss={best_loss:.6f})\")\n",
    "\n",
    "        # Final prediction with best model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            corr = model(x_feats, edge_index)\n",
    "            U_pred = U_init_t + corr\n",
    "\n",
    "        # Denormalize: multiply columns by original column norms of U_init\n",
    "        U_pred_np = U_pred.detach().cpu().numpy() * uinit_norms.reshape(1, -1)\n",
    "        return U_pred_np\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement with optional smoothing\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L_fine, M_fine, apply_smoothing=True, smooth_iters=3, tau=0.1):\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(self.device)\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        \n",
    "        # Optional: Apply smoothing to remove any remaining high-frequency noise\n",
    "        if apply_smoothing:\n",
    "            print(f\"    Applying {smooth_iters} iterations of implicit smoothing (tau={tau})...\")\n",
    "            U_refined = smooth_eigenfunctions(U_refined, L_fine, M_fine, n_iters=smooth_iters, tau=tau)\n",
    "        \n",
    "        # Explicit M-orthonormalization to ensure identity Gram matrix\n",
    "        U_refined = m_orthonormalize(U_refined, M_fine)\n",
    "        \n",
    "        return vals, U_refined\n",
    "\n",
    "    # ------------------------\n",
    "    # Refine one level (coarse -> fine) - FIXED VERSION\n",
    "    # ------------------------\n",
    "    def refine_level(self, X_coarse, U_coarse, X_fine, n_modes,\n",
    "                     hidden_sizes=(128, 64, 32),\n",
    "                     dropout=0.0,\n",
    "                     k_neighbors=4,\n",
    "                     epochs=200,\n",
    "                     lr=1e-3,\n",
    "                     init_scale=0.01,\n",
    "                     w_res=10.0,\n",
    "                     w_orth=1.0,\n",
    "                     w_proj=1e-3,\n",
    "                     w_smooth=1.0,  # NEW: smoothness weight\n",
    "                     freeze_layers=0,\n",
    "                     checkpoint_name=None,\n",
    "                     validate=False,\n",
    "                     apply_smoothing=True,\n",
    "                     smooth_iters=3,\n",
    "                     tau=0.1):  # NEW: smoothing strength parameter\n",
    "        \"\"\"\n",
    "        Single-level refinement with dimension fixes and smoothness regularization.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        print(f\"  Computing Laplacian for {X_fine.shape[0]} points...\")\n",
    "        L_fine, M_fine = robust_laplacian.point_cloud_laplacian(X_fine)\n",
    "\n",
    "        print(\"  Building prolongation operator...\")\n",
    "        P = self.build_prolongation(X_coarse, X_fine, k=1)\n",
    "\n",
    "        print(\"  Building kNN graph...\")\n",
    "        edge_index = self.build_knn_graph(X_fine, k=k_neighbors).to(device)\n",
    "\n",
    "        # CRITICAL FIX: Ensure U_coarse only uses first n_modes columns\n",
    "        if U_coarse.shape[1] > n_modes:\n",
    "            print(f\"  WARNING: U_coarse has {U_coarse.shape[1]} modes, taking first {n_modes}\")\n",
    "            U_coarse_subset = U_coarse[:, :n_modes]\n",
    "        else:\n",
    "            U_coarse_subset = U_coarse\n",
    "        \n",
    "        # Build U_init on fine grid\n",
    "        U_init = P @ U_coarse_subset  # shape [n_fine, n_modes]\n",
    "        \n",
    "        # Dimension validation\n",
    "        assert U_init.shape == (X_fine.shape[0], n_modes), \\\n",
    "            f\"U_init has wrong shape: {U_init.shape}, expected ({X_fine.shape[0]}, {n_modes})\"\n",
    "\n",
    "        # Build features: coords + U_init\n",
    "        x_feats = torch.FloatTensor(np.hstack([X_fine, U_init])).to(device)\n",
    "\n",
    "        in_dim = x_feats.shape[1]\n",
    "        out_dim = n_modes\n",
    "        \n",
    "        expected_in_dim = X_fine.shape[1] + n_modes\n",
    "        assert in_dim == expected_in_dim, f\"Feature dim mismatch: {in_dim} != {expected_in_dim}\"\n",
    "\n",
    "        print(f\"  Feature dimensions: coords={X_fine.shape[1]}, modes={n_modes}, total_in={in_dim}\")\n",
    "\n",
    "        if self.model is None:\n",
    "            print(f\"  Creating new corrector model (in_dim={in_dim}, out_dim={out_dim})...\")\n",
    "            self.model = AdaptiveCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, \n",
    "                                          dropout=dropout, init_scale=init_scale).to(device)\n",
    "        else:\n",
    "            # Check if input dimension matches\n",
    "            existing_in_dim = None\n",
    "            for m in self.model.net:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    existing_in_dim = m.in_features\n",
    "                    break\n",
    "            \n",
    "            if existing_in_dim != in_dim:\n",
    "                print(f\"  WARNING: Input dim changed {existing_in_dim} -> {in_dim}. Recreating model...\")\n",
    "                old_state = self.model.state_dict()\n",
    "                self.model = AdaptiveCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, \n",
    "                                              dropout=dropout, init_scale=init_scale).to(device)\n",
    "                # Try to copy matching weights\n",
    "                new_state = self.model.state_dict()\n",
    "                for k, v in old_state.items():\n",
    "                    if k in new_state and old_state[k].shape == new_state[k].shape:\n",
    "                        new_state[k] = old_state[k]\n",
    "                self.model.load_state_dict(new_state)\n",
    "            else:\n",
    "                print(f\"  Reusing existing model (in_dim={in_dim})\")\n",
    "\n",
    "        # Optionally freeze layers\n",
    "        if freeze_layers > 0:\n",
    "            linear_count = 0\n",
    "            for module in self.model.net:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    linear_count += 1\n",
    "                    if linear_count <= freeze_layers:\n",
    "                        for p in module.parameters():\n",
    "                            p.requires_grad = False\n",
    "            print(f\"  Frozen first {freeze_layers} linear layers.\")\n",
    "\n",
    "        print(f\"  Training corrector: epochs={epochs}, lr={lr}, init_scale={init_scale}, w_smooth={w_smooth}\")\n",
    "        U_pred = self.train_gnn(self.model, x_feats, edge_index, U_init, L_fine, M_fine, \n",
    "                                U_coarse_subset, P, n_modes,\n",
    "                                epochs=epochs, lr=lr, w_res=w_res, w_orth=w_orth, \n",
    "                                w_proj=w_proj, w_smooth=w_smooth)\n",
    "\n",
    "        print(\"  Rayleigh-Ritz refinement...\")\n",
    "        lambda_refined, U_refined = self.refine_eigenvectors(U_pred, L_fine, M_fine, \n",
    "                                                             apply_smoothing=apply_smoothing,\n",
    "                                                             smooth_iters=smooth_iters,\n",
    "                                                             tau=tau)\n",
    "\n",
    "        # Validation\n",
    "        if validate:\n",
    "            print(\"  Validating eigenvalue quality...\")\n",
    "            max_res, residuals = validate_eigenvalues(U_refined, L_fine, M_fine, lambda_refined)\n",
    "            print(f\"    Max relative residual: {max_res:.2e}\")\n",
    "            print(f\"    Residuals per mode: {np.round(residuals[:5], 8)}\")\n",
    "\n",
    "        if checkpoint_name is not None:\n",
    "            ckpt = {\"model_state\": self.model.state_dict(), \n",
    "                   \"lambda_refined\": lambda_refined,\n",
    "                   \"max_residual\": max_res if validate else None}\n",
    "            torch.save(ckpt, os.path.join(self.checkpoint_dir, checkpoint_name))\n",
    "            print(f\"  Saved checkpoint: {os.path.join(self.checkpoint_dir, checkpoint_name)}\")\n",
    "\n",
    "        return lambda_refined, U_refined, L_fine, M_fine\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Visualization helper\n",
    "# ------------------------\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None, show=True):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2],\n",
    "                    triangles=mesh.connectivity, alpha=0.35)\n",
    "    if highlight_indices is not None:\n",
    "        hv = mesh.verts[highlight_indices]\n",
    "        ax.scatter(hv[:, 0], hv[:, 1], hv[:, 2], s=6, label=f\"{len(highlight_indices)} pts\")\n",
    "        ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=120, azim=-90)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main script\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 11\n",
    "    hidden_sizes = (128, 128, 128)\n",
    "    dropout = 0.0\n",
    "\n",
    "    # Enhanced schedule with AGGRESSIVE smoothness regularization\n",
    "    epochs_schedule = {0: 0, 1: 1500, 2: 1000, 3: 800, 4: 800}\n",
    "    hierarchy = [1024, 2048, 4096]\n",
    "    k_neighbors = 4\n",
    "    lr_start = 5e-4    # Reduced learning rate\n",
    "    lr_min = 1e-4\n",
    "    init_scale = 0.0001  # VERY SMALL corrections\n",
    "    \n",
    "    # Loss weights - AGGRESSIVE SMOOTHNESS\n",
    "    w_res = 1000.0     # Very high residual weight\n",
    "    w_orth = 10.0\n",
    "    w_proj = 0.1       # Reduced projection weight\n",
    "    w_smooth = 1000.0  # VERY HIGH smoothness weight\n",
    "    \n",
    "    freeze_schedule = {1: 0, 2: 1, 3: 1, 4: 2}\n",
    "    \n",
    "    # Post-processing smoothing - MORE AGGRESSIVE\n",
    "    apply_smoothing = True\n",
    "    smooth_iters = 20  # Increased from 5 to 20\n",
    "    tau = 0.1          # Larger tau = more aggressive smoothing\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridEigensolver.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    print(f\"Mesh loaded: {n_total} vertices\")\n",
    "\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    all_idx = np.arange(n_total)\n",
    "    rng.shuffle(all_idx)\n",
    "    indices_per_level = {}\n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        indices_per_level[i] = all_idx[:n_points].copy()\n",
    "        print(f\"  Level {i}: {n_points} points (nested)\")\n",
    "\n",
    "    solver = MultigridEigensolver()\n",
    "\n",
    "    # Level 0 coarse solve\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(\"\\nLEVEL 0: coarse solving...\")\n",
    "    lambda_cur, U_cur, L_cur, M_cur = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda_cur, 6))\n",
    "    \n",
    "    # Validate initial\n",
    "    max_res, _ = validate_eigenvalues(U_cur, L_cur, M_cur, lambda_cur)\n",
    "    print(f\"Initial max residual: {max_res:.2e}\")\n",
    "\n",
    "    # Iterative refinement\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level - 1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "        epochs = epochs_schedule.get(level, 1000)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"LEVEL {level}: refine {Xc.shape[0]} -> {Xf.shape[0]}, epochs={epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        freeze_layers = freeze_schedule.get(level, 0)\n",
    "\n",
    "        total_levels = len(hierarchy)\n",
    "        decay = (level - 1) / max(1, total_levels - 1)\n",
    "        lr = lr_start * ((lr_min / lr_start) ** decay)\n",
    "\n",
    "        lambda_cur, U_cur, L_cur, M_cur = solver.refine_level(\n",
    "            Xc, U_cur, Xf, n_modes,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            k_neighbors=k_neighbors,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            init_scale=init_scale,\n",
    "            w_res=w_res,\n",
    "            w_orth=w_orth,\n",
    "            w_proj=w_proj,\n",
    "            w_smooth=w_smooth,  # NEW\n",
    "            freeze_layers=freeze_layers,\n",
    "            checkpoint_name=f\"level_{level}_ckpt.pt\",\n",
    "            validate=True,\n",
    "            apply_smoothing=apply_smoothing,\n",
    "            smooth_iters=smooth_iters,\n",
    "            tau=tau  # NEW\n",
    "        )\n",
    "\n",
    "        print(f\"\\nGNN-refined eigenvalues: {np.round(lambda_cur, 4)}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Final eigenvalues: {np.round(lambda_cur, 4)}\")\n",
    "    \n",
    "    return U_cur, L_cur, M_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ead6e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Mesh loaded: 2503 vertices\n",
      "Hierarchy: [1024, 2048, 2503]\n",
      "  Level 0: 1024 points (nested)\n",
      "  Level 1: 2048 points (nested)\n",
      "  Level 2: 2503 points (nested)\n",
      "\n",
      "LEVEL 0: coarse solving...\n",
      "Coarse eigenvalues: [0.   0.33 0.77 0.84 1.06 1.23 1.74 2.63 2.9  3.12 3.27]\n",
      "Initial max residual: 1.59e-01\n",
      "\n",
      "============================================================\n",
      "LEVEL 1: refine 1024 -> 2048, epochs=1500\n",
      "============================================================\n",
      "  Computing Laplacian for 2048 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Feature dimensions: coords=3, modes=11, total_in=14\n",
      "  Creating new corrector model (in_dim=14, out_dim=11)...\n",
      "  Training corrector: epochs=1500, lr=0.0005, init_scale=0.0001, w_smooth=1000.0\n",
      "    Epoch    0: Loss=0.962880 (Res=0.000058, Orth=0.088140, Proj=0.000700, Smooth=0.0000+0.0000)\n",
      "              U_norm=3.3167 corr_norm=0.001102 corr_std=0.000007 corr_mean=0.000000\n",
      "              Lambdas: [-0.    0.59  2.08  1.71  2.19]\n",
      "              Scales:  [ 0.  0. -0.  0. -0.]\n",
      "    Epoch  200: Loss=0.731226 (Res=0.000017, Orth=0.057686, Proj=0.037780, Smooth=0.0001+0.0001)\n",
      "              U_norm=14.0389 corr_norm=12.024937 corr_std=0.079416 corr_mean=-0.010585\n",
      "              Lambdas: [0.01 0.32 0.38 0.86 1.14]\n",
      "              Scales:  [ 0.03  0.05 -0.05  0.06 -0.09]\n",
      "    Epoch  400: Loss=0.688405 (Res=0.000013, Orth=0.048644, Proj=0.047054, Smooth=0.0001+0.0001)\n",
      "              U_norm=15.8052 corr_norm=13.304080 corr_std=0.087443 corr_mean=-0.014523\n",
      "              Lambdas: [0.01 0.31 0.81 0.87 1.1 ]\n",
      "              Scales:  [ 0.03  0.06 -0.12  0.07 -0.11]\n",
      "    Epoch  600: Loss=0.679588 (Res=0.000009, Orth=0.047280, Proj=0.048141, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.0564 corr_norm=13.412805 corr_std=0.088272 corr_mean=-0.013935\n",
      "              Lambdas: [0.   0.3  0.79 0.86 1.09]\n",
      "              Scales:  [ 0.04  0.06 -0.14  0.08 -0.12]\n",
      "    Epoch  800: Loss=0.676065 (Res=0.000007, Orth=0.046752, Proj=0.048643, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.1508 corr_norm=13.481368 corr_std=0.088705 corr_mean=-0.014120\n",
      "              Lambdas: [0.   0.3  0.78 0.86 1.08]\n",
      "              Scales:  [ 0.04  0.06 -0.15  0.08 -0.13]\n",
      "    Epoch 1000: Loss=0.674031 (Res=0.000006, Orth=0.046441, Proj=0.048935, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.2054 corr_norm=13.521912 corr_std=0.088927 corr_mean=-0.014442\n",
      "              Lambdas: [0.   0.3  0.78 0.86 1.08]\n",
      "              Scales:  [ 0.04  0.06 -0.16  0.08 -0.14]\n",
      "    Epoch 1200: Loss=0.672767 (Res=0.000005, Orth=0.046229, Proj=0.049173, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.2468 corr_norm=13.554305 corr_std=0.089151 corr_mean=-0.014407\n",
      "              Lambdas: [0.   0.3  0.78 0.85 1.08]\n",
      "              Scales:  [ 0.04  0.07 -0.16  0.09 -0.14]\n",
      "    Epoch 1400: Loss=0.671839 (Res=0.000005, Orth=0.046065, Proj=0.049366, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.2817 corr_norm=13.583113 corr_std=0.089322 corr_mean=-0.014551\n",
      "              Lambdas: [0.   0.3  0.78 0.85 1.08]\n",
      "              Scales:  [ 0.04  0.07 -0.17  0.09 -0.15]\n",
      "    Epoch 1499: Loss=0.671475 (Res=0.000005, Orth=0.046011, Proj=0.049427, Smooth=0.0001+0.0001)\n",
      "              U_norm=16.2940 corr_norm=13.593092 corr_std=0.089361 corr_mean=-0.014727\n",
      "              Lambdas: [0.   0.3  0.77 0.85 1.08]\n",
      "              Scales:  [ 0.04  0.07 -0.17  0.09 -0.15]\n",
      "    --> Restored best model from epoch 1494 (loss=0.671466)\n",
      "  Rayleigh-Ritz refinement...\n",
      "    Applying 20 iterations of implicit smoothing (tau=0.1)...\n",
      "  Validating eigenvalue quality...\n",
      "    Max relative residual: 2.07e+00\n",
      "    Residuals per mode: [2.07 0.02 0.01 0.01 0.01]\n",
      "  Saved checkpoint: ./checkpoints/level_1_ckpt.pt\n",
      "\n",
      "GNN-refined eigenvalues: [0.   0.3  0.77 0.85 1.08 1.23 1.78 2.69 3.05 3.3  3.43]\n",
      "\n",
      "============================================================\n",
      "LEVEL 2: refine 2048 -> 2503, epochs=1000\n",
      "============================================================\n",
      "  Computing Laplacian for 2503 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Feature dimensions: coords=3, modes=11, total_in=14\n",
      "  WARNING: Input dim changed 28 -> 14. Recreating model...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=1000, lr=0.00022360679774997898, init_scale=0.0001, w_smooth=1000.0\n",
      "    Epoch    0: Loss=0.761611 (Res=0.000027, Orth=0.058651, Proj=0.011400, Smooth=0.0001+0.0001)\n",
      "              U_norm=14.4339 corr_norm=13.290612 corr_std=0.078628 corr_mean=-0.015278\n",
      "              Lambdas: [0.   0.59 1.23 0.93 1.45]\n",
      "              Scales:  [ 0.04  0.07 -0.17  0.09 -0.15]\n",
      "    Epoch  200: Loss=0.656564 (Res=0.000014, Orth=0.044159, Proj=0.018838, Smooth=0.0001+0.0001)\n",
      "              U_norm=18.2019 corr_norm=17.172941 corr_std=0.102590 corr_mean=-0.013669\n",
      "              Lambdas: [0.   0.32 0.76 0.87 1.08]\n",
      "              Scales:  [ 0.04  0.08 -0.18  0.09 -0.15]\n",
      "    Epoch  400: Loss=0.649472 (Res=0.000011, Orth=0.043407, Proj=0.019023, Smooth=0.0001+0.0001)\n",
      "              U_norm=18.3625 corr_norm=17.233849 corr_std=0.103149 corr_mean=-0.012158\n",
      "              Lambdas: [0.   0.31 0.75 0.86 1.06]\n",
      "              Scales:  [ 0.04  0.08 -0.19  0.09 -0.15]\n",
      "    Epoch  600: Loss=0.642160 (Res=0.000010, Orth=0.042847, Proj=0.018827, Smooth=0.0001+0.0001)\n",
      "              U_norm=18.4924 corr_norm=17.083410 corr_std=0.102441 corr_mean=-0.010295\n",
      "              Lambdas: [0.   0.31 0.74 0.86 0.64]\n",
      "              Scales:  [ 0.04  0.08 -0.2   0.1  -0.12]\n",
      "    Epoch  800: Loss=0.635203 (Res=0.000009, Orth=0.042403, Proj=0.018442, Smooth=0.0001+0.0001)\n",
      "              U_norm=18.5895 corr_norm=16.821749 corr_std=0.100456 corr_mean=-0.013654\n",
      "              Lambdas: [0.   0.31 0.74 0.86 0.76]\n",
      "              Scales:  [ 0.04  0.09 -0.22  0.11 -0.09]\n",
      "    Epoch  999: Loss=0.630517 (Res=0.000008, Orth=0.042069, Proj=0.017943, Smooth=0.0001+0.0001)\n",
      "              U_norm=18.6698 corr_norm=16.488110 corr_std=0.098430 corr_mean=-0.013633\n",
      "              Lambdas: [0.   0.32 0.73 0.85 0.76]\n",
      "              Scales:  [ 0.04  0.07 -0.23  0.12 -0.09]\n",
      "    --> Restored best model from epoch 999 (loss=0.630517)\n",
      "  Rayleigh-Ritz refinement...\n",
      "    Applying 20 iterations of implicit smoothing (tau=0.1)...\n",
      "  Validating eigenvalue quality...\n",
      "    Max relative residual: 1.35e+01\n",
      "    Residuals per mode: [13.54  0.08  0.02  0.02  0.16]\n",
      "  Saved checkpoint: ./checkpoints/level_2_ckpt.pt\n",
      "\n",
      "GNN-refined eigenvalues: [0.   0.3  0.74 0.86 1.08 1.22 1.8  2.81 3.06 3.25 5.93]\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Final eigenvalues: [0.   0.3  0.74 0.86 1.08 1.22 1.8  2.81 3.06 3.25 5.93]\n",
      "\n",
      "M-orthonormal Gram matrix:\n",
      "[[ 0.99 -0.02  0.   -0.01  0.07 -0.    0.    0.18  0.04 -0.27  1.61]\n",
      " [-0.02  0.97  0.01 -0.01  0.13 -0.02  0.02  0.51 -0.18 -0.43  2.29]\n",
      " [ 0.    0.01  1.    0.   -0.01 -0.    0.    0.05 -0.03  0.04 -0.19]\n",
      " [-0.01 -0.01  0.    0.99  0.02 -0.   -0.    0.11 -0.19 -0.21  0.3 ]\n",
      " [ 0.07  0.13 -0.01  0.02  0.8   0.01 -0.02 -0.04 -0.03  0.12 -2.14]\n",
      " [-0.   -0.02 -0.   -0.    0.01  1.   -0.01 -0.01  0.02 -0.16  0.09]\n",
      " [ 0.    0.02  0.   -0.   -0.02 -0.01  1.    0.03 -0.01 -0.01  0.02]\n",
      " [ 0.18  0.51  0.05  0.11 -0.04 -0.01  0.03  1.28 -0.17 -0.26  1.76]\n",
      " [ 0.04 -0.18 -0.03 -0.19 -0.03  0.02 -0.01 -0.17  1.09  0.1  -0.4 ]\n",
      " [-0.27 -0.43  0.04 -0.21  0.12 -0.16 -0.01 -0.26  0.1   1.35 -2.22]\n",
      " [ 1.61  2.29 -0.19  0.3  -2.14  0.09  0.02  1.76 -0.4  -2.22 18.48]]\n"
     ]
    }
   ],
   "source": [
    "U_final, L_final, M_final = main()\n",
    "\n",
    "# Check M-orthonormality\n",
    "M_dense = M_final.toarray()\n",
    "gram = U_final.T @ M_dense @ U_final\n",
    "print(\"\\nM-orthonormal Gram matrix:\")\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=150)\n",
    "print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb80f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f795788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "cells = [('triangle', m.connectivity)]\n",
    "m_out = meshio.Mesh(m.verts, cells,point_data={f'v{i}': U_final[:, i] for i in range(1, 11)})\n",
    "\n",
    "m_out.write('bunny_eigfuncs_seq.vtu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e45f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
