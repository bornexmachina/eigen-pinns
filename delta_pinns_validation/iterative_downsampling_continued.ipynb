{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e986e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Mesh loaded: 2503 vertices\n",
      "Hierarchy: [32, 128, 512, 1024, 2503]\n",
      "  Level 0: 32 points (nested)\n",
      "  Level 1: 128 points (nested)\n",
      "  Level 2: 512 points (nested)\n",
      "  Level 3: 1024 points (nested)\n",
      "  Level 4: 2503 points (nested)\n",
      "\n",
      "LEVEL 0: coarse solving...\n",
      "Coarse eigenvalues: [-0.        0.359878  0.848913  1.158359  1.507636  1.78121   1.971736\n",
      "  2.292232  2.718251  2.782262]\n",
      "\n",
      "LEVEL 1: refine 32 -> 128, epochs=1500\n",
      "  Computing Laplacian for 128 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Creating new corrector model (in_dim=13, out_dim=10)...\n",
      "  Training corrector: epochs=1500, lr=0.001, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.732555 (Res=0.012540, Orth=0.060707, Proj=0.075425) U_norm=3.1661 corr_std=0.000681\n",
      "    Epoch  200: Loss=0.291884 (Res=0.009555, Orth=0.019600, Proj=0.331179) U_norm=5.7654 corr_std=0.085166\n",
      "    Epoch  400: Loss=0.055486 (Res=0.005403, Orth=0.000102, Proj=0.442424) U_norm=6.6783 corr_std=0.113448\n",
      "    Epoch  600: Loss=0.023319 (Res=0.002271, Orth=0.000018, Proj=0.433586) U_norm=6.6509 corr_std=0.116549\n",
      "    Epoch  800: Loss=0.014804 (Res=0.001428, Orth=0.000009, Proj=0.430467) U_norm=6.6362 corr_std=0.118901\n",
      "    Epoch 1000: Loss=0.010793 (Res=0.001029, Orth=0.000006, Proj=0.433758) U_norm=6.6380 corr_std=0.120912\n",
      "    Epoch 1200: Loss=0.008131 (Res=0.000765, Orth=0.000004, Proj=0.438561) U_norm=6.6406 corr_std=0.123220\n",
      "    Epoch 1400: Loss=0.006394 (Res=0.000593, Orth=0.000002, Proj=0.440219) U_norm=6.6347 corr_std=0.125122\n",
      "    Epoch 1499: Loss=0.005730 (Res=0.000525, Orth=0.000004, Proj=0.441689) U_norm=6.6337 corr_std=0.126260\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_1_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.005405 0.477795 0.871524 1.210911 1.465706 1.903703 2.300553 2.795263\n",
      " 3.044443 3.613509]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [0.       0.464393 0.860068 1.196359 1.413617 1.861207 2.273839 2.759782\n",
      " 2.87012  3.25778 ]\n",
      "  Relative errors:   [5.36281261e+09 2.88580000e-02 1.33200000e-02 1.21640000e-02\n",
      " 3.68480000e-02 2.28320000e-02 1.17480000e-02 1.28570000e-02\n",
      " 6.07370000e-02 1.09194000e-01]\n",
      "\n",
      "LEVEL 2: refine 128 -> 512, epochs=1000\n",
      "  Computing Laplacian for 512 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=1000, lr=0.0008408964152537145, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.359855 (Res=0.002428, Orth=0.033522, Proj=0.359261) U_norm=9.8131 corr_std=0.136225\n",
      "    Epoch  200: Loss=0.025959 (Res=0.002513, Orth=0.000018, Proj=0.648225) U_norm=13.0193 corr_std=0.187074\n",
      "    Epoch  400: Loss=0.019248 (Res=0.001851, Orth=0.000009, Proj=0.653275) U_norm=13.0593 corr_std=0.187590\n",
      "    Epoch  600: Loss=0.015315 (Res=0.001460, Orth=0.000006, Proj=0.655072) U_norm=13.0767 corr_std=0.187668\n",
      "    Epoch  800: Loss=0.012961 (Res=0.001226, Orth=0.000005, Proj=0.656449) U_norm=13.0917 corr_std=0.187804\n",
      "    Epoch  999: Loss=0.011451 (Res=0.001075, Orth=0.000004, Proj=0.658128) U_norm=13.1057 corr_std=0.187964\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_2_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.037138 0.534281 0.996192 1.369982 1.555423 1.973175 2.238186 3.049263\n",
      " 3.638124 4.28241 ]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [-0.        0.390175  0.85084   1.164091  1.297341  1.751831  1.869489\n",
      "  2.649577  2.911107  3.221319]\n",
      "  Relative errors:   [2.89189834e+10 3.69335000e-01 1.70834000e-01 1.76868000e-01\n",
      " 1.98931000e-01 1.26350000e-01 1.97218000e-01 1.50849000e-01\n",
      " 2.49739000e-01 3.29396000e-01]\n",
      "\n",
      "LEVEL 3: refine 512 -> 1024, epochs=800\n",
      "  Computing Laplacian for 1024 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=800, lr=0.0007071067811865476, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.092366 (Res=0.000877, Orth=0.008342, Proj=0.170955) U_norm=18.1783 corr_std=0.184931\n",
      "    Epoch  200: Loss=0.008574 (Res=0.000837, Orth=0.000002, Proj=0.180965) U_norm=18.7648 corr_std=0.189030\n",
      "    Epoch  400: Loss=0.007670 (Res=0.000747, Orth=0.000002, Proj=0.180938) U_norm=18.7664 corr_std=0.188995\n",
      "    Epoch  600: Loss=0.007085 (Res=0.000689, Orth=0.000002, Proj=0.180856) U_norm=18.7681 corr_std=0.188916\n",
      "    Epoch  799: Loss=0.006570 (Res=0.000637, Orth=0.000001, Proj=0.180712) U_norm=18.7683 corr_std=0.188817\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_3_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.053518 0.528231 1.006852 1.26008  1.470929 1.918295 2.18801  3.236952\n",
      " 3.806572 4.310052]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [-0.        0.333353  0.765464  0.8359    1.064607  1.2346    1.738815\n",
      "  2.626301  2.899525  3.119024]\n",
      "  Relative errors:   [4.99068719e+10 5.84602000e-01 3.15349000e-01 5.07454000e-01\n",
      " 3.81664000e-01 5.53778000e-01 2.58334000e-01 2.32514000e-01\n",
      " 3.12826000e-01 3.81859000e-01]\n",
      "\n",
      "LEVEL 4: refine 1024 -> 2503, epochs=800\n",
      "  Computing Laplacian for 2503 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 2 linear layers.\n",
      "  Training corrector: epochs=800, lr=0.0005946035575013605, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.121650 (Res=0.000353, Orth=0.011787, Proj=0.247549) U_norm=28.7330 corr_std=0.180706\n",
      "    Epoch  200: Loss=0.003836 (Res=0.000360, Orth=0.000001, Proj=0.230346) U_norm=27.7465 corr_std=0.174050\n",
      "    Epoch  400: Loss=0.003663 (Res=0.000343, Orth=0.000000, Proj=0.230427) U_norm=27.7519 corr_std=0.174064\n",
      "    Epoch  600: Loss=0.003494 (Res=0.000326, Orth=0.000000, Proj=0.230284) U_norm=27.7467 corr_std=0.174014\n",
      "    Epoch  799: Loss=0.003370 (Res=0.000314, Orth=0.000000, Proj=0.230228) U_norm=27.7457 corr_std=0.173987\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_4_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.070825 0.497811 1.006867 1.234324 1.487097 1.892893 2.40418  3.34443\n",
      " 3.99172  4.930622]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [-0.        0.287932  0.721703  0.841517  1.03936   1.202406  1.762114\n",
      "  2.59946   2.923435  2.972727]\n",
      "  Relative errors:   [2.88252915e+10 7.28919000e-01 3.95126000e-01 4.66785000e-01\n",
      " 4.30782000e-01 5.74254000e-01 3.64372000e-01 2.86586000e-01\n",
      " 3.65421000e-01 6.58619000e-01]\n",
      "\n",
      "Done. Final eigenvalues: [0.070825 0.497811 1.006867 1.234324 1.487097 1.892893 2.40418  3.34443\n",
      " 3.99172  4.930622]\n"
     ]
    }
   ],
   "source": [
    "# multigrid_gnn_refine_fixed.py\n",
    "\"\"\"\n",
    "Complete working rewrite of your multigrid + GNN eigen-refinement pipeline.\n",
    "\n",
    "Assumptions:\n",
    " - `Mesh` class exists and Mesh('bunny.obj') loads .verts (n x 3) and .connectivity (triangles).\n",
    " - `robust_laplacian.point_cloud_laplacian(X)` returns (L, M) as scipy sparse matrices where L and M are compatible with eigsh.\n",
    " - scikit-learn, scipy, numpy, matplotlib, torch are installed.\n",
    "\n",
    "Key features:\n",
    " - All classes and functions defined in one file (no missing names).\n",
    " - Stable training: column normalization, small correction scale, normalized losses, grad clipping, configurable weights.\n",
    " - Auto-detect input feature dimension to avoid matmul mismatches.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports (must be available in your environment)\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utility helpers\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    \"\"\"Convert scipy sparse matrix to torch.sparse_coo_tensor (CPU or GPU depending on .to(device)).\"\"\"\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    \"\"\"Normalize numpy matrix columns to have unit L2 norm. Returns normalized U and norms.\"\"\"\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    \"\"\"Normalize torch tensor columns to have unit L2 norm. Returns normalized U and norms (torch).\"\"\"\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Simple per-node corrector (message-passing via neighbor mean + MLP)\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 64, 32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2  # because we will concat self + neighbor-mean in forward\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [n, in_dim]\n",
    "        edge_index: LongTensor shape [2, n_edges] with (row=target, col=source) semantics\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # both LongTensor\n",
    "        n = x.shape[0]\n",
    "        # aggregate neighbor features: mean aggregator\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device)\n",
    "        deg = deg.clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)  # shape [n, 2*in_dim]\n",
    "        return self.net(h)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid eigensolver with GNN corrector\n",
    "# ------------------------\n",
    "class MultigridEigensolver:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        # use eigsh with M as mass\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Core training routine\n",
    "    # ------------------------\n",
    "    def train_gnn(self, model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                  n_modes,\n",
    "                  epochs=200,\n",
    "                  lr=1e-3,\n",
    "                  corr_scale=1e-2,\n",
    "                  w_res=10.0,\n",
    "                  w_orth=1.0,\n",
    "                  w_proj=1e-3,\n",
    "                  grad_clip=1.0,\n",
    "                  weight_decay=1e-6,\n",
    "                  log_every=200):\n",
    "        \"\"\"\n",
    "        Train corrector model:\n",
    "          - x_feats: torch.FloatTensor [n_fine, in_dim] on device\n",
    "          - edge_index: torch.LongTensor [2, n_edges] on device\n",
    "          - U_init: numpy array [n_fine, n_modes] (will be normalized inside)\n",
    "          - L_fine, M_fine: scipy sparse matrices\n",
    "          - U_coarse: numpy array [n_coarse, n_modes]\n",
    "          - P: scipy sparse prolongation (n_fine x n_coarse)\n",
    "        Returns U_pred (numpy array [n_fine, n_modes]) - denormalized to original U_init scale.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Convert sparse matrices to torch sparse on device\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(device)\n",
    "        R_t = sp_to_torch_sparse(P.T).to(device)\n",
    "\n",
    "        # Normalize columns of U_init and U_coarse (keep original norms for rescaling)\n",
    "        U_init_normed, uinit_norms = normalize_columns_np(U_init)\n",
    "        U_coarse_normed, ucoarse_norms = normalize_columns_np(U_coarse)\n",
    "\n",
    "        U_init_t = torch.FloatTensor(U_init_normed).to(device)   # [n_fine, n_modes]\n",
    "        U_coarse_t = torch.FloatTensor(U_coarse_normed).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        n_fine = U_init_t.shape[0]\n",
    "        n_coarse = U_coarse_t.shape[0]\n",
    "        denom_res = float(max(1, n_fine * n_modes))\n",
    "        denom_proj = float(max(1, n_coarse * n_modes))\n",
    "        I = torch.eye(n_modes, device=device)\n",
    "\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            corr_raw = model(x_feats, edge_index)  # [n_fine, n_modes]\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_init_t + corr\n",
    "\n",
    "            # Rayleigh-related tensors\n",
    "            Lu = torch.sparse.mm(L_t, U_pred)\n",
    "            Mu = torch.sparse.mm(M_t, U_pred)\n",
    "            num = torch.sum(U_pred * Lu, dim=0)\n",
    "            den = torch.sum(U_pred * Mu, dim=0) + 1e-12\n",
    "            lambdas = num / den\n",
    "\n",
    "            # Residual loss (normalized)\n",
    "            res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "            L_res = torch.sum(res**2) / denom_res\n",
    "\n",
    "            # Orthonormality loss (M-weighted Gram)\n",
    "            MUt = torch.sparse.mm(M_t, U_pred)\n",
    "            Gram = U_pred.t() @ MUt\n",
    "            L_orth = torch.sum((Gram - I)**2) / (n_modes * n_modes)\n",
    "\n",
    "            # Projection loss\n",
    "            proj = torch.sparse.mm(R_t, U_pred)\n",
    "            L_proj = torch.sum((proj - U_coarse_t)**2) / denom_proj\n",
    "\n",
    "            loss = w_res * L_res + w_orth * L_orth + w_proj * L_proj\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (ep % log_every == 0) or (ep == epochs - 1):\n",
    "                with torch.no_grad():\n",
    "                    u_norm = float(U_pred.norm().cpu().item())\n",
    "                    corr_std = float(corr.std().cpu().item())\n",
    "                print(f\"    Epoch {ep:4d}: Loss={loss.item():.6f} (Res={L_res.item():.6f}, Orth={L_orth.item():.6f}, Proj={L_proj.item():.6f}) U_norm={u_norm:.4f} corr_std={corr_std:.6f}\")\n",
    "\n",
    "        # Denormalize: multiply columns by original column norms of U_init\n",
    "        U_pred_np = U_pred.detach().cpu().numpy() * uinit_norms.reshape(1, -1)\n",
    "        return U_pred_np\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L_fine, M_fine):\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(self.device)\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "\n",
    "    # ------------------------\n",
    "    # Refine one level (coarse -> fine)\n",
    "    # ------------------------\n",
    "    def refine_level(self, X_coarse, U_coarse, X_fine, n_modes,\n",
    "                     hidden_sizes=(128, 64, 32),\n",
    "                     dropout=0.0,\n",
    "                     k_neighbors=4,\n",
    "                     epochs=200,\n",
    "                     lr=1e-3,\n",
    "                     corr_scale=1e-2,\n",
    "                     w_res=10.0,\n",
    "                     w_orth=1.0,\n",
    "                     w_proj=1e-3,\n",
    "                     freeze_layers=0,\n",
    "                     checkpoint_name=None):\n",
    "        \"\"\"\n",
    "        Single-level refinement.\n",
    "        - Automatically sets input dim from features.\n",
    "        - Creates model if not existing; reuses and optionally freezes layers if existing.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        print(f\"  Computing Laplacian for {X_fine.shape[0]} points...\")\n",
    "        L_fine, M_fine = robust_laplacian.point_cloud_laplacian(X_fine)\n",
    "\n",
    "        print(\"  Building prolongation operator...\")\n",
    "        P = self.build_prolongation(X_coarse, X_fine, k=1)\n",
    "\n",
    "        print(\"  Building kNN graph...\")\n",
    "        edge_index = self.build_knn_graph(X_fine, k=k_neighbors).to(device)\n",
    "\n",
    "        # Build U_init on fine grid\n",
    "        U_init = P @ U_coarse  # shape [n_fine, n_modes]\n",
    "\n",
    "        # Build features: coords + U_init (we pass raw U_init; normalization happens inside train_gnn)\n",
    "        x_feats = torch.FloatTensor(np.hstack([X_fine, U_init])).to(device)\n",
    "\n",
    "        in_dim = x_feats.shape[1]\n",
    "        out_dim = n_modes\n",
    "\n",
    "        if self.model is None:\n",
    "            print(f\"  Creating new corrector model (in_dim={in_dim}, out_dim={out_dim})...\")\n",
    "            self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "        else:\n",
    "            # If model exists but input dimension changed, re-create model to match new in_dim\n",
    "            # (safer than trying to partially load weights with mismatched shapes)\n",
    "            existing_in_dim = None\n",
    "            # try to infer existing in_dim by checking first Linear in model.net if present\n",
    "            for m in self.model.net:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    existing_in_dim = m.in_features\n",
    "                    break\n",
    "            if existing_in_dim != in_dim:\n",
    "                print(f\"  Recreating model to match new input dim (was {existing_in_dim}, now {in_dim})...\")\n",
    "                # Optionally copy weights for layers that match by size\n",
    "                old_state = self.model.state_dict()\n",
    "                self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "                # attempt to copy subset of weights where shapes match\n",
    "                new_state = self.model.state_dict()\n",
    "                for k, v in old_state.items():\n",
    "                    if k in new_state and old_state[k].shape == new_state[k].shape:\n",
    "                        new_state[k] = old_state[k]\n",
    "                self.model.load_state_dict(new_state)\n",
    "\n",
    "        # Optionally freeze first few linear layers (count of Linear modules)\n",
    "        if freeze_layers > 0:\n",
    "            linear_count = 0\n",
    "            for module in self.model.net:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    linear_count += 1\n",
    "                    if linear_count <= freeze_layers:\n",
    "                        for p in module.parameters():\n",
    "                            p.requires_grad = False\n",
    "            print(f\"  Frozen first {freeze_layers} linear layers.\")\n",
    "\n",
    "        print(f\"  Training corrector: epochs={epochs}, lr={lr}, corr_scale={corr_scale}\")\n",
    "        U_pred = self.train_gnn(self.model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                                n_modes,\n",
    "                                epochs=epochs,\n",
    "                                lr=lr,\n",
    "                                corr_scale=corr_scale,\n",
    "                                w_res=w_res,\n",
    "                                w_orth=w_orth,\n",
    "                                w_proj=w_proj)\n",
    "\n",
    "        print(\"  Rayleigh-Ritz refinement...\")\n",
    "        lambda_refined, U_refined = self.refine_eigenvectors(U_pred, L_fine, M_fine)\n",
    "\n",
    "        if checkpoint_name is not None:\n",
    "            ckpt = {\"model_state\": self.model.state_dict(), \"lambda_refined\": lambda_refined}\n",
    "            torch.save(ckpt, os.path.join(self.checkpoint_dir, checkpoint_name))\n",
    "            print(f\"  Saved checkpoint: {os.path.join(self.checkpoint_dir, checkpoint_name)}\")\n",
    "\n",
    "        return lambda_refined, U_refined, L_fine, M_fine\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Visualization helper\n",
    "# ------------------------\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None, show=True):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2],\n",
    "                    triangles=mesh.connectivity, alpha=0.35)\n",
    "    if highlight_indices is not None:\n",
    "        hv = mesh.verts[highlight_indices]\n",
    "        ax.scatter(hv[:, 0], hv[:, 1], hv[:, 2], s=6, label=f\"{len(highlight_indices)} pts\")\n",
    "        ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=120, azim=-90)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main script\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hidden_sizes = (128, 64, 32)\n",
    "    dropout = 0.0\n",
    "\n",
    "    # schedule and hyperparams\n",
    "    epochs_schedule = {0: 0, 1: 1500, 2: 1000, 3: 800, 4: 800}\n",
    "    hierarchy = [32, 128, 512, 1024]  # final level will append full\n",
    "    k_neighbors = 4\n",
    "    lr_start = 1e-3\n",
    "    lr_min = 5e-4\n",
    "    corr_scale = 1e-2\n",
    "    w_res = 10.0\n",
    "    w_orth = 10.0\n",
    "    w_proj = 1e-3\n",
    "    freeze_schedule = {1: 0, 2: 1, 3: 1, 4: 2}\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridEigensolver.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    print(f\"Mesh loaded: {n_total} vertices\")\n",
    "\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    all_idx = np.arange(n_total)\n",
    "    rng.shuffle(all_idx)\n",
    "    indices_per_level = {}\n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        indices_per_level[i] = all_idx[:n_points].copy()\n",
    "        print(f\"  Level {i}: {n_points} points (nested)\")\n",
    "\n",
    "    solver = MultigridEigensolver()\n",
    "\n",
    "    # Level 0 coarse solve\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(\"\\nLEVEL 0: coarse solving...\")\n",
    "    lambda_cur, U_cur, L_cur, M_cur = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "    # iterative refinement\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level - 1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "        epochs = epochs_schedule.get(level, 1000)\n",
    "\n",
    "        print(f\"\\nLEVEL {level}: refine {Xc.shape[0]} -> {Xf.shape[0]}, epochs={epochs}\")\n",
    "        freeze_layers = freeze_schedule.get(level, 0)\n",
    "\n",
    "        total_levels = len(hierarchy)\n",
    "        decay = (level - 1) / max(1, total_levels - 1)\n",
    "        lr = lr_start * ((lr_min / lr_start) ** decay)\n",
    "\n",
    "        lambda_cur, U_cur, L_cur, M_cur = solver.refine_level(\n",
    "            Xc, U_cur, Xf, n_modes,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            k_neighbors=k_neighbors,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            corr_scale=corr_scale,\n",
    "            w_res=w_res,\n",
    "            w_orth=w_orth,\n",
    "            w_proj=w_proj,\n",
    "            freeze_layers=freeze_layers,\n",
    "            checkpoint_name=f\"level_{level}_ckpt.pt\"\n",
    "        )\n",
    "\n",
    "        print(\"GNN-refined eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "        # exact eigenvalues for verification\n",
    "        print(\"  computing exact eigenvalues for verification...\")\n",
    "        lambda_exact, _, _, _ = solver.solve_eigenvalue_problem(Xf, n_modes)\n",
    "        rel_err = np.abs(lambda_cur - lambda_exact) / (np.abs(lambda_exact) + 1e-12)\n",
    "        print(\"  Exact eigenvalues:\", np.round(lambda_exact, 6))\n",
    "        print(\"  Relative errors:  \", np.round(rel_err, 6))\n",
    "\n",
    "    print(\"\\nDone. Final eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d53b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab55f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b596c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c162b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3778b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3082a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37961e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b7ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397c3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Hierarchy: [128, 512, 1024, 2503]\n",
      "\n",
      "LEVEL 0: exact solve on 128 points...\n",
      "Coarse eigenvalues: [0.       0.464393 0.860068 1.196359 1.413617 1.861207 2.273839 2.759782\n",
      " 2.87012  3.25778 ]\n",
      "\n",
      "Training physics-informed multiresolution GNN...\n",
      "Epoch    0: Loss=0.554847\n",
      "Epoch  100: Loss=0.402844\n",
      "Epoch  200: Loss=0.218859\n",
      "Epoch  300: Loss=0.112217\n",
      "Epoch  400: Loss=0.056183\n",
      "Epoch  500: Loss=0.037726\n",
      "Epoch  600: Loss=0.028144\n",
      "Epoch  700: Loss=0.023266\n",
      "Epoch  800: Loss=0.020314\n",
      "Epoch  900: Loss=0.018311\n",
      "Epoch  999: Loss=0.016806\n",
      "Level 0 refined eigenvalues: [0.    0.473 0.877 1.211 1.43  1.878 2.299 2.789 2.903 3.366]\n",
      "Level 1 refined eigenvalues: [0.    0.457 0.927 1.269 1.435 1.89  2.057 2.912 3.185 3.799]\n",
      "Level 2 refined eigenvalues: [0.    0.461 0.951 1.179 1.278 1.448 1.948 2.99  3.298 3.702]\n",
      "Level 3 refined eigenvalues: [0.    0.505 1.009 1.378 1.39  1.585 2.087 3.245 3.536 3.838]\n"
     ]
    }
   ],
   "source": [
    "# multigrid_gnn_multires_physics.py\n",
    "\"\"\"\n",
    "Physics-informed Multigrid + GNN eigen-refinement\n",
    "- Exact solve only on coarsest mesh\n",
    "- Multiresolution GNN with residual + orthonormality + projection loss\n",
    "- Coarse-to-fine prolongation only\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "# ------------------------\n",
    "# Simple neighbor-mean corrector\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128,64,32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        n = x.shape[0]\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device).clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)\n",
    "        return self.net(h)\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid GNN solver\n",
    "# ------------------------\n",
    "class MultigridGNN:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Physics-informed GNN training\n",
    "    # ------------------------\n",
    "    def train_multiresolution(self, X_list, U_init_list, edge_index_list,\n",
    "                              epochs=1000, lr=1e-3, corr_scale=1e-2,\n",
    "                              w_res=10.0, w_orth=1.0, w_proj=1e-3,\n",
    "                              grad_clip=1.0, weight_decay=1e-6, log_every=100):\n",
    "        device = self.device\n",
    "        n_modes = U_init_list[0].shape[1]\n",
    "\n",
    "        # Build torch tensors and resolution indicators\n",
    "        x_feats_all, U_all, edge_index_all = [], [], []\n",
    "        node_offset = 0\n",
    "        max_nodes = max([X.shape[0] for X in X_list])\n",
    "        for X, U_init, edge_index in zip(X_list, U_init_list, edge_index_list):\n",
    "            res_feat = np.full((X.shape[0], 1), X.shape[0]/max_nodes)\n",
    "            x_feats_all.append(np.hstack([X, U_init, res_feat]))\n",
    "            U_all.append(U_init)\n",
    "            edge_index_all.append(edge_index + node_offset)\n",
    "            node_offset += X.shape[0]\n",
    "\n",
    "        x_feats_all = torch.FloatTensor(np.vstack(x_feats_all)).to(device)\n",
    "        U_all_tensor = torch.FloatTensor(np.vstack(U_all)).to(device)\n",
    "        edge_index_all = torch.cat(edge_index_all, dim=1).to(device)\n",
    "\n",
    "        in_dim = x_feats_all.shape[1]\n",
    "        if self.model is None:\n",
    "            self.model = SimpleCorrector(in_dim, n_modes).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "\n",
    "        # ------------------------\n",
    "        # Precompute Laplacians per level\n",
    "        L_list, M_list = [], []\n",
    "        node_offset = 0\n",
    "        for X in X_list:\n",
    "            L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "            L_list.append(sp_to_torch_sparse(L).to(device))\n",
    "            M_list.append(sp_to_torch_sparse(M).to(device))\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            corr_raw = self.model(x_feats_all, edge_index_all)\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_all_tensor + corr\n",
    "\n",
    "            # Physics-informed loss\n",
    "            loss = 0.0\n",
    "            node_offset = 0\n",
    "            for i, (L_t, M_t, U_init) in enumerate(zip(L_list, M_list, U_init_list)):\n",
    "                n_nodes = U_init.shape[0]\n",
    "                U_level = U_pred[node_offset:node_offset+n_nodes]\n",
    "\n",
    "                # Rayleigh residual\n",
    "                Lu = torch.sparse.mm(L_t, U_level)\n",
    "                Mu = torch.sparse.mm(M_t, U_level)\n",
    "                num = torch.sum(U_level * Lu, dim=0)\n",
    "                den = torch.sum(U_level * Mu, dim=0) + 1e-12\n",
    "                lambdas = num / den\n",
    "                res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "                L_res = torch.mean(res**2)\n",
    "\n",
    "                # Orthonormality\n",
    "                Gram = U_level.t() @ Mu\n",
    "                L_orth = torch.mean((Gram - torch.eye(n_modes, device=device))**2)\n",
    "\n",
    "                loss += w_res * L_res + w_orth * L_orth\n",
    "                node_offset += n_nodes\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.model.parameters()), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if ep % log_every == 0 or ep == epochs-1:\n",
    "                print(f\"Epoch {ep:4d}: Loss={loss.item():.6f}\")\n",
    "\n",
    "        return U_pred.detach().cpu().numpy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L, M):\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        L_t = sp_to_torch_sparse(L).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "\n",
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hierarchy = [128, 512, 1024]  # final level is full mesh\n",
    "    k_neighbors = 4\n",
    "    epochs = 1000\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridGNN.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    all_idx = np.arange(n_total)\n",
    "    rng.shuffle(all_idx)\n",
    "    indices_per_level = {i: all_idx[:n].copy() for i,n in enumerate(hierarchy)}\n",
    "\n",
    "    solver = MultigridGNN()\n",
    "\n",
    "    # ------------------------\n",
    "    # Level 0: exact coarse solve\n",
    "    # ------------------------\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(f\"\\nLEVEL 0: exact solve on {X0.shape[0]} points...\")\n",
    "    lambda0, U0, L0, M0 = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda0,6))\n",
    "\n",
    "    # ------------------------\n",
    "    # Coarse-to-fine prolongation\n",
    "    # ------------------------\n",
    "    U_prev = U0.copy()\n",
    "    X_list, U_init_list, edge_index_list = [X0], [U0], [solver.build_knn_graph(X0, k=k_neighbors)]\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level-1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "\n",
    "        P = solver.build_prolongation(Xc, Xf, k=1)\n",
    "        U_init = P @ U_prev\n",
    "        edge_index = solver.build_knn_graph(Xf, k=k_neighbors)\n",
    "\n",
    "        X_list.append(Xf)\n",
    "        U_init_list.append(U_init)\n",
    "        edge_index_list.append(edge_index)\n",
    "\n",
    "        U_prev = U_init.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Train physics-informed GNN\n",
    "    # ------------------------\n",
    "    print(\"\\nTraining physics-informed multiresolution GNN...\")\n",
    "    U_pred_all = solver.train_multiresolution(X_list, U_init_list, edge_index_list,\n",
    "                                              epochs=epochs)\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement per level\n",
    "    # ------------------------\n",
    "    node_offset = 0\n",
    "    for level, X in enumerate(X_list):\n",
    "        n_nodes = X.shape[0]\n",
    "        U_pred = U_pred_all[node_offset:node_offset+n_nodes]\n",
    "        node_offset += n_nodes\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        vals_refined, _ = solver.refine_eigenvectors(U_pred, L, M)\n",
    "        print(f\"Level {level} refined eigenvalues: {np.round(vals_refined,3)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f5f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exact eigenvalues: [0.000, 0.288, 0.722, 0.842, 1.039, 1.202, 1.762, 2.600, 2.923, 2.973]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe359d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097f40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5aa5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7267b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22470fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a30bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf0665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6db8cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Mesh loaded: 2503 vertices\n",
      "Hierarchy: [128, 512, 1024, 2503]\n",
      "  Level 0: 128 points (nested)\n",
      "  Level 1: 512 points (nested)\n",
      "  Level 2: 1024 points (nested)\n",
      "  Level 3: 2503 points (nested)\n",
      "\n",
      "LEVEL 0: coarse solving...\n",
      "Coarse eigenvalues: [0.       0.464393 0.860068 1.196359 1.413617 1.861207 2.273839 2.759782\n",
      " 2.87012  3.25778 ]\n",
      "\n",
      "LEVEL 1: refine 128 -> 512, epochs=1500\n",
      "  Computing Laplacian for 512 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Creating new corrector model (in_dim=13, out_dim=10)...\n",
      "  Training corrector: epochs=1500, lr=0.001, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.896934 (Res=0.001216, Orth=0.088476, Proj=0.016340) U_norm=3.1672 corr_std=0.000563\n",
      "    Epoch  200: Loss=0.175740 (Res=0.002795, Orth=0.014735, Proj=0.435962) U_norm=11.7478 corr_std=0.122789\n",
      "    Epoch  400: Loss=0.018089 (Res=0.001746, Orth=0.000009, Proj=0.545564) U_norm=13.0934 corr_std=0.141240\n",
      "    Epoch  600: Loss=0.010099 (Res=0.000952, Orth=0.000004, Proj=0.546351) U_norm=13.1368 corr_std=0.142120\n",
      "    Epoch  800: Loss=0.006977 (Res=0.000641, Orth=0.000002, Proj=0.546992) U_norm=13.1528 corr_std=0.142541\n",
      "    Epoch 1000: Loss=0.005461 (Res=0.000490, Orth=0.000002, Proj=0.547357) U_norm=13.1571 corr_std=0.142740\n",
      "    Epoch 1200: Loss=0.004611 (Res=0.000404, Orth=0.000002, Proj=0.547892) U_norm=13.1613 corr_std=0.142899\n",
      "    Epoch 1400: Loss=0.003977 (Res=0.000342, Orth=0.000001, Proj=0.547781) U_norm=13.1626 corr_std=0.142985\n",
      "    Epoch 1499: Loss=0.003734 (Res=0.000317, Orth=0.000001, Proj=0.547825) U_norm=13.1630 corr_std=0.143042\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_1_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.021753 0.430463 0.893302 1.257199 1.45683  1.856976 2.086186 2.796744\n",
      " 3.133543 3.597457]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [0.       0.390175 0.85084  1.164091 1.297341 1.751831 1.869489 2.649577\n",
      " 2.911107 3.221319]\n",
      "  Relative errors:   [1.9426757e+10 1.0325500e-01 4.9905000e-02 7.9984000e-02 1.2293500e-01\n",
      " 6.0020000e-02 1.1591200e-01 5.5544000e-02 7.6410000e-02 1.1676500e-01]\n",
      "\n",
      "LEVEL 2: refine 512 -> 1024, epochs=1000\n",
      "  Computing Laplacian for 1024 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=1000, lr=0.0007937005259840999, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.141627 (Res=0.000499, Orth=0.013650, Proj=0.131590) U_norm=16.1748 corr_std=0.157456\n",
      "    Epoch  200: Loss=0.005151 (Res=0.000496, Orth=0.000001, Proj=0.179646) U_norm=18.7428 corr_std=0.186122\n",
      "    Epoch  400: Loss=0.004262 (Res=0.000408, Orth=0.000001, Proj=0.180211) U_norm=18.7643 corr_std=0.186356\n",
      "    Epoch  600: Loss=0.003805 (Res=0.000362, Orth=0.000001, Proj=0.180478) U_norm=18.7766 corr_std=0.186457\n",
      "    Epoch  800: Loss=0.003507 (Res=0.000332, Orth=0.000001, Proj=0.180537) U_norm=18.7811 corr_std=0.186488\n",
      "    Epoch  999: Loss=0.003268 (Res=0.000308, Orth=0.000001, Proj=0.180540) U_norm=18.7829 corr_std=0.186489\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_2_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.037715 0.473994 0.938441 1.144395 1.366978 1.572206 2.033276 2.923415\n",
      " 3.354062 3.63487 ]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [0.       0.333353 0.765464 0.8359   1.064607 1.2346   1.738815 2.626301\n",
      " 2.899525 3.119024]\n",
      "  Relative errors:   [3.22016442e+10 4.21900000e-01 2.25977000e-01 3.69058000e-01\n",
      " 2.84022000e-01 2.73454000e-01 1.69346000e-01 1.13130000e-01\n",
      " 1.56763000e-01 1.65387000e-01]\n",
      "\n",
      "LEVEL 3: refine 1024 -> 2503, epochs=800\n",
      "  Computing Laplacian for 2503 points...\n",
      "  Building prolongation operator...\n",
      "  Building kNN graph...\n",
      "  Recreating model to match new input dim (was 26, now 13)...\n",
      "  Frozen first 1 linear layers.\n",
      "  Training corrector: epochs=800, lr=0.0006299605249474366, corr_scale=0.01\n",
      "    Epoch    0: Loss=0.075972 (Res=0.000183, Orth=0.007391, Proj=0.224794) U_norm=27.0362 corr_std=0.171489\n",
      "    Epoch  200: Loss=0.001934 (Res=0.000170, Orth=0.000000, Proj=0.236359) U_norm=27.7788 corr_std=0.176247\n",
      "    Epoch  400: Loss=0.001717 (Res=0.000148, Orth=0.000000, Proj=0.236451) U_norm=27.7824 corr_std=0.176261\n",
      "    Epoch  600: Loss=0.001600 (Res=0.000136, Orth=0.000000, Proj=0.236509) U_norm=27.7858 corr_std=0.176262\n",
      "    Epoch  799: Loss=0.001507 (Res=0.000127, Orth=0.000000, Proj=0.236617) U_norm=27.7925 corr_std=0.176277\n",
      "  Rayleigh-Ritz refinement...\n",
      "  Saved checkpoint: ./checkpoints/level_3_ckpt.pt\n",
      "GNN-refined eigenvalues: [0.049106 0.439215 0.927102 1.097043 1.378341 1.638068 2.138497 2.963156\n",
      " 3.414624 3.886225]\n",
      "  computing exact eigenvalues for verification...\n",
      "  Exact eigenvalues: [-0.        0.287932  0.721703  0.841517  1.03936   1.202406  1.762114\n",
      "  2.59946   2.923435  2.972727]\n",
      "  Relative errors:   [4.43956206e+10 5.25413000e-01 2.84602000e-01 3.03650000e-01\n",
      " 3.26144000e-01 3.62324000e-01 2.13597000e-01 1.39912000e-01\n",
      " 1.68018000e-01 3.07293000e-01]\n",
      "\n",
      "Done. Final eigenvalues: [0.049106 0.439215 0.927102 1.097043 1.378341 1.638068 2.138497 2.963156\n",
      " 3.414624 3.886225]\n"
     ]
    }
   ],
   "source": [
    "# multigrid_gnn_refine_fixed.py\n",
    "\"\"\"\n",
    "Complete working rewrite of your multigrid + GNN eigen-refinement pipeline.\n",
    "\n",
    "Assumptions:\n",
    " - `Mesh` class exists and Mesh('bunny.obj') loads .verts (n x 3) and .connectivity (triangles).\n",
    " - `robust_laplacian.point_cloud_laplacian(X)` returns (L, M) as scipy sparse matrices where L and M are compatible with eigsh.\n",
    " - scikit-learn, scipy, numpy, matplotlib, torch are installed.\n",
    "\n",
    "Key features:\n",
    " - All classes and functions defined in one file (no missing names).\n",
    " - Stable training: column normalization, small correction scale, normalized losses, grad clipping, configurable weights.\n",
    " - Auto-detect input feature dimension to avoid matmul mismatches.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports (must be available in your environment)\n",
    "from Mesh import Mesh\n",
    "import robust_laplacian\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utility helpers\n",
    "# ------------------------\n",
    "def sp_to_torch_sparse(A):\n",
    "    \"\"\"Convert scipy sparse matrix to torch.sparse_coo_tensor (CPU or GPU depending on .to(device)).\"\"\"\n",
    "    A = A.tocoo()\n",
    "    indices = np.vstack((A.row, A.col)).astype(np.int64)\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(A.data)\n",
    "    return torch.sparse_coo_tensor(i, v, A.shape).coalesce()\n",
    "\n",
    "\n",
    "def normalize_columns_np(U, eps=1e-12):\n",
    "    \"\"\"Normalize numpy matrix columns to have unit L2 norm. Returns normalized U and norms.\"\"\"\n",
    "    norms = np.linalg.norm(U, axis=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "def normalize_columns_torch(U, eps=1e-12):\n",
    "    \"\"\"Normalize torch tensor columns to have unit L2 norm. Returns normalized U and norms (torch).\"\"\"\n",
    "    norms = torch.norm(U, dim=0) + eps\n",
    "    return U / norms, norms\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Simple per-node corrector (message-passing via neighbor mean + MLP)\n",
    "# ------------------------\n",
    "class SimpleCorrector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 64, 32), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim * 2  # because we will concat self + neighbor-mean in forward\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [n, in_dim]\n",
    "        edge_index: LongTensor shape [2, n_edges] with (row=target, col=source) semantics\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # both LongTensor\n",
    "        n = x.shape[0]\n",
    "        # aggregate neighbor features: mean aggregator\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg = torch.bincount(row, minlength=n).unsqueeze(1).to(x.dtype).to(x.device)\n",
    "        deg = deg.clamp(min=1.0)\n",
    "        agg = agg / deg\n",
    "        h = torch.cat([x, agg], dim=1)  # shape [n, 2*in_dim]\n",
    "        return self.net(h)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Multigrid eigensolver with GNN corrector\n",
    "# ------------------------\n",
    "class MultigridEigensolver:\n",
    "    def __init__(self, device=None, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mesh(mesh):\n",
    "        centroid = mesh.verts.mean(0)\n",
    "        std_max = mesh.verts.std(0).max() + 1e-12\n",
    "        verts_normalized = (mesh.verts - centroid) / std_max\n",
    "        return Mesh(verts=verts_normalized, connectivity=mesh.connectivity)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prolongation(X_coarse, X_fine, k=1):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_coarse)\n",
    "        distances, indices = nbrs.kneighbors(X_fine)\n",
    "        n_fine, n_coarse = X_fine.shape[0], X_coarse.shape[0]\n",
    "        rows, cols, vals = [], [], []\n",
    "        for i in range(n_fine):\n",
    "            weights = 1.0 / (distances[i] + 1e-12)\n",
    "            weights /= weights.sum()\n",
    "            for j, idx in enumerate(indices[i]):\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                vals.append(weights[j])\n",
    "        return coo_matrix((vals, (rows, cols)), shape=(n_fine, n_coarse))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_knn_graph(X, k=4):\n",
    "        n_points = X.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
    "        _, neighbors = nbrs.kneighbors(X)\n",
    "        rows, cols = [], []\n",
    "        for i in range(n_points):\n",
    "            for j in neighbors[i][1:]:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "        return torch.LongTensor([rows, cols]).to(torch.long)\n",
    "\n",
    "    def solve_eigenvalue_problem(self, X, n_modes):\n",
    "        L, M = robust_laplacian.point_cloud_laplacian(X)\n",
    "        # use eigsh with M as mass\n",
    "        vals, vecs = eigsh(L, k=n_modes, M=M, which='SM')\n",
    "        return vals, np.array(vecs), L, M\n",
    "\n",
    "    # ------------------------\n",
    "    # Core training routine\n",
    "    # ------------------------\n",
    "    def train_gnn(self, model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                  n_modes,\n",
    "                  epochs=200,\n",
    "                  lr=1e-3,\n",
    "                  corr_scale=1e-2,\n",
    "                  w_res=10.0,\n",
    "                  w_orth=1.0,\n",
    "                  w_proj=1e-3,\n",
    "                  grad_clip=1.0,\n",
    "                  weight_decay=1e-6,\n",
    "                  log_every=200):\n",
    "        \"\"\"\n",
    "        Train corrector model:\n",
    "          - x_feats: torch.FloatTensor [n_fine, in_dim] on device\n",
    "          - edge_index: torch.LongTensor [2, n_edges] on device\n",
    "          - U_init: numpy array [n_fine, n_modes] (will be normalized inside)\n",
    "          - L_fine, M_fine: scipy sparse matrices\n",
    "          - U_coarse: numpy array [n_coarse, n_modes]\n",
    "          - P: scipy sparse prolongation (n_fine x n_coarse)\n",
    "        Returns U_pred (numpy array [n_fine, n_modes]) - denormalized to original U_init scale.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Convert sparse matrices to torch sparse on device\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(device)\n",
    "        R_t = sp_to_torch_sparse(P.T).to(device)\n",
    "\n",
    "        # Normalize columns of U_init and U_coarse (keep original norms for rescaling)\n",
    "        U_init_normed, uinit_norms = normalize_columns_np(U_init)\n",
    "        U_coarse_normed, ucoarse_norms = normalize_columns_np(U_coarse)\n",
    "\n",
    "        U_init_t = torch.FloatTensor(U_init_normed).to(device)   # [n_fine, n_modes]\n",
    "        U_coarse_t = torch.FloatTensor(U_coarse_normed).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        n_fine = U_init_t.shape[0]\n",
    "        n_coarse = U_coarse_t.shape[0]\n",
    "        denom_res = float(max(1, n_fine * n_modes))\n",
    "        denom_proj = float(max(1, n_coarse * n_modes))\n",
    "        I = torch.eye(n_modes, device=device)\n",
    "\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            corr_raw = model(x_feats, edge_index)  # [n_fine, n_modes]\n",
    "            corr = corr_scale * corr_raw\n",
    "            U_pred = U_init_t + corr\n",
    "\n",
    "            # Rayleigh-related tensors\n",
    "            Lu = torch.sparse.mm(L_t, U_pred)\n",
    "            Mu = torch.sparse.mm(M_t, U_pred)\n",
    "            num = torch.sum(U_pred * Lu, dim=0)\n",
    "            den = torch.sum(U_pred * Mu, dim=0) + 1e-12\n",
    "            lambdas = num / den\n",
    "\n",
    "            # Residual loss (normalized)\n",
    "            res = Lu - Mu * lambdas.unsqueeze(0)\n",
    "            L_res = torch.sum(res**2) / denom_res\n",
    "\n",
    "            # Orthonormality loss (M-weighted Gram)\n",
    "            MUt = torch.sparse.mm(M_t, U_pred)\n",
    "            Gram = U_pred.t() @ MUt\n",
    "            L_orth = torch.sum((Gram - I)**2) / (n_modes * n_modes)\n",
    "\n",
    "            # Projection loss\n",
    "            proj = torch.sparse.mm(R_t, U_pred)\n",
    "            L_proj = torch.sum((proj - U_coarse_t)**2) / denom_proj\n",
    "\n",
    "            loss = w_res * L_res + w_orth * L_orth + w_proj * L_proj\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (ep % log_every == 0) or (ep == epochs - 1):\n",
    "                with torch.no_grad():\n",
    "                    u_norm = float(U_pred.norm().cpu().item())\n",
    "                    corr_std = float(corr.std().cpu().item())\n",
    "                print(f\"    Epoch {ep:4d}: Loss={loss.item():.6f} (Res={L_res.item():.6f}, Orth={L_orth.item():.6f}, Proj={L_proj.item():.6f}) U_norm={u_norm:.4f} corr_std={corr_std:.6f}\")\n",
    "\n",
    "        # Denormalize: multiply columns by original column norms of U_init\n",
    "        U_pred_np = U_pred.detach().cpu().numpy() * uinit_norms.reshape(1, -1)\n",
    "        return U_pred_np\n",
    "\n",
    "    # ------------------------\n",
    "    # Rayleigh-Ritz refinement\n",
    "    # ------------------------\n",
    "    def refine_eigenvectors(self, U_pred, L_fine, M_fine):\n",
    "        L_t = sp_to_torch_sparse(L_fine).to(self.device)\n",
    "        M_t = sp_to_torch_sparse(M_fine).to(self.device)\n",
    "        U = torch.FloatTensor(U_pred).to(self.device)\n",
    "        A = (U.t() @ torch.sparse.mm(L_t, U)).cpu().numpy()\n",
    "        B = (U.t() @ torch.sparse.mm(M_t, U)).cpu().numpy()\n",
    "        vals, C = eigh(A, B)\n",
    "        U_refined = U.cpu().numpy() @ C\n",
    "        return vals, U_refined\n",
    "\n",
    "    # ------------------------\n",
    "    # Refine one level (coarse -> fine)\n",
    "    # ------------------------\n",
    "    def refine_level(self, X_coarse, U_coarse, X_fine, n_modes,\n",
    "                     hidden_sizes=(128, 64, 32),\n",
    "                     dropout=0.0,\n",
    "                     k_neighbors=4,\n",
    "                     epochs=200,\n",
    "                     lr=1e-3,\n",
    "                     corr_scale=1e-2,\n",
    "                     w_res=10.0,\n",
    "                     w_orth=1.0,\n",
    "                     w_proj=1e-3,\n",
    "                     freeze_layers=0,\n",
    "                     checkpoint_name=None):\n",
    "        \"\"\"\n",
    "        Single-level refinement.\n",
    "        - Automatically sets input dim from features.\n",
    "        - Creates model if not existing; reuses and optionally freezes layers if existing.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        print(f\"  Computing Laplacian for {X_fine.shape[0]} points...\")\n",
    "        L_fine, M_fine = robust_laplacian.point_cloud_laplacian(X_fine)\n",
    "\n",
    "        print(\"  Building prolongation operator...\")\n",
    "        P = self.build_prolongation(X_coarse, X_fine, k=1)\n",
    "\n",
    "        print(\"  Building kNN graph...\")\n",
    "        edge_index = self.build_knn_graph(X_fine, k=k_neighbors).to(device)\n",
    "\n",
    "        # Build U_init on fine grid\n",
    "        U_init = P @ U_coarse  # shape [n_fine, n_modes]\n",
    "\n",
    "        # Build features: coords + U_init (we pass raw U_init; normalization happens inside train_gnn)\n",
    "        x_feats = torch.FloatTensor(np.hstack([X_fine, U_init])).to(device)\n",
    "\n",
    "        in_dim = x_feats.shape[1]\n",
    "        out_dim = n_modes\n",
    "\n",
    "        if self.model is None:\n",
    "            print(f\"  Creating new corrector model (in_dim={in_dim}, out_dim={out_dim})...\")\n",
    "            self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "        else:\n",
    "            # If model exists but input dimension changed, re-create model to match new in_dim\n",
    "            # (safer than trying to partially load weights with mismatched shapes)\n",
    "            existing_in_dim = None\n",
    "            # try to infer existing in_dim by checking first Linear in model.net if present\n",
    "            for m in self.model.net:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    existing_in_dim = m.in_features\n",
    "                    break\n",
    "            if existing_in_dim != in_dim:\n",
    "                print(f\"  Recreating model to match new input dim (was {existing_in_dim}, now {in_dim})...\")\n",
    "                # Optionally copy weights for layers that match by size\n",
    "                old_state = self.model.state_dict()\n",
    "                self.model = SimpleCorrector(in_dim, out_dim, hidden_sizes=hidden_sizes, dropout=dropout).to(device)\n",
    "                # attempt to copy subset of weights where shapes match\n",
    "                new_state = self.model.state_dict()\n",
    "                for k, v in old_state.items():\n",
    "                    if k in new_state and old_state[k].shape == new_state[k].shape:\n",
    "                        new_state[k] = old_state[k]\n",
    "                self.model.load_state_dict(new_state)\n",
    "\n",
    "        # Optionally freeze first few linear layers (count of Linear modules)\n",
    "        if freeze_layers > 0:\n",
    "            linear_count = 0\n",
    "            for module in self.model.net:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    linear_count += 1\n",
    "                    if linear_count <= freeze_layers:\n",
    "                        for p in module.parameters():\n",
    "                            p.requires_grad = False\n",
    "            print(f\"  Frozen first {freeze_layers} linear layers.\")\n",
    "\n",
    "        print(f\"  Training corrector: epochs={epochs}, lr={lr}, corr_scale={corr_scale}\")\n",
    "        U_pred = self.train_gnn(self.model, x_feats, edge_index, U_init, L_fine, M_fine, U_coarse, P,\n",
    "                                n_modes,\n",
    "                                epochs=epochs,\n",
    "                                lr=lr,\n",
    "                                corr_scale=corr_scale,\n",
    "                                w_res=w_res,\n",
    "                                w_orth=w_orth,\n",
    "                                w_proj=w_proj)\n",
    "\n",
    "        print(\"  Rayleigh-Ritz refinement...\")\n",
    "        lambda_refined, U_refined = self.refine_eigenvectors(U_pred, L_fine, M_fine)\n",
    "\n",
    "        if checkpoint_name is not None:\n",
    "            ckpt = {\"model_state\": self.model.state_dict(), \"lambda_refined\": lambda_refined}\n",
    "            torch.save(ckpt, os.path.join(self.checkpoint_dir, checkpoint_name))\n",
    "            print(f\"  Saved checkpoint: {os.path.join(self.checkpoint_dir, checkpoint_name)}\")\n",
    "\n",
    "        return lambda_refined, U_refined, L_fine, M_fine\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Visualization helper\n",
    "# ------------------------\n",
    "def visualize_mesh(mesh, title='Mesh Visualization', highlight_indices=None, show=True):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_trisurf(mesh.verts[:, 0], mesh.verts[:, 1], mesh.verts[:, 2],\n",
    "                    triangles=mesh.connectivity, alpha=0.35)\n",
    "    if highlight_indices is not None:\n",
    "        hv = mesh.verts[highlight_indices]\n",
    "        ax.scatter(hv[:, 0], hv[:, 1], hv[:, 2], s=6, label=f\"{len(highlight_indices)} pts\")\n",
    "        ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(elev=120, azim=-90)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main script\n",
    "# ------------------------\n",
    "def main():\n",
    "    mesh_path = \"bunny.obj\"\n",
    "    n_modes = 10\n",
    "    hidden_sizes = (128, 128, 128)\n",
    "    dropout = 0.0\n",
    "\n",
    "    # schedule and hyperparams\n",
    "    epochs_schedule = {0: 0, 1: 1500, 2: 1000, 3: 800, 4: 800}\n",
    "    hierarchy = [128, 512, 1024]  # final level will append full\n",
    "    k_neighbors = 4\n",
    "    lr_start = 1e-3\n",
    "    lr_min = 5e-4\n",
    "    corr_scale = 1e-2\n",
    "    w_res = 10.0\n",
    "    w_orth = 10.0\n",
    "    w_proj = 1e-3\n",
    "    freeze_schedule = {1: 0, 2: 1, 3: 1, 4: 2}\n",
    "\n",
    "    print(\"Loading mesh...\")\n",
    "    mesh = Mesh(mesh_path)\n",
    "    mesh = MultigridEigensolver.normalize_mesh(mesh)\n",
    "    X_full = mesh.verts\n",
    "    n_total = X_full.shape[0]\n",
    "    print(f\"Mesh loaded: {n_total} vertices\")\n",
    "\n",
    "    hierarchy = [n for n in hierarchy if n <= n_total]\n",
    "    if hierarchy[-1] != n_total:\n",
    "        hierarchy.append(n_total)\n",
    "    print(\"Hierarchy:\", hierarchy)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    all_idx = np.arange(n_total)\n",
    "    rng.shuffle(all_idx)\n",
    "    indices_per_level = {}\n",
    "    for i, n_points in enumerate(hierarchy):\n",
    "        indices_per_level[i] = all_idx[:n_points].copy()\n",
    "        print(f\"  Level {i}: {n_points} points (nested)\")\n",
    "\n",
    "    solver = MultigridEigensolver()\n",
    "\n",
    "    # Level 0 coarse solve\n",
    "    idx0 = indices_per_level[0]\n",
    "    X0 = X_full[idx0]\n",
    "    print(\"\\nLEVEL 0: coarse solving...\")\n",
    "    lambda_cur, U_cur, L_cur, M_cur = solver.solve_eigenvalue_problem(X0, n_modes)\n",
    "    print(\"Coarse eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "    # iterative refinement\n",
    "    for level in range(1, len(hierarchy)):\n",
    "        idx_coarse = indices_per_level[level - 1]\n",
    "        idx_fine = indices_per_level[level]\n",
    "        Xc = X_full[idx_coarse]\n",
    "        Xf = X_full[idx_fine]\n",
    "        epochs = epochs_schedule.get(level, 1000)\n",
    "\n",
    "        print(f\"\\nLEVEL {level}: refine {Xc.shape[0]} -> {Xf.shape[0]}, epochs={epochs}\")\n",
    "        freeze_layers = freeze_schedule.get(level, 0)\n",
    "\n",
    "        total_levels = len(hierarchy)\n",
    "        decay = (level - 1) / max(1, total_levels - 1)\n",
    "        lr = lr_start * ((lr_min / lr_start) ** decay)\n",
    "\n",
    "        lambda_cur, U_cur, L_cur, M_cur = solver.refine_level(\n",
    "            Xc, U_cur, Xf, n_modes,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            k_neighbors=k_neighbors,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            corr_scale=corr_scale,\n",
    "            w_res=w_res,\n",
    "            w_orth=w_orth,\n",
    "            w_proj=w_proj,\n",
    "            freeze_layers=freeze_layers,\n",
    "            checkpoint_name=f\"level_{level}_ckpt.pt\"\n",
    "        )\n",
    "\n",
    "        print(\"GNN-refined eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "        # exact eigenvalues for verification\n",
    "        print(\"  computing exact eigenvalues for verification...\")\n",
    "        lambda_exact, _, _, _ = solver.solve_eigenvalue_problem(Xf, n_modes)\n",
    "        rel_err = np.abs(lambda_cur - lambda_exact) / (np.abs(lambda_exact) + 1e-12)\n",
    "        print(\"  Exact eigenvalues:\", np.round(lambda_exact, 6))\n",
    "        print(\"  Relative errors:  \", np.round(rel_err, 6))\n",
    "\n",
    "    print(\"\\nDone. Final eigenvalues:\", np.round(lambda_cur, 6))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acccf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
