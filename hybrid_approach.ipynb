{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONFIG = {\n",
    "    'k': 50,\n",
    "    'max_epochs': 150_000,\n",
    "    'stage1_epochs': 50_000,\n",
    "    'lr_stage1': 0.01,\n",
    "    'lr_stage2': 0.001,\n",
    "    'lr_min': 0.0001,\n",
    "    'print_every': 2000,\n",
    "    'checkpoint_every': 5000,\n",
    "    'grad_clip': 1.0,\n",
    "    'early_stopping_patience': 30_000,\n",
    "    'early_stopping_threshold': 0.999,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading mesh...\")\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "# Normalize vertices\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "verts_new = (m.verts - centroid) / std_max\n",
    "m = Mesh(verts=verts_new, connectivity=m.connectivity)\n",
    "\n",
    "print('Computing Laplacian...')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "print('Computing reference eigenvalues...')\n",
    "eigvals, eigvecs = linalg.eigh(K, M)\n",
    "\n",
    "# Convert to torch\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "\n",
    "k = CONFIG['k']\n",
    "N = X.shape[0]\n",
    "\n",
    "# ============================================================================\n",
    "# MATRIX CONDITIONING (Your careful approach)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Matrix Diagnostics ===\")\n",
    "print(f\"N = {N}, k = {k}, ratio = {N/k:.1f}\")\n",
    "print(f\"Condition number of K: {torch.linalg.cond(K).item():.2e}\")\n",
    "print(f\"Condition number of M: {torch.linalg.cond(M).item():.2e}\")\n",
    "print(f\"Target eigenvalue range: [{eigvals[0]:.6f}, {eigvals[k-1]:.6f}]\")\n",
    "\n",
    "if eigvals[k-1] / (eigvals[1] + 1e-10) > 100:\n",
    "    print(\"⚠ Large eigenvalue spread detected\")\n",
    "\n",
    "# Regularization\n",
    "epsilon = 1e-4\n",
    "K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "print(f\"\\nRegularization: ε={epsilon}\")\n",
    "print(f\"Condition number after reg: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "\n",
    "# Separate normalization (your approach - more careful)\n",
    "K_scale = torch.norm(K_reg, p='fro')\n",
    "M_scale = torch.norm(M, p='fro')\n",
    "\n",
    "K = K_reg / K_scale\n",
    "M = M / M_scale\n",
    "\n",
    "print(f\"\\nNormalization:\")\n",
    "print(f\"  K_scale = {K_scale.item():.2e}\")\n",
    "print(f\"  M_scale = {M_scale.item():.2e}\")\n",
    "print(f\"  ||K||_F = {torch.norm(K, p='fro').item():.4f}\")\n",
    "print(f\"  ||M||_F = {torch.norm(M, p='fro').item():.4f}\")\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"\\nSanity checks:\")\n",
    "print(f\"  K symmetric: {torch.allclose(K, K.T, atol=1e-6)}\")\n",
    "print(f\"  M positive definite: {torch.all(torch.linalg.eigvalsh(M) > 0)}\")\n",
    "print(f\"  No NaN/Inf: {not (torch.isnan(K).any() or torch.isinf(K).any())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL DEFINITION (My improved architecture)\n",
    "# ============================================================================\n",
    "\n",
    "class HybridMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines:\n",
    "    - Fourier features for high-frequency modes\n",
    "    - LayerNorm for training stability\n",
    "    - Residual connections for gradient flow\n",
    "    - Wider architecture for capacity\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=50, hidden=[256, 256, 128, 128],\n",
    "                 use_fourier=True, n_fourier_features=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_fourier = use_fourier\n",
    "        self.n_fourier = n_fourier_features\n",
    "        \n",
    "        # Learnable Fourier frequencies\n",
    "        if use_fourier:\n",
    "            self.freq_scale = nn.Parameter(torch.ones(in_dim) * 10.0)\n",
    "            input_dim = in_dim * (1 + 2 * n_fourier_features)\n",
    "        else:\n",
    "            input_dim = in_dim\n",
    "        \n",
    "        # Network layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        last_dim = input_dim\n",
    "        for h in hidden:\n",
    "            self.layers.append(nn.Linear(last_dim, h))\n",
    "            self.norms.append(nn.LayerNorm(h))\n",
    "            last_dim = h\n",
    "        \n",
    "        self.output = nn.Linear(last_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Fourier feature mapping\n",
    "        if self.use_fourier:\n",
    "            features = [x]\n",
    "            for i in range(self.n_fourier):\n",
    "                freq = self.freq_scale * (2 ** i)\n",
    "                features.append(torch.sin(freq * x))\n",
    "                features.append(torch.cos(freq * x))\n",
    "            h = torch.cat(features, dim=-1)\n",
    "        else:\n",
    "            h = x\n",
    "        \n",
    "        # Forward with residuals and normalization\n",
    "        for i, (layer, norm) in enumerate(zip(self.layers, self.norms)):\n",
    "            h_new = layer(h)\n",
    "            h_new = norm(h_new)\n",
    "            h_new = torch.nn.functional.silu(h_new)\n",
    "            \n",
    "            # Residual connection (scaled)\n",
    "            if i > 0 and h.shape[-1] == h_new.shape[-1]:\n",
    "                h = h_new + 0.1 * h\n",
    "            else:\n",
    "                h = h_new\n",
    "        \n",
    "        return self.output(h)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== Initializing Model ===\")\n",
    "model = HybridMLP(in_dim=3, out_dim=k, use_fourier=True).double().to(device)\n",
    "\n",
    "# Careful initialization\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if 'output' in name:\n",
    "            nn.init.normal_(p.data, std=1e-4)\n",
    "        elif 'freq_scale' not in name and p.ndim >= 2:\n",
    "            nn.init.xavier_uniform_(p.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.zeros_(p.data)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Architecture: {[l.out_features for l in model.layers]} -> {k}\")\n",
    "print(f\"Fourier features: Enabled ({model.n_fourier} frequencies)\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS COMPUTATION (Hybrid approach)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(U, U_orth, K, M, rayleigh_matrix, B, S, eigvals_ref, epoch, config):\n",
    "    \"\"\"\n",
    "    Hybrid loss combining:\n",
    "    - Your SVD stability monitoring\n",
    "    - My zero eigenvalue constraint\n",
    "    - Adaptive weighting for two-stage training\n",
    "    \"\"\"\n",
    "    k = U.shape[1]\n",
    "    identity_k = torch.eye(k, device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Extract eigenvalues\n",
    "    eigenvalues_approx = torch.diag(rayleigh_matrix)\n",
    "    sorted_eigs, sort_idx = torch.sort(eigenvalues_approx)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. ZERO EIGENVALUE CONSTRAINT (Critical for first mode)\n",
    "    # ========================================================================\n",
    "    zero_eig_loss = sorted_eigs[0] ** 2\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. TRACE LOSS (Minimize sum, excluding first eigenvalue)\n",
    "    # ========================================================================\n",
    "    trace_loss = torch.sum(sorted_eigs[1:]) / (k - 1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. DIVERSITY LOSS (Adaptive gap)\n",
    "    # ========================================================================\n",
    "    gaps = sorted_eigs[1:] - sorted_eigs[:-1]\n",
    "    current_spread = sorted_eigs[-1] - sorted_eigs[1] + 1e-8\n",
    "    min_gap = current_spread * 0.005 / k  # 0.5% of spread\n",
    "    diversity_loss = torch.sum(torch.relu(min_gap - gaps)) / (k - 1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. OFF-DIAGONAL PENALTY (Enforce diagonalization)\n",
    "    # ========================================================================\n",
    "    off_diag_mask = 1 - identity_k\n",
    "    off_diag_loss = torch.sum((rayleigh_matrix * off_diag_mask) ** 2) / (k * (k-1))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. ORTHOGONALITY RESIDUAL (Should be small due to SVD)\n",
    "    # ========================================================================\n",
    "    B_orth = U_orth.T @ (M @ U_orth)\n",
    "    orth_loss = torch.norm(B_orth - identity_k, p='fro') ** 2\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. ORDERING LOSS (Maintain sorted eigenvalues)\n",
    "    # ========================================================================\n",
    "    ordering_loss = torch.sum(torch.relu(sorted_eigs[:-1] - sorted_eigs[1:])) / k\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. STABILITY LOSS (Your SVD monitoring)\n",
    "    # ========================================================================\n",
    "    S_ratio = S.max() / (S.min() + 1e-10)\n",
    "    stability_loss = torch.relu(S_ratio - 1e3) / 1e3\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 8. SMOOTHNESS REGULARIZATION (Stage 2 only)\n",
    "    # ========================================================================\n",
    "    if epoch > config['stage1_epochs']:\n",
    "        U_sorted = U_orth[:, sort_idx]\n",
    "        smoothness_loss = torch.mean(torch.sum((U_sorted[:, 1:] - U_sorted[:, :-1]) ** 2, dim=0))\n",
    "    else:\n",
    "        smoothness_loss = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ADAPTIVE WEIGHTING (Two-stage)\n",
    "    # ========================================================================\n",
    "    if epoch <= config['stage1_epochs']:\n",
    "        # Stage 1: Establish orthogonality, find low modes, force λ₁=0\n",
    "        w_zero = 100.0\n",
    "        w_trace = 5.0\n",
    "        w_div = 2.0\n",
    "        w_offdiag = 10.0\n",
    "        w_orth = 5.0\n",
    "        w_order = 0.5\n",
    "        w_stability = 0.1\n",
    "        w_smooth = 0.0\n",
    "    else:\n",
    "        # Stage 2: Fine-tune all eigenvalues, maintain constraints\n",
    "        w_zero = 50.0\n",
    "        w_trace = 2.0\n",
    "        w_div = 1.0\n",
    "        w_offdiag = 15.0\n",
    "        w_orth = 2.0\n",
    "        w_order = 0.2\n",
    "        w_stability = 0.1\n",
    "        w_smooth = 0.1\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (w_zero * zero_eig_loss +\n",
    "                  w_trace * trace_loss +\n",
    "                  w_div * diversity_loss +\n",
    "                  w_offdiag * off_diag_loss +\n",
    "                  w_orth * orth_loss +\n",
    "                  w_order * ordering_loss +\n",
    "                  w_stability * stability_loss +\n",
    "                  w_smooth * smoothness_loss)\n",
    "    \n",
    "    # Components for logging\n",
    "    components = {\n",
    "        'zero_eig': zero_eig_loss.item(),\n",
    "        'trace': trace_loss.item(),\n",
    "        'diversity': diversity_loss.item(),\n",
    "        'offdiag': off_diag_loss.item(),\n",
    "        'orth': orth_loss.item(),\n",
    "        'order': ordering_loss.item(),\n",
    "        'stability': stability_loss.item(),\n",
    "        'smooth': smoothness_loss.item(),\n",
    "        'svd_cond': S_ratio.item(),\n",
    "    }\n",
    "    \n",
    "    return total_loss, components\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr_stage1'], weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10000, T_mult=2, eta_min=CONFIG['lr_min']\n",
    ")\n",
    "\n",
    "loss_history = []\n",
    "best_loss = float('inf')\n",
    "no_improve_count = 0\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP (Your SVD orthogonalization + My improvements)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYBRID TRAINING START\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Strategy: SVD orthogonalization + Fourier features + Two-stage training\")\n",
    "print(f\"Stage 1: 0-{CONFIG['stage1_epochs']:,} | Stage 2: {CONFIG['stage1_epochs']:,}-{CONFIG['max_epochs']:,}\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG['max_epochs'] + 1):\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE TRANSITION\n",
    "    # ========================================================================\n",
    "    if epoch == CONFIG['stage1_epochs'] + 1:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STAGE 2: Fine-tuning all eigenvalues\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = CONFIG['lr_stage2']\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=CONFIG['max_epochs'] - CONFIG['stage1_epochs'],\n",
    "            eta_min=CONFIG['lr_min']\n",
    "        )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FORWARD PASS WITH SVD ORTHOGONALIZATION (Your approach)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    \n",
    "    U = model(X)  # (N, k)\n",
    "    \n",
    "    # SVD-based M-orthogonalization\n",
    "    B = U.T @ (M @ U)  # Gram matrix\n",
    "    V, S, _ = torch.linalg.svd(B)\n",
    "    \n",
    "    # Compute B^(-1/2)\n",
    "    S_inv_sqrt = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S, min=1e-7)))\n",
    "    B_inv_sqrt = V @ S_inv_sqrt @ V.T\n",
    "    U_orth = U @ B_inv_sqrt  # M-orthonormal basis\n",
    "    \n",
    "    # Rayleigh quotient\n",
    "    rayleigh_matrix = U_orth.T @ (K @ U_orth)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOSS COMPUTATION\n",
    "    # ========================================================================\n",
    "    optimizer.zero_grad()\n",
    "    loss, loss_components = compute_loss(\n",
    "        U, U_orth, K, M, rayleigh_matrix, B, S, eigvals, epoch, CONFIG\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BACKWARD PASS\n",
    "    # ========================================================================\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOGGING\n",
    "    # ========================================================================\n",
    "    if epoch % CONFIG['print_every'] == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get eigenvalues in original scale\n",
    "            approx_eigs = torch.diag(rayleigh_matrix).cpu().numpy()\n",
    "            approx_eigs.sort()\n",
    "            approx_eigs_original = approx_eigs * (K_scale / M_scale).cpu().numpy()\n",
    "            \n",
    "            # Compute errors\n",
    "            abs_error = np.abs(approx_eigs_original[:k] - eigvals[:k])\n",
    "            rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "            mean_rel_error = np.mean(rel_error)\n",
    "            median_rel_error = np.median(rel_error)\n",
    "            \n",
    "            # Orthogonality check\n",
    "            B_orth = U_orth.T @ (M @ U_orth)\n",
    "            orth_residual = torch.norm(B_orth - torch.eye(k, device=device, dtype=torch.float64), p='fro').item()\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            stage = \"Stage1\" if epoch <= CONFIG['stage1_epochs'] else \"Stage2\"\n",
    "        \n",
    "        print(f\"[{stage}] Epoch {epoch:>6} | LR={current_lr:.6f}\")\n",
    "        print(f\"  Loss={loss.item():.6f} | λ₁²={loss_components['zero_eig']:.2e} | Trace={loss_components['trace']:.3f}\")\n",
    "        print(f\"  Orth={orth_residual:.2e} | OffDiag={loss_components['offdiag']:.2e} | SVD_cond={loss_components['svd_cond']:.2e}\")\n",
    "        print(f\"  MeanRelErr={mean_rel_error:.4%} | MedianRelErr={median_rel_error:.4%}\")\n",
    "        print(f\"  λ∈[{approx_eigs_original[0]:.6f}, {approx_eigs_original[-1]:.6f}]\")\n",
    "        \n",
    "        if epoch % (CONFIG['print_every'] * 5) == 0:\n",
    "            print(f\"  First 5 - Pred: {approx_eigs_original[:5]}\")\n",
    "            print(f\"  First 5 - True: {eigvals[:5]}\")\n",
    "            print(f\"  Last  5 - Pred: {approx_eigs_original[-5:]}\")\n",
    "            print(f\"  Last  5 - True: {eigvals[k-5:k]}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CHECKPOINTING & EARLY STOPPING\n",
    "    # ========================================================================\n",
    "    if epoch % CONFIG['checkpoint_every'] == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'loss_history': loss_history,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "        \n",
    "        if loss.item() < best_loss * CONFIG['early_stopping_threshold']:\n",
    "            best_loss = loss.item()\n",
    "            no_improve_count = 0\n",
    "            torch.save(checkpoint, checkpoint_dir / 'best_model.pt')\n",
    "            print(f\"  ✓ New best model (loss={best_loss:.6f})\")\n",
    "        else:\n",
    "            no_improve_count += CONFIG['checkpoint_every']\n",
    "        \n",
    "        if no_improve_count >= CONFIG['early_stopping_patience']:\n",
    "            print(f\"\\n⚠ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading best model...\")\n",
    "best_checkpoint = torch.load(checkpoint_dir / 'best_model.pt')\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    \n",
    "    # Final SVD orthogonalization\n",
    "    B_final = U_final.T @ (M @ U_final)\n",
    "    V_final, S_final, _ = torch.linalg.svd(B_final)\n",
    "    S_inv_sqrt_final = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S_final, min=1e-7)))\n",
    "    B_inv_sqrt_final = V_final @ S_inv_sqrt_final @ V_final.T\n",
    "    U_orth_final = U_final @ B_inv_sqrt_final\n",
    "    \n",
    "    # Final matrices\n",
    "    final_rayleigh = U_orth_final.T @ (K @ U_orth_final)\n",
    "    final_ortho = U_orth_final.T @ (M @ U_orth_final)\n",
    "    \n",
    "    # Eigenvalues\n",
    "    final_eigs = torch.diag(final_rayleigh).cpu().numpy()\n",
    "    final_eigs.sort()\n",
    "    final_eigs_original = final_eigs * (K_scale / M_scale).cpu().numpy()\n",
    "    \n",
    "    # Errors\n",
    "    abs_error = np.abs(final_eigs_original - eigvals[:k])\n",
    "    rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Orthogonality\n",
    "    identity_k = torch.eye(k, device=device, dtype=torch.float64)\n",
    "    orth_residual = torch.norm(final_ortho - identity_k, p='fro').item()\n",
    "    orth_diag = torch.diag(final_ortho).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n Orthogonality Quality:\")\n",
    "    print(f\"  ||U^T M U - I||_F = {orth_residual:.2e}\")\n",
    "    print(f\"  Diagonal range: [{orth_diag.min():.6f}, {orth_diag.max():.6f}]\")\n",
    "    \n",
    "    # Rayleigh matrix\n",
    "    rayleigh_diag = torch.diag(final_rayleigh).cpu().numpy()\n",
    "    rayleigh_offdiag = (final_rayleigh - torch.diag(torch.diag(final_rayleigh))).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n Rayleigh Matrix:\")\n",
    "    print(f\"  Diagonal norm: {np.linalg.norm(rayleigh_diag):.6f}\")\n",
    "    print(f\"  Off-diagonal norm: {np.linalg.norm(rayleigh_offdiag, 'fro'):.2e}\")\n",
    "    \n",
    "    # SVD condition\n",
    "    print(f\"\\n SVD Stability:\")\n",
    "    print(f\"  Condition number: {S_final.max().item() / S_final.min().item():.2e}\")\n",
    "    \n",
    "    # Eigenvalue comparison\n",
    "    print(f\"\\n Eigenvalue Comparison (First 10):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<14} {'Reference':<14} {'Abs Error':<14} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i in range(min(10, k)):\n",
    "        print(f\"{i+1:<6} {final_eigs_original[i]:<14.8f} {eigvals[i]:<14.8f} \"\n",
    "              f\"{abs_error[i]:<14.8f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    print(f\"\\n Eigenvalue Comparison (Last 10):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<14} {'Reference':<14} {'Abs Error':<14} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i in range(max(0, k-10), k):\n",
    "        print(f\"{i+1:<6} {final_eigs_original[i]:<14.8f} {eigvals[i]:<14.8f} \"\n",
    "              f\"{abs_error[i]:<14.8f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n Overall Statistics ({k} modes):\")\n",
    "    print(f\"  Mean Absolute Error:     {np.mean(abs_error):.8f}\")\n",
    "    print(f\"  Mean Relative Error:     {np.mean(rel_error):.4%}\")\n",
    "    print(f\"  Median Relative Error:   {np.median(rel_error):.4%}\")\n",
    "    print(f\"  Max Relative Error:      {np.max(rel_error):.4%}\")\n",
    "    print(f\"  Modes with <1% error:    {np.sum(rel_error < 0.01)}/{k}\")\n",
    "    print(f\"  Modes with <5% error:    {np.sum(rel_error < 0.05)}/{k}\")\n",
    "    print(f\"  Modes with <10% error:   {np.sum(rel_error < 0.10)}/{k}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'final_eigenvalues': final_eigs_original,\n",
    "    'reference_eigenvalues': eigvals[:k],\n",
    "    'errors': {'absolute': abs_error, 'relative': rel_error},\n",
    "    'normalization': {'K_scale': K_scale.item(), 'M_scale': M_scale.item()},\n",
    "}, 'hybrid_model_final.pt')\n",
    "\n",
    "print(\"\\n✓ Model saved to 'hybrid_model_final.pt'\")\n",
    "print(\"✓ Checkpoints in 'checkpoints/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d01eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
