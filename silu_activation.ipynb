{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73485d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab728374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391ee252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n"
     ]
    }
   ],
   "source": [
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1a913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5963311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbe9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1    , LR=0.009995, Total Loss=49.9957, Eig Loss=0.0004, Orth Loss=49.9954\n",
      "Epoch 500  , LR=0.007943, Total Loss=47.0539, Eig Loss=1.5662, Orth Loss=45.4877\n",
      "Epoch 1000 , LR=0.006310, Total Loss=46.5962, Eig Loss=2.0098, Orth Loss=44.5864\n",
      "Epoch 1500 , LR=0.005012, Total Loss=46.1922, Eig Loss=2.7743, Orth Loss=43.4179\n",
      "Epoch 2000 , LR=0.003981, Total Loss=46.0724, Eig Loss=2.8329, Orth Loss=43.2396\n",
      "Epoch 2500 , LR=0.003162, Total Loss=46.0244, Eig Loss=2.8508, Orth Loss=43.1736\n",
      "Epoch 3000 , LR=0.002512, Total Loss=46.0040, Eig Loss=2.8743, Orth Loss=43.1297\n",
      "Epoch 3500 , LR=0.001995, Total Loss=45.9602, Eig Loss=3.1526, Orth Loss=42.8076\n",
      "Epoch 4000 , LR=0.001585, Total Loss=45.9368, Eig Loss=3.2058, Orth Loss=42.7310\n",
      "Epoch 4500 , LR=0.001259, Total Loss=45.9196, Eig Loss=3.3049, Orth Loss=42.6148\n",
      "Epoch 5000 , LR=0.001000, Total Loss=45.9053, Eig Loss=3.3778, Orth Loss=42.5275\n",
      "Epoch 5500 , LR=0.000794, Total Loss=45.8934, Eig Loss=3.4101, Orth Loss=42.4833\n",
      "Epoch 6000 , LR=0.000631, Total Loss=45.8835, Eig Loss=3.4312, Orth Loss=42.4523\n",
      "Epoch 6500 , LR=0.000501, Total Loss=45.8729, Eig Loss=3.4440, Orth Loss=42.4289\n",
      "Epoch 7000 , LR=0.000398, Total Loss=45.8503, Eig Loss=3.4513, Orth Loss=42.3990\n",
      "Epoch 7500 , LR=0.000316, Total Loss=45.8128, Eig Loss=3.4565, Orth Loss=42.3563\n",
      "Epoch 8000 , LR=0.000251, Total Loss=45.7701, Eig Loss=3.4403, Orth Loss=42.3298\n",
      "Epoch 8500 , LR=0.000200, Total Loss=45.7320, Eig Loss=3.4281, Orth Loss=42.3039\n",
      "Epoch 9000 , LR=0.000158, Total Loss=45.7035, Eig Loss=3.4226, Orth Loss=42.2809\n",
      "Epoch 9500 , LR=0.000126, Total Loss=45.6706, Eig Loss=3.4095, Orth Loss=42.2611\n",
      "Epoch 10000, LR=0.000100, Total Loss=45.6307, Eig Loss=3.3875, Orth Loss=42.2432\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron for mapping coordinates to k eigenmodes.\n",
    "    Uses SiLU (Swish) activation for better gradient flow than Tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64, 64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            # Using nn.SiLU (Swish) instead of nn.Tanh\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# --- 3. Model Instantiation and Initialization ---\n",
    "\n",
    "# Instantiate model\n",
    "model = MLP().to(device)\n",
    "\n",
    "# Initialize all layers (Xavier), final layer small (Best practice for PINNs)\n",
    "for name, p in model.named_parameters():\n",
    "    if 'net' in name:\n",
    "        # Standard Xavier for hidden layers (weights)\n",
    "        if p.dim() > 1 and name.split('.')[1] != str(len(model.net) - 1):\n",
    "            nn.init.xavier_uniform_(p.data)\n",
    "        # Final Linear layer: Small weights and zero bias\n",
    "        if name.split('.')[1] == str(len(model.net) - 1):\n",
    "            if p.ndim == 2:\n",
    "                # Weights: Very small normal distribution\n",
    "                nn.init.normal_(p.data, std=1e-3)\n",
    "            else:\n",
    "                # Biases: Zero\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "# --- 4. Training Setup ---\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_orth = 1.0           # CRITICAL: Weight for the orthogonality loss.\n",
    "                            # Adjust this if orth_loss and eig_loss have vastly different scales.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "max_epochs = 10_000         # Reduced epochs for quicker demonstration\n",
    "print_every = 500\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_start)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "# --- 5. Training Loop ---\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "identity_k = torch.eye(k, device=device)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward Pass\n",
    "    U = model(X)  # N x k (Basis functions evaluated at coordinates X)\n",
    "\n",
    "    # Calculate Loss Components\n",
    "    \n",
    "    # 1. Eigenvalue Loss (Minimize Rayleigh Quotient)\n",
    "    # The term U.T @ (K @ U) results in a k x k matrix. The trace sums the k eigenvalues.\n",
    "    eig_loss = torch.trace(U.T @ (K @ U)) \n",
    "    \n",
    "    # 2. Orthogonality Loss (Ensure M-orthonormality: U^T M U = I)\n",
    "    B = U.T @ (M @ U)        # k x k (Orthogonality matrix)\n",
    "    orth_loss = torch.norm(B - identity_k, p='fro')**2\n",
    "\n",
    "    # Total Loss (Weighted sum)\n",
    "    # Improvement: Explicitly use lambda_orth for weighting\n",
    "    loss = eig_loss + lambda_orth * orth_loss\n",
    "\n",
    "    # Backpropagation and Step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Logging and Analysis\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        # Calculate the approximate eigenvalues by extracting the diagonal of U.T @ K @ U\n",
    "        # Note: These values should be minimized, and they represent the k smallest modes.\n",
    "        approx_vals = torch.diag(U.T @ (K @ U)).detach().cpu().numpy()\n",
    "        \n",
    "        # Sort and print the approximate eigenvalues for meaningful comparison\n",
    "        sorted_vals = np.sort(approx_vals)\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch:<5}, LR={current_lr:.6f}, \"\n",
    "            f\"Total Loss={loss.item():.4f}, \"\n",
    "            f\"Eig Loss={eig_loss.item():.4f}, \"\n",
    "            f\"Orth Loss={orth_loss.item():.4f}\"\n",
    "        )\n",
    "        # print(f\"  Approx Eigenvalues (Min k): {sorted_vals}\")\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3887a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "\n",
    "# Final Eigenvalue Check\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    final_rayleigh_matrix = U_final.T @ (K @ U_final)\n",
    "    final_ortho_matrix = U_final.T @ (M @ U_final)\n",
    "\n",
    "    final_eigenvalues = torch.diag(final_rayleigh_matrix).cpu().numpy()\n",
    "    final_eigenvalues.sort()\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(f\"Final Approximate Eigenvalues (Sorted): {np.round(final_eigenvalues[:5], 6)}\")\n",
    "    print(\"Reference eigenvalues (first k):   \", np.round(eigvals[:5], 6))\n",
    "    \n",
    "    # Print the Orthogonality Matrix (should be close to Identity)\n",
    "    print(\"\\nFinal Orthogonality Matrix (U^T M U):\")\n",
    "    print(final_ortho_matrix.cpu().numpy().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70db4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Total Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
