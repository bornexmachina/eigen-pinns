{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ce50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "\n",
      "Using epsilon=0.0001, final condition number: 1.47e+05\n",
      "\n",
      "=== Matrix Normalization ===\n",
      "Epoch 1, total_loss=1075.187125, eig_loss=0.077544, orth_loss=1075.109581)\n",
      "Epoch 1000, total_loss=2.311047, eig_loss=2.205674, orth_loss=0.105373)\n",
      "Epoch 2000, total_loss=2.163453, eig_loss=2.069941, orth_loss=0.093511)\n",
      "Epoch 3000, total_loss=2.052492, eig_loss=1.997693, orth_loss=0.054800)\n",
      "Epoch 4000, total_loss=2.031165, eig_loss=1.937679, orth_loss=0.093486)\n",
      "Epoch 5000, total_loss=1.907106, eig_loss=1.860658, orth_loss=0.046448)\n",
      "Epoch 6000, total_loss=1.886454, eig_loss=1.808165, orth_loss=0.078289)\n",
      "Epoch 7000, total_loss=1.818474, eig_loss=1.773256, orth_loss=0.045218)\n",
      "Epoch 8000, total_loss=1.790870, eig_loss=1.749750, orth_loss=0.041120)\n",
      "Epoch 9000, total_loss=1.771857, eig_loss=1.726291, orth_loss=0.045565)\n",
      "Epoch 10000, total_loss=1.751434, eig_loss=1.711365, orth_loss=0.040069)\n",
      "Epoch 11000, total_loss=1.733853, eig_loss=1.702562, orth_loss=0.031291)\n",
      "Epoch 12000, total_loss=1.721075, eig_loss=1.691161, orth_loss=0.029914)\n",
      "Epoch 13000, total_loss=1.708445, eig_loss=1.683286, orth_loss=0.025160)\n",
      "Epoch 14000, total_loss=1.703276, eig_loss=1.678194, orth_loss=0.025082)\n",
      "Epoch 15000, total_loss=1.691476, eig_loss=1.670108, orth_loss=0.021368)\n",
      "Epoch 16000, total_loss=1.684650, eig_loss=1.666274, orth_loss=0.018376)\n",
      "Epoch 17000, total_loss=1.684236, eig_loss=1.661220, orth_loss=0.023016)\n",
      "Epoch 18000, total_loss=1.679509, eig_loss=1.656458, orth_loss=0.023051)\n",
      "Epoch 19000, total_loss=1.672751, eig_loss=1.656158, orth_loss=0.016593)\n",
      "Epoch 20000, total_loss=1.665641, eig_loss=1.651245, orth_loss=0.014396)\n",
      "Epoch 21000, total_loss=1.663648, eig_loss=1.650416, orth_loss=0.013232)\n",
      "Epoch 22000, total_loss=1.662866, eig_loss=1.647063, orth_loss=0.015803)\n",
      "Epoch 23000, total_loss=1.658172, eig_loss=1.646119, orth_loss=0.012053)\n",
      "Epoch 24000, total_loss=1.654269, eig_loss=1.645204, orth_loss=0.009065)\n",
      "Epoch 25000, total_loss=1.653359, eig_loss=1.643886, orth_loss=0.009474)\n",
      "Epoch 26000, total_loss=1.652110, eig_loss=1.642639, orth_loss=0.009471)\n",
      "Epoch 27000, total_loss=1.650134, eig_loss=1.642474, orth_loss=0.007660)\n",
      "Epoch 28000, total_loss=1.649330, eig_loss=1.641843, orth_loss=0.007487)\n",
      "Epoch 29000, total_loss=1.648597, eig_loss=1.641178, orth_loss=0.007419)\n",
      "Epoch 30000, total_loss=1.648155, eig_loss=1.640461, orth_loss=0.007693)\n",
      "Epoch 31000, total_loss=1.789644, eig_loss=1.773393, orth_loss=0.016250)\n",
      "Epoch 32000, total_loss=1.735630, eig_loss=1.715539, orth_loss=0.020091)\n",
      "Epoch 33000, total_loss=1.736645, eig_loss=1.692492, orth_loss=0.044152)\n",
      "Epoch 34000, total_loss=1.708900, eig_loss=1.676960, orth_loss=0.031940)\n",
      "Epoch 35000, total_loss=1.690411, eig_loss=1.666941, orth_loss=0.023470)\n",
      "Epoch 36000, total_loss=1.683228, eig_loss=1.656784, orth_loss=0.026444)\n",
      "Epoch 37000, total_loss=1.673711, eig_loss=1.649480, orth_loss=0.024231)\n",
      "Epoch 38000, total_loss=1.672144, eig_loss=1.647537, orth_loss=0.024608)\n",
      "Epoch 39000, total_loss=1.668354, eig_loss=1.642076, orth_loss=0.026278)\n",
      "Epoch 40000, total_loss=1.664858, eig_loss=1.641963, orth_loss=0.022895)\n",
      "Epoch 41000, total_loss=1.662794, eig_loss=1.640691, orth_loss=0.022103)\n",
      "Epoch 42000, total_loss=1.659247, eig_loss=1.636406, orth_loss=0.022841)\n",
      "Epoch 43000, total_loss=1.658917, eig_loss=1.637289, orth_loss=0.021628)\n",
      "Epoch 44000, total_loss=1.652682, eig_loss=1.634591, orth_loss=0.018091)\n",
      "Epoch 45000, total_loss=1.655246, eig_loss=1.634572, orth_loss=0.020674)\n",
      "Epoch 46000, total_loss=1.650598, eig_loss=1.631715, orth_loss=0.018884)\n",
      "Epoch 47000, total_loss=1.649433, eig_loss=1.628852, orth_loss=0.020582)\n",
      "Epoch 48000, total_loss=1.651275, eig_loss=1.629095, orth_loss=0.022180)\n",
      "Epoch 49000, total_loss=1.646725, eig_loss=1.629348, orth_loss=0.017377)\n",
      "Epoch 50000, total_loss=1.643012, eig_loss=1.629194, orth_loss=0.013818)\n",
      "Epoch 51000, total_loss=1.647332, eig_loss=1.627225, orth_loss=0.020107)\n",
      "Epoch 52000, total_loss=1.643466, eig_loss=1.627201, orth_loss=0.016265)\n",
      "Epoch 53000, total_loss=1.642331, eig_loss=1.626311, orth_loss=0.016021)\n",
      "Epoch 54000, total_loss=1.643545, eig_loss=1.626849, orth_loss=0.016696)\n",
      "Epoch 55000, total_loss=1.641936, eig_loss=1.626585, orth_loss=0.015351)\n",
      "Epoch 56000, total_loss=1.640706, eig_loss=1.626464, orth_loss=0.014242)\n",
      "Epoch 57000, total_loss=1.639717, eig_loss=1.625220, orth_loss=0.014497)\n",
      "Epoch 58000, total_loss=1.639047, eig_loss=1.624286, orth_loss=0.014760)\n",
      "Epoch 59000, total_loss=1.637549, eig_loss=1.623425, orth_loss=0.014124)\n",
      "Epoch 60000, total_loss=1.636062, eig_loss=1.624600, orth_loss=0.011462)\n",
      "Epoch 61000, total_loss=1.637390, eig_loss=1.624795, orth_loss=0.012595)\n",
      "Epoch 62000, total_loss=1.635362, eig_loss=1.623714, orth_loss=0.011648)\n",
      "Epoch 63000, total_loss=1.634760, eig_loss=1.622529, orth_loss=0.012231)\n",
      "Epoch 64000, total_loss=1.634763, eig_loss=1.622160, orth_loss=0.012603)\n",
      "Epoch 65000, total_loss=1.633594, eig_loss=1.622437, orth_loss=0.011157)\n",
      "Epoch 66000, total_loss=1.633233, eig_loss=1.622099, orth_loss=0.011134)\n",
      "Epoch 67000, total_loss=1.633059, eig_loss=1.621232, orth_loss=0.011827)\n",
      "Epoch 68000, total_loss=1.632097, eig_loss=1.621604, orth_loss=0.010493)\n",
      "Epoch 69000, total_loss=1.631761, eig_loss=1.621941, orth_loss=0.009820)\n",
      "Epoch 70000, total_loss=1.630521, eig_loss=1.620515, orth_loss=0.010006)\n",
      "Epoch 71000, total_loss=1.630954, eig_loss=1.621368, orth_loss=0.009587)\n",
      "Epoch 72000, total_loss=1.630008, eig_loss=1.621366, orth_loss=0.008642)\n",
      "Epoch 73000, total_loss=1.629603, eig_loss=1.620648, orth_loss=0.008955)\n",
      "Epoch 74000, total_loss=1.629390, eig_loss=1.620004, orth_loss=0.009385)\n",
      "Epoch 75000, total_loss=1.629348, eig_loss=1.620962, orth_loss=0.008386)\n",
      "Epoch 76000, total_loss=1.628697, eig_loss=1.619831, orth_loss=0.008866)\n",
      "Epoch 77000, total_loss=1.628168, eig_loss=1.620250, orth_loss=0.007918)\n",
      "Epoch 78000, total_loss=1.627643, eig_loss=1.619800, orth_loss=0.007843)\n",
      "Epoch 79000, total_loss=1.627455, eig_loss=1.619941, orth_loss=0.007514)\n",
      "Epoch 80000, total_loss=1.627510, eig_loss=1.619933, orth_loss=0.007577)\n",
      "Epoch 81000, total_loss=1.627061, eig_loss=1.619688, orth_loss=0.007373)\n",
      "Epoch 82000, total_loss=1.626836, eig_loss=1.619295, orth_loss=0.007541)\n",
      "Epoch 83000, total_loss=1.626907, eig_loss=1.619877, orth_loss=0.007030)\n",
      "Epoch 84000, total_loss=1.626562, eig_loss=1.619300, orth_loss=0.007262)\n",
      "Epoch 85000, total_loss=1.626450, eig_loss=1.619437, orth_loss=0.007013)\n",
      "Epoch 86000, total_loss=1.626277, eig_loss=1.619205, orth_loss=0.007072)\n",
      "Epoch 87000, total_loss=1.626197, eig_loss=1.619064, orth_loss=0.007134)\n",
      "Epoch 88000, total_loss=1.626165, eig_loss=1.619029, orth_loss=0.007135)\n",
      "Epoch 89000, total_loss=1.626155, eig_loss=1.619002, orth_loss=0.007153)\n",
      "Epoch 90000, total_loss=1.626117, eig_loss=1.619032, orth_loss=0.007085)\n",
      "Epoch 91000, total_loss=1.655231, eig_loss=1.643169, orth_loss=0.012061)\n",
      "Epoch 92000, total_loss=1.646544, eig_loss=1.632034, orth_loss=0.014510)\n",
      "Epoch 93000, total_loss=1.643797, eig_loss=1.626800, orth_loss=0.016997)\n",
      "Epoch 94000, total_loss=1.641061, eig_loss=1.624140, orth_loss=0.016922)\n",
      "Epoch 95000, total_loss=1.652747, eig_loss=1.622946, orth_loss=0.029801)\n",
      "Epoch 96000, total_loss=1.641397, eig_loss=1.624300, orth_loss=0.017097)\n",
      "Epoch 97000, total_loss=1.639252, eig_loss=1.622896, orth_loss=0.016357)\n",
      "Epoch 98000, total_loss=1.638397, eig_loss=1.622221, orth_loss=0.016176)\n",
      "Epoch 99000, total_loss=1.641035, eig_loss=1.619790, orth_loss=0.021245)\n",
      "Epoch 100000, total_loss=1.637405, eig_loss=1.619172, orth_loss=0.018233)\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Eigenvalue Comparison (first 10 modes):\n",
      "Mode   Predicted    Reference    Abs Error    Rel Error   \n",
      "------------------------------------------------------------------\n",
      "1      504282051.136643 0.000000     504282051.136643 502521672918175252480.0000%\n",
      "2      544464659.210990 0.007574     544464659.203416 7188464076158.5957%\n",
      "3      613832972.024351 0.030308     613832971.994043 2025322477493.5474%\n",
      "4      628332957.862635 0.068146     628332957.794488 922032879695.5225%\n",
      "5      645062716.747953 0.121208     645062716.626745 532194976768.2278%\n",
      "6      648805376.798260 0.189243     648805376.609017 342842926222.1324%\n",
      "7      692252443.380499 0.272231     692252443.108267 254288150263.7819%\n",
      "8      750602049.534070 0.370536     750602049.163534 202571990065.6979%\n",
      "9      751465620.067040 0.483409     751465619.583630 155451175202.9999%\n",
      "10     764703413.216981 0.611343     764703412.605638 125085914901.8893%\n",
      "\n",
      "Eigenvalue Comparison (last 10 modes):\n",
      "Mode   Predicted    Reference    Abs Error    Rel Error   \n",
      "------------------------------------------------------------------\n",
      "41     1182958268.411090 7.422193     1182958260.988897 15938123020.3902%\n",
      "42     1195147973.902245 7.449484     1195147966.452760 16043365449.2520%\n",
      "43     1205215076.292625 7.455601     1205215068.837025 16165231146.3168%\n",
      "44     1219103948.443435 7.508104     1219103940.935331 16237174837.9991%\n",
      "45     1219738756.970441 7.526791     1219738749.443650 16205296243.4673%\n",
      "46     1257260206.670696 7.607393     1257260199.063303 16526819716.2622%\n",
      "47     1314684191.161184 7.614276     1314684183.546908 17266042565.8505%\n",
      "48     1324708243.639195 7.708104     1324708235.931091 17185916099.6864%\n",
      "49     1381608762.031353 7.724856     1381608754.306497 17885236716.4912%\n",
      "50     1527389789.285752 7.834566     1527389781.451186 19495526097.4042%\n",
      "\n",
      "Overall Statistics (all 50 modes):\n",
      "  Mean Absolute Error:   973115775.829554\n",
      "  Mean Relative Error:   10050433716225822720.0000%\n",
      "  Median Relative Error: 21974251322.7628%\n",
      "  Max Relative Error:    502521672918175252480.0000%\n",
      "  Modes with <5% error:  0/50\n",
      "  Modes with <10% error: 0/50\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "N = X.shape[0]\n",
    "\n",
    "\n",
    "# 1. Start with stronger regularization\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Apply the regularization\n",
    "K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "print(f\"\\nUsing epsilon={epsilon}, final condition number: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "\n",
    "print(\"\\n=== Matrix Normalization ===\")\n",
    "K_scale = torch.norm(K_reg, p='fro')\n",
    "M_scale = torch.norm(M, p='fro')\n",
    "\n",
    "K = K_reg / K_scale\n",
    "M = M / M_scale\n",
    "\n",
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50\n",
    "\n",
    "# Build the neural network that maps coordinates -> k outputs per node\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[256, 256]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h, dtype=torch.double))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim, dtype=torch.double))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = MLP().to(device)\n",
    "# Initialize all layers (Xavier), final layer small\n",
    "for name, p in model.named_parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "for p in model.net[-1].parameters():  # last Linear\n",
    "    if p.ndim == 2:\n",
    "        nn.init.normal_(p, std=1e-2)\n",
    "    else:\n",
    "        nn.init.zeros_(p)\n",
    "\n",
    "# --- Training loop ---\n",
    "max_epochs = 100_000\n",
    "print_every = 1_000\n",
    "loss_history = []\n",
    "\n",
    "lr_start = 0.05\n",
    "lr_end = 0.001\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr_start, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30000, T_mult=2, eta_min=lr_end)\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Predict eigenvectors\n",
    "    U = model(X)  # N x k\n",
    "\n",
    "    # --- Column-wise normalization w.r.t M ---\n",
    "    col_norms = torch.sqrt(torch.diagonal(U.T @ M @ U))  # k\n",
    "    U_normed = U / col_norms.unsqueeze(0)                # normalize each eigenvector\n",
    "    \n",
    "    # --- Compute matrices ---\n",
    "    UMU = U_normed.T @ M @ U_normed\n",
    "    UKU = U_normed.T @ K @ U_normed\n",
    "\n",
    "    # --- Eigenvalue loss ---\n",
    "    eigenvalues_approx = torch.diag(UKU)\n",
    "    sorted_eigs, _ = torch.sort(eigenvalues_approx)\n",
    "    zero_eig_loss = sorted_eigs[0] ** 2\n",
    "    eig_loss_trace = torch.sum(sorted_eigs[1:])\n",
    "    \n",
    "    gaps = sorted_eigs[1:] - sorted_eigs[:-1]\n",
    "    min_gap = 1e-4\n",
    "    diversity_loss = torch.sum(torch.relu(min_gap - gaps))\n",
    "    \n",
    "    off_diag_mask = 1 - torch.eye(k, device=device, dtype=torch.float64)\n",
    "    eig_loss_offdiag = torch.sum((UKU * off_diag_mask)**2)\n",
    "    \n",
    "    eig_loss = zero_eig_loss + eig_loss_trace + diversity_loss + eig_loss_offdiag\n",
    "\n",
    "    # --- Orthogonality loss ---\n",
    "    orth_loss = torch.norm(UMU - torch.eye(k, device=device), p='fro')**2\n",
    "\n",
    " \n",
    "    loss = eig_loss +  orth_loss\n",
    "\n",
    "    # --- Backprop ---\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # optional gradient clipping\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}, total_loss={loss.item():.6f}, eig_loss={eig_loss.item():.6f}, \"\n",
    "              f\"orth_loss={orth_loss.item():.6f})\")\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "# ==== Final result ====\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    UKU = U_final.T @ (K @ U_final)\n",
    "    final_eigs = torch.diag(UKU).cpu().numpy() * (K_scale / M_scale).cpu().numpy()\n",
    "    final_eigs.sort()\n",
    "\n",
    "    # FINAL EVALUATION\n",
    "    abs_error = np.abs(final_eigs - eigvals[:k])\n",
    "    rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "    #print(\"\\nLearned Ritz values (from U^T K U):\", np.round(mu[:5], 6))\n",
    "    #print(\"Reference eigenvalues (first k):   \", np.round(eigvals[:5], 6))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Eigenvalue comparison\n",
    "    print(f\"\\nEigenvalue Comparison (first 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(min(10, k)):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    print(f\"\\nEigenvalue Comparison (last 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(max(0, k-10), k):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics (all {k} modes):\")\n",
    "    print(f\"  Mean Absolute Error:   {np.mean(abs_error):.6f}\")\n",
    "    print(f\"  Mean Relative Error:   {np.mean(rel_error):.4%}\")\n",
    "    print(f\"  Median Relative Error: {np.median(rel_error):.4%}\")\n",
    "    print(f\"  Max Relative Error:    {np.max(rel_error):.4%}\")\n",
    "    print(f\"  Modes with <5% error:  {np.sum(rel_error < 0.05)}/{k}\")\n",
    "    print(f\"  Modes with <10% error: {np.sum(rel_error < 0.10)}/{k}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978b8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
