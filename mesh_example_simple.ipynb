{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8a717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e27da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d16402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n"
     ]
    }
   ],
   "source": [
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abff7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "N = X.shape[0]\n",
    "\n",
    "\n",
    "# 1. Start with stronger regularization\n",
    "#epsilon = 1e-4\n",
    "\n",
    "# Apply the regularization\n",
    "#K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "#print(f\"\\nUsing epsilon={epsilon}, final condition number: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "\n",
    "#print(\"\\n=== Matrix Normalization ===\")\n",
    "#K_scale = torch.norm(K_reg, p='fro')\n",
    "#M_scale = torch.norm(M, p='fro')\n",
    "\n",
    "#K = K_reg / K_scale\n",
    "#M = M / K_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653bef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dddb7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network that maps coordinates -> k outputs per node\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h, dtype=torch.double))\n",
    "            layers.append(nn.Tanh())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim, dtype=torch.double))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = MLP().to(device)\n",
    "# Initialize all layers (Xavier), final layer small\n",
    "for name, p in model.named_parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "for p in model.net[-1].parameters():  # last Linear\n",
    "    if p.ndim == 2:\n",
    "        nn.init.normal_(p, std=1e-3)\n",
    "    else:\n",
    "        nn.init.zeros_(p)\n",
    "\n",
    "lr_start = 0.05\n",
    "lr_end = 0.001\n",
    "max_epochs = 10_000\n",
    "print_every = 1_000\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a1472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, total loss=2497.217700, eig_loss=0.000010, orth_loss=49.944354\n",
      "Epoch 1000, total loss=2201.805288, eig_loss=5.767390, orth_loss=43.920758\n",
      "Epoch 2000, total loss=2009.677305, eig_loss=16.129654, orth_loss=39.870953\n",
      "Epoch 3000, total loss=1921.751906, eig_loss=24.566942, orth_loss=37.943699\n",
      "Epoch 4000, total loss=1833.446380, eig_loss=41.898296, orth_loss=35.830962\n",
      "Epoch 5000, total loss=1694.038486, eig_loss=72.249184, orth_loss=32.435786\n",
      "Epoch 6000, total loss=1641.323055, eig_loss=81.010171, orth_loss=31.206258\n",
      "Epoch 7000, total loss=1536.596362, eig_loss=117.722790, orth_loss=28.377471\n",
      "Epoch 8000, total loss=1464.815133, eig_loss=140.231847, orth_loss=26.491666\n",
      "Epoch 9000, total loss=1389.144278, eig_loss=167.379269, orth_loss=24.435300\n",
      "Epoch 10000, total loss=1306.605024, eig_loss=200.447962, orth_loss=22.123141\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, max_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    U = model(X)  # N x k\n",
    "\n",
    "    # losses\n",
    "    UMU = U.T @ (M @ U)        # k x k\n",
    "    UKU = U.T @ (K @ U) \n",
    "    orth_loss = torch.norm(UMU - torch.eye(k, device=device), p='fro')**2\n",
    "    eig_loss = torch.norm(UKU, p='fro')**2\n",
    "\n",
    "    loss = eig_loss + orth_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        approx_vals = torch.diag(U.T @ (K @ U)).detach().cpu().numpy()\n",
    "        print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"total loss={loss.item():.6f}, \"\n",
    "        f\"eig_loss={eig_loss.item():.6f}, \"\n",
    "        f\"orth_loss={orth_loss.item():.6f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfdfc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Eigenvalue Comparison (first 10 modes):\n",
      "Mode   Predicted    Reference    Abs Error    Rel Error   \n",
      "------------------------------------------------------------------\n",
      "1      2.478222     0.000000     2.478222     2469570552646.7212%\n",
      "2      0.771758     0.007574     0.764183     10089.3704% \n",
      "3      1.144325     0.030308     1.114017     3675.6628%  \n",
      "4      1.149125     0.068146     1.080978     1586.2571%  \n",
      "5      2.586748     0.121208     2.465541     2034.1406%  \n",
      "6      2.687989     0.189243     2.498746     1320.3920%  \n",
      "7      1.191597     0.272231     0.919366     337.7147%   \n",
      "8      1.556120     0.370536     1.185584     319.9646%   \n",
      "9      0.911725     0.483409     0.428315     88.6030%    \n",
      "10     0.911606     0.611343     0.300263     49.1154%    \n",
      "\n",
      "Eigenvalue Comparison (last 10 modes):\n",
      "Mode   Predicted    Reference    Abs Error    Rel Error   \n",
      "------------------------------------------------------------------\n",
      "41     1.109715     7.422193     6.312478     85.0487%    \n",
      "42     0.888963     7.449484     6.560521     88.0668%    \n",
      "43     2.565029     7.455601     4.890572     65.5959%    \n",
      "44     0.689021     7.508104     6.819083     90.8230%    \n",
      "45     0.799333     7.526791     6.727458     89.3802%    \n",
      "46     0.756126     7.607393     6.851267     90.0606%    \n",
      "47     0.950414     7.614276     6.663863     87.5180%    \n",
      "48     1.261456     7.708104     6.446648     83.6347%    \n",
      "49     1.516996     7.724856     6.207860     80.3621%    \n",
      "50     1.545145     7.834566     6.289421     80.2779%    \n",
      "\n",
      "Overall Statistics (all 50 modes):\n",
      "  Mean Absolute Error:   3.502904\n",
      "  Mean Relative Error:   49391411501.8322%\n",
      "  Median Relative Error: 83.6628%\n",
      "  Max Relative Error:    2469570552646.7212%\n",
      "  Modes with <5% error:  0/50\n",
      "  Modes with <10% error: 0/50\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "# ==== Final result ====\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    UKU = U_final.T @ (K @ U_final)\n",
    "    final_eigs = torch.diag(UKU).cpu().numpy()\n",
    "    #mu, Wsmall = np.linalg.eigh(UKU.cpu().numpy())\n",
    "    #mu = np.real(mu)\n",
    "    # sort\n",
    "    #idx = np.argsort(mu)\n",
    "    #mu = mu[idx]\n",
    "\n",
    "    # FINAL EVALUATION\n",
    "    abs_error = np.abs(final_eigs - eigvals[:k])\n",
    "    rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "    #print(\"\\nLearned Ritz values (from U^T K U):\", np.round(mu[:5], 6))\n",
    "    #print(\"Reference eigenvalues (first k):   \", np.round(eigvals[:5], 6))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Eigenvalue comparison\n",
    "    print(f\"\\nEigenvalue Comparison (first 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(min(10, k)):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    print(f\"\\nEigenvalue Comparison (last 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(max(0, k-10), k):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics (all {k} modes):\")\n",
    "    print(f\"  Mean Absolute Error:   {np.mean(abs_error):.6f}\")\n",
    "    print(f\"  Mean Relative Error:   {np.mean(rel_error):.4%}\")\n",
    "    print(f\"  Median Relative Error: {np.median(rel_error):.4%}\")\n",
    "    print(f\"  Max Relative Error:    {np.max(rel_error):.4%}\")\n",
    "    print(f\"  Modes with <5% error:  {np.sum(rel_error < 0.05)}/{k}\")\n",
    "    print(f\"  Modes with <10% error: {np.sum(rel_error < 0.10)}/{k}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef0315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
