{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5814b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)\n",
    "\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "\n",
    "\n",
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53207a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Full-Batch Training with SVD Stabilization ---\n",
      "Epoch 1    , LR=0.010000, Total Loss=128.8134, Eig Loss=128.4884, Orth Loss=32.4945\n",
      "Epoch 500  , LR=0.009772, Total Loss=0.4149, Eig Loss=0.1408, Orth Loss=27.4080\n",
      "Epoch 1000 , LR=0.009550, Total Loss=0.4283, Eig Loss=0.1544, Orth Loss=27.3827\n",
      "Epoch 1500 , LR=0.009333, Total Loss=0.3189, Eig Loss=0.0460, Orth Loss=27.2878\n",
      "Epoch 2000 , LR=0.009120, Total Loss=0.2954, Eig Loss=0.0236, Orth Loss=27.1823\n",
      "Epoch 2500 , LR=0.008913, Total Loss=0.3088, Eig Loss=0.0383, Orth Loss=27.0472\n",
      "Epoch 3000 , LR=0.008710, Total Loss=0.2743, Eig Loss=0.0048, Orth Loss=26.9456\n",
      "Epoch 3500 , LR=0.008511, Total Loss=0.2914, Eig Loss=0.0225, Orth Loss=26.8887\n",
      "Epoch 4000 , LR=0.008318, Total Loss=0.3171, Eig Loss=0.0488, Orth Loss=26.8321\n",
      "Epoch 4500 , LR=0.008128, Total Loss=0.3200, Eig Loss=0.0522, Orth Loss=26.7817\n",
      "Epoch 5000 , LR=0.007943, Total Loss=0.2818, Eig Loss=0.0146, Orth Loss=26.7207\n",
      "Epoch 5500 , LR=0.007762, Total Loss=0.2720, Eig Loss=0.0053, Orth Loss=26.6664\n",
      "Epoch 6000 , LR=0.007586, Total Loss=0.3123, Eig Loss=0.0464, Orth Loss=26.5933\n",
      "Epoch 6500 , LR=0.007413, Total Loss=0.2717, Eig Loss=0.0065, Orth Loss=26.5128\n",
      "Epoch 7000 , LR=0.007244, Total Loss=0.3077, Eig Loss=0.0434, Orth Loss=26.4321\n",
      "Epoch 7500 , LR=0.007079, Total Loss=0.2720, Eig Loss=0.0082, Orth Loss=26.3728\n",
      "Epoch 8000 , LR=0.006918, Total Loss=0.2738, Eig Loss=0.0109, Orth Loss=26.2817\n",
      "Epoch 8500 , LR=0.006761, Total Loss=0.2762, Eig Loss=0.0144, Orth Loss=26.1853\n",
      "Epoch 9000 , LR=0.006607, Total Loss=0.2759, Eig Loss=0.0157, Orth Loss=26.0259\n",
      "Epoch 9500 , LR=0.006457, Total Loss=0.2662, Eig Loss=0.0071, Orth Loss=25.9053\n",
      "Epoch 10000, LR=0.006310, Total Loss=0.2710, Eig Loss=0.0133, Orth Loss=25.7693\n",
      "Epoch 10500, LR=0.006166, Total Loss=0.2653, Eig Loss=0.0087, Orth Loss=25.6611\n",
      "Epoch 11000, LR=0.006026, Total Loss=0.2722, Eig Loss=0.0168, Orth Loss=25.5482\n",
      "Epoch 11500, LR=0.005888, Total Loss=0.2852, Eig Loss=0.0313, Orth Loss=25.3950\n",
      "Epoch 12000, LR=0.005754, Total Loss=0.2731, Eig Loss=0.0195, Orth Loss=25.3592\n",
      "Epoch 12500, LR=0.005623, Total Loss=0.2549, Eig Loss=0.0022, Orth Loss=25.2705\n",
      "Epoch 13000, LR=0.005495, Total Loss=0.2574, Eig Loss=0.0062, Orth Loss=25.1121\n",
      "Epoch 13500, LR=0.005370, Total Loss=0.2523, Eig Loss=0.0026, Orth Loss=24.9758\n",
      "Epoch 14000, LR=0.005248, Total Loss=0.2576, Eig Loss=0.0092, Orth Loss=24.8368\n",
      "Epoch 14500, LR=0.005129, Total Loss=0.2493, Eig Loss=0.0019, Orth Loss=24.7367\n",
      "Epoch 15000, LR=0.005012, Total Loss=0.2564, Eig Loss=0.0102, Orth Loss=24.6247\n",
      "Epoch 15500, LR=0.004898, Total Loss=0.2688, Eig Loss=0.0234, Orth Loss=24.5410\n",
      "Epoch 16000, LR=0.004786, Total Loss=0.2519, Eig Loss=0.0069, Orth Loss=24.5019\n",
      "Epoch 16500, LR=0.004677, Total Loss=0.2479, Eig Loss=0.0035, Orth Loss=24.4431\n",
      "Epoch 17000, LR=0.004571, Total Loss=0.2541, Eig Loss=0.0102, Orth Loss=24.3885\n",
      "Epoch 17500, LR=0.004467, Total Loss=0.2533, Eig Loss=0.0101, Orth Loss=24.3175\n",
      "Epoch 18000, LR=0.004365, Total Loss=0.2467, Eig Loss=0.0038, Orth Loss=24.2872\n",
      "Epoch 18500, LR=0.004266, Total Loss=0.2528, Eig Loss=0.0108, Orth Loss=24.1999\n",
      "Epoch 19000, LR=0.004169, Total Loss=0.2521, Eig Loss=0.0106, Orth Loss=24.1558\n",
      "Epoch 19500, LR=0.004074, Total Loss=0.2849, Eig Loss=0.0447, Orth Loss=24.0180\n",
      "Epoch 20000, LR=0.003981, Total Loss=0.2412, Eig Loss=0.0018, Orth Loss=23.9431\n",
      "Epoch 20500, LR=0.003890, Total Loss=0.2440, Eig Loss=0.0052, Orth Loss=23.8818\n",
      "Epoch 21000, LR=0.003802, Total Loss=0.2475, Eig Loss=0.0095, Orth Loss=23.8045\n",
      "Epoch 21500, LR=0.003715, Total Loss=0.2450, Eig Loss=0.0078, Orth Loss=23.7205\n",
      "Epoch 22000, LR=0.003631, Total Loss=0.2378, Eig Loss=0.0014, Orth Loss=23.6457\n",
      "Epoch 22500, LR=0.003548, Total Loss=0.2394, Eig Loss=0.0041, Orth Loss=23.5246\n",
      "Epoch 23000, LR=0.003467, Total Loss=0.2353, Eig Loss=0.0003, Orth Loss=23.5032\n",
      "Epoch 23500, LR=0.003388, Total Loss=0.2348, Eig Loss=0.0012, Orth Loss=23.3593\n",
      "Epoch 24000, LR=0.003311, Total Loss=0.2412, Eig Loss=0.0085, Orth Loss=23.2720\n",
      "Epoch 24500, LR=0.003236, Total Loss=0.2722, Eig Loss=0.0405, Orth Loss=23.1784\n",
      "Epoch 25000, LR=0.003162, Total Loss=0.2326, Eig Loss=0.0003, Orth Loss=23.2296\n",
      "Epoch 25500, LR=0.003090, Total Loss=0.2327, Eig Loss=0.0025, Orth Loss=23.0258\n",
      "Epoch 26000, LR=0.003020, Total Loss=0.2344, Eig Loss=0.0001, Orth Loss=23.4301\n",
      "Epoch 26500, LR=0.002951, Total Loss=0.2315, Eig Loss=0.0010, Orth Loss=23.0465\n",
      "Epoch 27000, LR=0.002884, Total Loss=0.2307, Eig Loss=0.0004, Orth Loss=23.0302\n",
      "Epoch 27500, LR=0.002818, Total Loss=0.2372, Eig Loss=0.0081, Orth Loss=22.9086\n",
      "Epoch 28000, LR=0.002754, Total Loss=0.2293, Eig Loss=0.0010, Orth Loss=22.8245\n",
      "Epoch 28500, LR=0.002692, Total Loss=0.2302, Eig Loss=0.0002, Orth Loss=23.0026\n",
      "Epoch 29000, LR=0.002630, Total Loss=0.2299, Eig Loss=0.0017, Orth Loss=22.8205\n",
      "Epoch 29500, LR=0.002570, Total Loss=0.2348, Eig Loss=0.0073, Orth Loss=22.7583\n",
      "Epoch 30000, LR=0.002512, Total Loss=0.2277, Eig Loss=0.0006, Orth Loss=22.7086\n",
      "Epoch 30500, LR=0.002455, Total Loss=0.2272, Eig Loss=0.0003, Orth Loss=22.6860\n",
      "Epoch 31000, LR=0.002399, Total Loss=0.2287, Eig Loss=0.0030, Orth Loss=22.5682\n",
      "Epoch 31500, LR=0.002344, Total Loss=0.2259, Eig Loss=0.0002, Orth Loss=22.5711\n",
      "Epoch 32000, LR=0.002291, Total Loss=0.2263, Eig Loss=0.0017, Orth Loss=22.4590\n",
      "Epoch 32500, LR=0.002239, Total Loss=0.2288, Eig Loss=0.0052, Orth Loss=22.3627\n",
      "Epoch 33000, LR=0.002188, Total Loss=0.2234, Eig Loss=0.0004, Orth Loss=22.2980\n",
      "Epoch 33500, LR=0.002138, Total Loss=0.2225, Eig Loss=0.0003, Orth Loss=22.2219\n",
      "Epoch 34000, LR=0.002089, Total Loss=0.2227, Eig Loss=0.0011, Orth Loss=22.1534\n",
      "Epoch 34500, LR=0.002042, Total Loss=0.2221, Eig Loss=0.0012, Orth Loss=22.0834\n",
      "Epoch 35000, LR=0.001995, Total Loss=0.2215, Eig Loss=0.0015, Orth Loss=22.0003\n",
      "Epoch 35500, LR=0.001950, Total Loss=0.2268, Eig Loss=0.0076, Orth Loss=21.9159\n",
      "Epoch 36000, LR=0.001905, Total Loss=0.2219, Eig Loss=0.0036, Orth Loss=21.8261\n",
      "Epoch 36500, LR=0.001862, Total Loss=0.2213, Eig Loss=0.0037, Orth Loss=21.7658\n",
      "Epoch 37000, LR=0.001820, Total Loss=0.2178, Eig Loss=0.0012, Orth Loss=21.6605\n",
      "Epoch 37500, LR=0.001778, Total Loss=0.2163, Eig Loss=0.0003, Orth Loss=21.5967\n",
      "Epoch 38000, LR=0.001738, Total Loss=0.2176, Eig Loss=0.0025, Orth Loss=21.5087\n",
      "Epoch 38500, LR=0.001698, Total Loss=0.2202, Eig Loss=0.0060, Orth Loss=21.4211\n",
      "Epoch 39000, LR=0.001660, Total Loss=0.2140, Eig Loss=0.0005, Orth Loss=21.3510\n",
      "Epoch 39500, LR=0.001622, Total Loss=0.2135, Eig Loss=0.0009, Orth Loss=21.2584\n",
      "Epoch 40000, LR=0.001585, Total Loss=0.2127, Eig Loss=0.0012, Orth Loss=21.1481\n",
      "Epoch 40500, LR=0.001549, Total Loss=0.2104, Eig Loss=0.0002, Orth Loss=21.0185\n",
      "Epoch 41000, LR=0.001514, Total Loss=0.2112, Eig Loss=0.0025, Orth Loss=20.8793\n",
      "Epoch 41500, LR=0.001479, Total Loss=0.2092, Eig Loss=0.0013, Orth Loss=20.7869\n",
      "Epoch 42000, LR=0.001445, Total Loss=0.2387, Eig Loss=0.0313, Orth Loss=20.7400\n",
      "Epoch 42500, LR=0.001413, Total Loss=0.2167, Eig Loss=0.0100, Orth Loss=20.6625\n",
      "Epoch 43000, LR=0.001380, Total Loss=0.2064, Eig Loss=0.0003, Orth Loss=20.6089\n",
      "Epoch 43500, LR=0.001349, Total Loss=0.2094, Eig Loss=0.0040, Orth Loss=20.5397\n",
      "Epoch 44000, LR=0.001318, Total Loss=0.2102, Eig Loss=0.0053, Orth Loss=20.4871\n",
      "Epoch 44500, LR=0.001288, Total Loss=0.2048, Eig Loss=0.0006, Orth Loss=20.4225\n",
      "Epoch 45000, LR=0.001259, Total Loss=0.2080, Eig Loss=0.0044, Orth Loss=20.3609\n",
      "Epoch 45500, LR=0.001230, Total Loss=0.2058, Eig Loss=0.0030, Orth Loss=20.2735\n",
      "Epoch 46000, LR=0.001202, Total Loss=0.2046, Eig Loss=0.0026, Orth Loss=20.1993\n",
      "Epoch 46500, LR=0.001175, Total Loss=0.2026, Eig Loss=0.0014, Orth Loss=20.1203\n",
      "Epoch 47000, LR=0.001148, Total Loss=0.2056, Eig Loss=0.0052, Orth Loss=20.0406\n",
      "Epoch 47500, LR=0.001122, Total Loss=0.2019, Eig Loss=0.0023, Orth Loss=19.9654\n",
      "Epoch 48000, LR=0.001096, Total Loss=0.1993, Eig Loss=0.0004, Orth Loss=19.8968\n",
      "Epoch 48500, LR=0.001072, Total Loss=0.1998, Eig Loss=0.0014, Orth Loss=19.8413\n",
      "Epoch 49000, LR=0.001047, Total Loss=0.2001, Eig Loss=0.0021, Orth Loss=19.7987\n",
      "Epoch 49500, LR=0.001023, Total Loss=0.2044, Eig Loss=0.0069, Orth Loss=19.7539\n",
      "Epoch 50000, LR=0.001000, Total Loss=0.2043, Eig Loss=0.0071, Orth Loss=19.7215\n",
      "Epoch 50500, LR=0.000977, Total Loss=0.1972, Eig Loss=0.0003, Orth Loss=19.6922\n",
      "Epoch 51000, LR=0.000955, Total Loss=0.1971, Eig Loss=0.0005, Orth Loss=19.6641\n",
      "Epoch 51500, LR=0.000933, Total Loss=0.1973, Eig Loss=0.0010, Orth Loss=19.6357\n",
      "Epoch 52000, LR=0.000912, Total Loss=0.1966, Eig Loss=0.0004, Orth Loss=19.6144\n",
      "Epoch 52500, LR=0.000891, Total Loss=0.1962, Eig Loss=0.0003, Orth Loss=19.5918\n",
      "Epoch 53000, LR=0.000871, Total Loss=0.1976, Eig Loss=0.0020, Orth Loss=19.5685\n",
      "Epoch 53500, LR=0.000851, Total Loss=0.2137, Eig Loss=0.0182, Orth Loss=19.5493\n",
      "Epoch 54000, LR=0.000832, Total Loss=0.1955, Eig Loss=0.0002, Orth Loss=19.5253\n",
      "Epoch 54500, LR=0.000813, Total Loss=0.1955, Eig Loss=0.0004, Orth Loss=19.5052\n",
      "Epoch 55000, LR=0.000794, Total Loss=0.2066, Eig Loss=0.0118, Orth Loss=19.4801\n",
      "Epoch 55500, LR=0.000776, Total Loss=0.1949, Eig Loss=0.0002, Orth Loss=19.4634\n",
      "Epoch 56000, LR=0.000759, Total Loss=0.1954, Eig Loss=0.0010, Orth Loss=19.4427\n",
      "Epoch 56500, LR=0.000741, Total Loss=0.1947, Eig Loss=0.0005, Orth Loss=19.4222\n",
      "Epoch 57000, LR=0.000724, Total Loss=0.2117, Eig Loss=0.0176, Orth Loss=19.4054\n",
      "Epoch 57500, LR=0.000708, Total Loss=0.1941, Eig Loss=0.0003, Orth Loss=19.3803\n",
      "Epoch 58000, LR=0.000692, Total Loss=0.1938, Eig Loss=0.0002, Orth Loss=19.3561\n",
      "Epoch 58500, LR=0.000676, Total Loss=0.1934, Eig Loss=0.0002, Orth Loss=19.3294\n",
      "Epoch 59000, LR=0.000661, Total Loss=0.1942, Eig Loss=0.0011, Orth Loss=19.3020\n",
      "Epoch 59500, LR=0.000646, Total Loss=0.1953, Eig Loss=0.0026, Orth Loss=19.2642\n",
      "Epoch 60000, LR=0.000631, Total Loss=0.1998, Eig Loss=0.0075, Orth Loss=19.2297\n",
      "Epoch 60500, LR=0.000617, Total Loss=0.1919, Eig Loss=0.0001, Orth Loss=19.1807\n",
      "Epoch 61000, LR=0.000603, Total Loss=0.1925, Eig Loss=0.0011, Orth Loss=19.1452\n",
      "Epoch 61500, LR=0.000589, Total Loss=0.1922, Eig Loss=0.0011, Orth Loss=19.1100\n",
      "Epoch 62000, LR=0.000575, Total Loss=0.1910, Eig Loss=0.0002, Orth Loss=19.0806\n",
      "Epoch 62500, LR=0.000562, Total Loss=0.1907, Eig Loss=0.0002, Orth Loss=19.0493\n",
      "Epoch 63000, LR=0.000550, Total Loss=0.1903, Eig Loss=0.0001, Orth Loss=19.0173\n",
      "Epoch 63500, LR=0.000537, Total Loss=0.1903, Eig Loss=0.0004, Orth Loss=18.9857\n",
      "Epoch 64000, LR=0.000525, Total Loss=0.1932, Eig Loss=0.0036, Orth Loss=18.9542\n",
      "Epoch 64500, LR=0.000513, Total Loss=0.1898, Eig Loss=0.0006, Orth Loss=18.9214\n",
      "Epoch 65000, LR=0.000501, Total Loss=0.1891, Eig Loss=0.0001, Orth Loss=18.8909\n",
      "Epoch 65500, LR=0.000490, Total Loss=0.1887, Eig Loss=0.0001, Orth Loss=18.8616\n",
      "Epoch 66000, LR=0.000479, Total Loss=0.1911, Eig Loss=0.0028, Orth Loss=18.8337\n",
      "Epoch 66500, LR=0.000468, Total Loss=0.1884, Eig Loss=0.0003, Orth Loss=18.8081\n",
      "Epoch 67000, LR=0.000457, Total Loss=0.1883, Eig Loss=0.0004, Orth Loss=18.7879\n",
      "Epoch 67500, LR=0.000447, Total Loss=0.1885, Eig Loss=0.0008, Orth Loss=18.7689\n",
      "Epoch 68000, LR=0.000437, Total Loss=0.1879, Eig Loss=0.0003, Orth Loss=18.7536\n",
      "Epoch 68500, LR=0.000427, Total Loss=0.1877, Eig Loss=0.0003, Orth Loss=18.7381\n",
      "Epoch 69000, LR=0.000417, Total Loss=0.1876, Eig Loss=0.0004, Orth Loss=18.7226\n",
      "Epoch 69500, LR=0.000407, Total Loss=0.1872, Eig Loss=0.0001, Orth Loss=18.7081\n",
      "Epoch 70000, LR=0.000398, Total Loss=0.1870, Eig Loss=0.0001, Orth Loss=18.6922\n",
      "Epoch 70500, LR=0.000389, Total Loss=0.1870, Eig Loss=0.0003, Orth Loss=18.6737\n",
      "Epoch 71000, LR=0.000380, Total Loss=0.1866, Eig Loss=0.0001, Orth Loss=18.6507\n",
      "Epoch 71500, LR=0.000372, Total Loss=0.1889, Eig Loss=0.0027, Orth Loss=18.6224\n",
      "Epoch 72000, LR=0.000363, Total Loss=0.1861, Eig Loss=0.0001, Orth Loss=18.5978\n",
      "Epoch 72500, LR=0.000355, Total Loss=0.1858, Eig Loss=0.0001, Orth Loss=18.5734\n",
      "Epoch 73000, LR=0.000347, Total Loss=0.1857, Eig Loss=0.0001, Orth Loss=18.5513\n",
      "Epoch 73500, LR=0.000339, Total Loss=0.1854, Eig Loss=0.0001, Orth Loss=18.5319\n",
      "Epoch 74000, LR=0.000331, Total Loss=0.1852, Eig Loss=0.0001, Orth Loss=18.5154\n",
      "Epoch 74500, LR=0.000324, Total Loss=0.1853, Eig Loss=0.0003, Orth Loss=18.5038\n",
      "Epoch 75000, LR=0.000316, Total Loss=0.1850, Eig Loss=0.0001, Orth Loss=18.4952\n",
      "Epoch 75500, LR=0.000309, Total Loss=0.1849, Eig Loss=0.0000, Orth Loss=18.4876\n",
      "Epoch 76000, LR=0.000302, Total Loss=0.1849, Eig Loss=0.0001, Orth Loss=18.4808\n",
      "Epoch 76500, LR=0.000295, Total Loss=0.1851, Eig Loss=0.0003, Orth Loss=18.4742\n",
      "Epoch 77000, LR=0.000288, Total Loss=0.1847, Eig Loss=0.0000, Orth Loss=18.4684\n",
      "Epoch 77500, LR=0.000282, Total Loss=0.1852, Eig Loss=0.0006, Orth Loss=18.4621\n",
      "Epoch 78000, LR=0.000275, Total Loss=0.1847, Eig Loss=0.0001, Orth Loss=18.4570\n",
      "Epoch 78500, LR=0.000269, Total Loss=0.1846, Eig Loss=0.0001, Orth Loss=18.4515\n",
      "Epoch 79000, LR=0.000263, Total Loss=0.1848, Eig Loss=0.0003, Orth Loss=18.4462\n",
      "Epoch 79500, LR=0.000257, Total Loss=0.1858, Eig Loss=0.0014, Orth Loss=18.4402\n",
      "Epoch 80000, LR=0.000251, Total Loss=0.1845, Eig Loss=0.0002, Orth Loss=18.4354\n",
      "Epoch 80500, LR=0.000245, Total Loss=0.1843, Eig Loss=0.0000, Orth Loss=18.4299\n",
      "Epoch 81000, LR=0.000240, Total Loss=0.1843, Eig Loss=0.0000, Orth Loss=18.4244\n",
      "Epoch 81500, LR=0.000234, Total Loss=0.1844, Eig Loss=0.0002, Orth Loss=18.4183\n",
      "Epoch 82000, LR=0.000229, Total Loss=0.1842, Eig Loss=0.0000, Orth Loss=18.4120\n",
      "Epoch 82500, LR=0.000224, Total Loss=0.1841, Eig Loss=0.0000, Orth Loss=18.4048\n",
      "Epoch 83000, LR=0.000219, Total Loss=0.1847, Eig Loss=0.0007, Orth Loss=18.3967\n",
      "Epoch 83500, LR=0.000214, Total Loss=0.1840, Eig Loss=0.0002, Orth Loss=18.3871\n",
      "Epoch 84000, LR=0.000209, Total Loss=0.1839, Eig Loss=0.0002, Orth Loss=18.3753\n",
      "Epoch 84500, LR=0.000204, Total Loss=0.1838, Eig Loss=0.0002, Orth Loss=18.3615\n",
      "Epoch 85000, LR=0.000200, Total Loss=0.1844, Eig Loss=0.0009, Orth Loss=18.3480\n",
      "Epoch 85500, LR=0.000195, Total Loss=0.1835, Eig Loss=0.0001, Orth Loss=18.3379\n",
      "Epoch 86000, LR=0.000191, Total Loss=0.1833, Eig Loss=0.0000, Orth Loss=18.3297\n",
      "Epoch 86500, LR=0.000186, Total Loss=0.1834, Eig Loss=0.0002, Orth Loss=18.3235\n",
      "Epoch 87000, LR=0.000182, Total Loss=0.1833, Eig Loss=0.0001, Orth Loss=18.3181\n",
      "Epoch 87500, LR=0.000178, Total Loss=0.1832, Eig Loss=0.0001, Orth Loss=18.3133\n",
      "Epoch 88000, LR=0.000174, Total Loss=0.1831, Eig Loss=0.0000, Orth Loss=18.3092\n",
      "Epoch 88500, LR=0.000170, Total Loss=0.1831, Eig Loss=0.0000, Orth Loss=18.3054\n",
      "Epoch 89000, LR=0.000166, Total Loss=0.1831, Eig Loss=0.0000, Orth Loss=18.3017\n",
      "Epoch 89500, LR=0.000162, Total Loss=0.1831, Eig Loss=0.0001, Orth Loss=18.2985\n",
      "Epoch 90000, LR=0.000158, Total Loss=0.1832, Eig Loss=0.0003, Orth Loss=18.2953\n",
      "Epoch 90500, LR=0.000155, Total Loss=0.1830, Eig Loss=0.0001, Orth Loss=18.2923\n",
      "Epoch 91000, LR=0.000151, Total Loss=0.1831, Eig Loss=0.0003, Orth Loss=18.2892\n",
      "Epoch 91500, LR=0.000148, Total Loss=0.1829, Eig Loss=0.0001, Orth Loss=18.2864\n",
      "Epoch 92000, LR=0.000145, Total Loss=0.1830, Eig Loss=0.0001, Orth Loss=18.2836\n",
      "Epoch 92500, LR=0.000141, Total Loss=0.1829, Eig Loss=0.0001, Orth Loss=18.2809\n",
      "Epoch 93000, LR=0.000138, Total Loss=0.1828, Eig Loss=0.0000, Orth Loss=18.2783\n",
      "Epoch 93500, LR=0.000135, Total Loss=0.1829, Eig Loss=0.0002, Orth Loss=18.2760\n",
      "Epoch 94000, LR=0.000132, Total Loss=0.1828, Eig Loss=0.0001, Orth Loss=18.2729\n",
      "Epoch 94500, LR=0.000129, Total Loss=0.1827, Eig Loss=0.0000, Orth Loss=18.2705\n",
      "Epoch 95000, LR=0.000126, Total Loss=0.1829, Eig Loss=0.0002, Orth Loss=18.2678\n",
      "Epoch 95500, LR=0.000123, Total Loss=0.1831, Eig Loss=0.0004, Orth Loss=18.2654\n",
      "Epoch 96000, LR=0.000120, Total Loss=0.1826, Eig Loss=0.0000, Orth Loss=18.2629\n",
      "Epoch 96500, LR=0.000117, Total Loss=0.1831, Eig Loss=0.0005, Orth Loss=18.2606\n",
      "Epoch 97000, LR=0.000115, Total Loss=0.1826, Eig Loss=0.0000, Orth Loss=18.2580\n",
      "Epoch 97500, LR=0.000112, Total Loss=0.1827, Eig Loss=0.0002, Orth Loss=18.2558\n",
      "Epoch 98000, LR=0.000110, Total Loss=0.1826, Eig Loss=0.0000, Orth Loss=18.2531\n",
      "Epoch 98500, LR=0.000107, Total Loss=0.1825, Eig Loss=0.0000, Orth Loss=18.2506\n",
      "Epoch 99000, LR=0.000105, Total Loss=0.1825, Eig Loss=0.0000, Orth Loss=18.2482\n",
      "Epoch 99500, LR=0.000102, Total Loss=0.1825, Eig Loss=0.0000, Orth Loss=18.2457\n",
      "Epoch 100000, LR=0.000100, Total Loss=0.1825, Eig Loss=0.0001, Orth Loss=18.2433\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron for mapping coordinates to k eigenmodes.\n",
    "    Uses SiLU (Swish) activation for better gradient flow than Tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64, 64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            # Using nn.SiLU (Swish) instead of nn.Tanh\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# --- 3. Model Instantiation and Initialization ---\n",
    "\n",
    "# Instantiate model\n",
    "model = MLP().to(device)\n",
    "\n",
    "# Initialize all layers (Xavier), final layer small (Best practice for PINNs)\n",
    "for name, p in model.named_parameters():\n",
    "    if 'net' in name:\n",
    "        # Standard Xavier for hidden layers (weights)\n",
    "        if p.dim() > 1 and name.split('.')[1] != str(len(model.net) - 1):\n",
    "            nn.init.xavier_uniform_(p.data)\n",
    "        # Final Linear layer: Small weights and zero bias\n",
    "        if name.split('.')[1] == str(len(model.net) - 1):\n",
    "            if p.ndim == 2:\n",
    "                # Weights: Very small normal distribution\n",
    "                nn.init.normal_(p.data, std=1e-3)\n",
    "            else:\n",
    "                # Biases: Zero\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "# --- 4. Training Setup ---\n",
    "\n",
    "# Hyperparameters\n",
    "# CRITICAL FIX: Since we are using M-orthogonalization (SVD projection), \n",
    "# lambda_orth can be set very low, focusing the optimizer on eig_loss.\n",
    "lambda_orth = 0.01         \n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "max_epochs = 100_000 # Reverting to a more manageable epoch count for testing         \n",
    "print_every = 500\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_start)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "# --- 5. Training Loop ---\n",
    "\n",
    "print(\"\\n--- Starting Full-Batch Training with SVD Stabilization ---\")\n",
    "identity_k = torch.eye(k, device=device)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward Pass (Full Batch)\n",
    "    U = model(X)  # N x k (Basis functions evaluated at coordinates X)\n",
    "    \n",
    "    # --- M-Orthogonalization via SVD ---\n",
    "    B = U.T @ (M @ U)        # k x k \n",
    "    V, S, _ = torch.linalg.svd(B)\n",
    "    S_inv_sqrt = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S, min=1e-8)))\n",
    "    B_inv_sqrt = V @ S_inv_sqrt @ V.T\n",
    "    U_orth = U @ B_inv_sqrt\n",
    "    \n",
    "    # --- Rayleigh matrix ---\n",
    "    R = U_orth.T @ (K @ U_orth)   # k x k\n",
    "\n",
    "    # --- Eigenvalue Loss (unsupervised) ---\n",
    "    # Option A: Penalize off-diagonals only\n",
    "    eig_loss = torch.norm(R - torch.diag(torch.diag(R)), p='fro')**2\n",
    "\n",
    "    # Option B: Eigen-equation residual (alternative form, comment/uncomment)\n",
    "    # Lambda = torch.diag(torch.diag(R))\n",
    "    # residual = K @ U_orth - (M @ U_orth) @ Lambda\n",
    "    # eig_loss = torch.norm(residual, p='fro')**2 / k\n",
    "\n",
    "    # --- Orthogonality Loss ---\n",
    "    B_orth = U_orth.T @ (M @ U_orth)        \n",
    "    orth_loss = torch.norm(B_orth - identity_k, p='fro')**2\n",
    "\n",
    "    # --- Total Loss ---\n",
    "    loss = eig_loss + lambda_orth * orth_loss\n",
    "\n",
    "    # Backpropagation and Step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Logging\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eigvals = torch.diag(R).cpu().numpy()   # read off eigenvalues\n",
    "            eigvals.sort()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch:<5}, LR={current_lr:.6f}, \"\n",
    "            f\"Total Loss={loss.item():.4f}, \"\n",
    "            f\"Eig Loss={eig_loss.item():.4f}, \"\n",
    "            f\"Orth Loss={orth_loss.item():.4f}\"\n",
    "        )\n",
    "        # print(f\"  Approx Eigenvalues: {eigvals[:5]}\")\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9efb511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results ---\n",
      "Final Approximate Eigenvalues (Sorted): ['0.000036', '0.000038', '0.000038', '0.000038', '0.000039']\n",
      "Reference eigenvalues (first k): ['0.000036', '0.000038', '0.000038', '0.000038', '0.000039']\n",
      "\n",
      "Final Orthogonality Matrix (U_orth^T M U_orth):\n",
      "[[ 9.707e-01 -3.700e-03  2.000e-04 ... -3.300e-03  1.200e-03  0.000e+00]\n",
      " [-3.700e-03  9.687e-01 -1.000e-03 ... -7.100e-03 -2.700e-03 -0.000e+00]\n",
      " [ 2.000e-04 -1.000e-03  9.991e-01 ... -1.500e-03 -0.000e+00  1.000e-04]\n",
      " ...\n",
      " [-3.300e-03 -7.100e-03 -1.500e-03 ...  9.837e-01  7.000e-04  0.000e+00]\n",
      " [ 1.200e-03 -2.700e-03 -0.000e+00 ...  7.000e-04  9.949e-01  1.000e-04]\n",
      " [ 0.000e+00 -0.000e+00  1.000e-04 ...  0.000e+00  1.000e-04  5.680e-02]]\n"
     ]
    }
   ],
   "source": [
    "# --- Final Eigenvalue Check ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    B_final = U_final.T @ (M @ U_final)\n",
    "    V_final, S_final, _ = torch.linalg.svd(B_final)\n",
    "    S_inv_sqrt_final = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S_final, min=1e-8)))\n",
    "    B_inv_sqrt_final = V_final @ S_inv_sqrt_final @ V_final.T\n",
    "    U_orth_final = U_final @ B_inv_sqrt_final\n",
    "    \n",
    "    R_final = U_orth_final.T @ (K @ U_orth_final)\n",
    "    final_eigenvalues = torch.diag(R_final).cpu().numpy()\n",
    "    final_eigenvalues.sort()\n",
    "    final_ortho_matrix = U_orth_final.T @ (M @ U_orth_final)\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(\"Final Approximate Eigenvalues (Sorted):\", \n",
    "      [f\"{val:.6f}\" for val in final_eigenvalues[:5]])\n",
    "\n",
    "    print(\"Reference eigenvalues (first k):\", \n",
    "        [f\"{val:.6f}\" for val in eigvals[:5]])\n",
    "\n",
    "    print(\"\\nFinal Orthogonality Matrix (U_orth^T M U_orth):\")\n",
    "    print(final_ortho_matrix.cpu().numpy().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b58b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49646f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd5eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361864d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9ec7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cc0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f700b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7809a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285b748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939688e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d107f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4090df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccd66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
