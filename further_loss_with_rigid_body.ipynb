{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdee03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "N = 1546, k = 50, ratio = 30.9\n",
      "Condition number of K: 2.56e+16\n",
      "Condition number of M: 3.70e+02\n",
      "Target eigenvalue range: [0.0000, 7.8346]\n",
      "WARNING: Large eigenvalue spread, consider normalization\n",
      "\n",
      "Using epsilon=0.0001, final condition number: 1.47e+05\n",
      "\n",
      "=== Matrix Normalization ===\n",
      "K Frobenius norm (normalized): 1.0000\n",
      "M Frobenius norm (normalized): 1.0000\n",
      "Normalization: K_scale=1.58e+02, M_scale=1.22e+00\n",
      "Target eigenvalue range (original): [0.000000, 7.834566]\n",
      "\n",
      "=== Matrix Diagnostics ===\n",
      "K has NaN: False\n",
      "K has Inf: False\n",
      "K is symmetric: True\n",
      "M is positive definite: True\n",
      "||K*v|| / ||v|| = 4.05e+00\n",
      "\n",
      "K_reg diagonal stats:\n",
      "  min: 2.18e+00, max: 7.60e+00\n",
      "  mean: 3.63e+00, std: 4.33e-01\n",
      "  negative entries: 0/1546\n",
      "\n",
      "M diagonal stats:\n",
      "  min: 1.80e-04, max: 4.15e-02\n",
      "  mean: 2.29e-02, std: 5.67e-03\n",
      "\n",
      "Matrix norms:\n",
      "  ||K_reg||_F: 1.58e+02\n",
      "  ||M||_F: 1.00e+00\n",
      "\n",
      "Condition number estimates:\n",
      "  K_reg: 1.47e+05\n",
      "  M: 3.70e+02\n",
      "\n",
      "Reference eigenvalues:\n",
      "  First 10: [3.50308915e-13 7.57414444e-03 3.03079128e-02 6.81464805e-02\n",
      " 1.21207968e-01 1.89242748e-01 2.72231499e-01 3.70535951e-01\n",
      " 4.83409417e-01 6.11342543e-01]\n",
      "  Last 10 of k=50: [7.42219306 7.44948415 7.45560059 7.5081038  7.52679082 7.60739344\n",
      " 7.61427628 7.70810371 7.72485585 7.8345656 ]\n",
      "  Range: [0.000000, 7.834566]\n",
      "\n",
      "============================================================\n",
      "\n",
      "Model: 106,162 parameters\n",
      "\n",
      "=== Starting Training ===\n",
      "Epochs: 300,000 | Print every: 2,000\n",
      "\n",
      "Epoch      1 | LR=0.050000 | λ_orth=1.00 | Loss=47.575862\n",
      "           | λ₁²=2.24e-09 | λ₁=0.000047 | Eig=0.001491 (trace:0.000, div:0.0001)\n",
      "           | Orth=4.67e+01 | Order=0.000000 | Stab=8.980008 | SVD: σ_max/σ_min=9.98e+03\n",
      "           | MeanRelErr=1225835.1211 | λ∈[0.0062, 0.1017]\n",
      "           | λ_spread=0.0955\n",
      "\n",
      "Epoch   2000 | LR=0.049455 | λ_orth=1.00 | Loss=0.168704\n",
      "           | λ₁²=5.07e-07 | λ₁=0.000712 | Eig=0.168704 (trace:0.034, div:0.0000)\n",
      "           | Orth=2.28e-26 | Order=0.000000 | Stab=0.000000 | SVD: σ_max/σ_min=6.34e+02\n",
      "           | MeanRelErr=18422242.0693 | λ∈[0.0924, 5.6387]\n",
      "           | λ_spread=5.5463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)\n",
    "\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "\n",
    "\n",
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50\n",
    "\n",
    "N = X.shape[0]\n",
    "\n",
    "# Before training, verify:\n",
    "print(f\"N = {N}, k = {k}, ratio = {N/k:.1f}\")\n",
    "print(f\"Condition number of K: {torch.linalg.cond(K).item():.2e}\")\n",
    "print(f\"Condition number of M: {torch.linalg.cond(M).item():.2e}\")\n",
    "print(f\"Target eigenvalue range: [{eigvals[0]:.4f}, {eigvals[k-1]:.4f}]\")\n",
    "\n",
    "# If eigenvalues span many orders of magnitude, normalize:\n",
    "if eigvals[k-1] / eigvals[0] > 100:\n",
    "    print(\"WARNING: Large eigenvalue spread, consider normalization\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# --- CRITICAL: Fix the ill-conditioning ---\n",
    "\n",
    "# 1. Start with stronger regularization\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Apply the regularization\n",
    "K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "print(f\"\\nUsing epsilon={epsilon}, final condition number: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "\n",
    "print(\"\\n=== Matrix Normalization ===\")\n",
    "K_scale = torch.norm(K_reg, p='fro')\n",
    "M_scale = torch.norm(M, p='fro')\n",
    "\n",
    "K = K_reg / K_scale\n",
    "M = M / M_scale\n",
    "\n",
    "print(f\"K Frobenius norm (normalized): {torch.norm(K, p='fro').item():.4f}\")\n",
    "print(f\"M Frobenius norm (normalized): {torch.norm(M, p='fro').item():.4f}\")\n",
    "print(f\"Normalization: K_scale={K_scale.item():.2e}, M_scale={M_scale.item():.2e}\")\n",
    "\n",
    "# Keep reference eigenvalues as-is (original scale)\n",
    "# We'll scale the predicted eigenvalues instead when comparing\n",
    "print(f\"Target eigenvalue range (original): [{eigvals[0]:.6f}, {eigvals[k-1]:.6f}]\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Matrix Diagnostics ===\")\n",
    "\n",
    "# Basic checks\n",
    "print(f\"K has NaN: {torch.isnan(K_reg).any()}\")\n",
    "print(f\"K has Inf: {torch.isinf(K_reg).any()}\")\n",
    "print(f\"K is symmetric: {torch.allclose(K_reg, K_reg.T, atol=1e-6)}\")\n",
    "print(f\"M is positive definite: {torch.all(torch.linalg.eigvalsh(M) > 0)}\")\n",
    "\n",
    "# Matrix-vector multiply check\n",
    "v = torch.randn(N, device=device, dtype=torch.float64)\n",
    "Kv = K_reg @ v\n",
    "print(f\"||K*v|| / ||v|| = {torch.norm(Kv) / torch.norm(v):.2e}\")\n",
    "\n",
    "# Additional diagnostics\n",
    "print(f\"\\nK_reg diagonal stats:\")\n",
    "K_diag = torch.diag(K_reg)\n",
    "print(f\"  min: {K_diag.min():.2e}, max: {K_diag.max():.2e}\")\n",
    "print(f\"  mean: {K_diag.mean():.2e}, std: {K_diag.std():.2e}\")\n",
    "print(f\"  negative entries: {(K_diag < 0).sum().item()}/{N}\")\n",
    "\n",
    "print(f\"\\nM diagonal stats:\")\n",
    "M_diag = torch.diag(M)\n",
    "print(f\"  min: {M_diag.min():.2e}, max: {M_diag.max():.2e}\")\n",
    "print(f\"  mean: {M_diag.mean():.2e}, std: {M_diag.std():.2e}\")\n",
    "\n",
    "print(f\"\\nMatrix norms:\")\n",
    "print(f\"  ||K_reg||_F: {torch.norm(K_reg, p='fro'):.2e}\")\n",
    "print(f\"  ||M||_F: {torch.norm(M, p='fro'):.2e}\")\n",
    "\n",
    "# Estimate condition numbers (cheap approximation)\n",
    "print(f\"\\nCondition number estimates:\")\n",
    "print(f\"  K_reg: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "print(f\"  M: {torch.linalg.cond(M).item():.2e}\")\n",
    "\n",
    "# Reference eigenvalues\n",
    "print(f\"\\nReference eigenvalues:\")\n",
    "print(f\"  First 10: {eigvals[:10]}\")\n",
    "print(f\"  Last 10 of k=50: {eigvals[k-10:k]}\")\n",
    "print(f\"  Range: [{eigvals[0]:.6f}, {eigvals[k-1]:.6f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP for approximating k=50 eigenmodes.\n",
    "    Uses 3-layer architecture with increased capacity for k=50.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=50, hidden=[256, 256, 128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "model = MLP().double().to(device)\n",
    "\n",
    "# Xavier initialization for hidden layers, small weights for output layer\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        layer_idx = int(name.split('.')[1])\n",
    "        if layer_idx < len(model.net) - 1:  # Hidden layers\n",
    "            nn.init.xavier_uniform_(p.data)\n",
    "        else:  # Final layer - very small initialization\n",
    "            nn.init.normal_(p.data, std=1e-4)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.zeros_(p.data)\n",
    "\n",
    "print(f\"\\nModel: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Dynamic orthogonality weight scheduling\n",
    "def get_lambda_orth(epoch):\n",
    "    if epoch < 50000:\n",
    "        return 1.0      # Strong constraint initially\n",
    "    elif epoch < 100000:\n",
    "        return 0.1      # Relax after orthogonality established\n",
    "    else:\n",
    "        return 0.01     # Focus on eigenvalues in final phase\n",
    "\n",
    "# Learning rate and optimizer\n",
    "lr_start = 0.05\n",
    "lr_end = 0.0001\n",
    "max_epochs = 300_000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr_start, weight_decay=1e-5)\n",
    "\n",
    "# Cosine annealing with warm restarts for better convergence\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=30000, T_mult=2, eta_min=lr_end\n",
    ")\n",
    "\n",
    "# Logging\n",
    "print_every = 2000\n",
    "loss_history = []\n",
    "identity_k = torch.eye(k, device=device, dtype=torch.float64)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "print(f\"Epochs: {max_epochs:,} | Print every: {print_every:,}\\n\")\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- Forward Pass ---\n",
    "    U = model(X)  # (N, k) - basis functions at all points\n",
    "    \n",
    "    # --- M-Orthogonalization via SVD (differentiable, stable) ---\n",
    "    # This projects U onto M-orthonormal subspace\n",
    "    B = U.T @ (M @ U)  # (k, k) - Gram matrix\n",
    "    V, S, _ = torch.linalg.svd(B)  # SVD of small k×k matrix (cheap!)\n",
    "    \n",
    "    # Compute B^(-1/2) for orthonormalization\n",
    "    S_inv_sqrt = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S, min=1e-7)))\n",
    "    B_inv_sqrt = V @ S_inv_sqrt @ V.T\n",
    "    U_orth = U @ B_inv_sqrt  # M-orthonormalized basis\n",
    "    \n",
    "    # --- Loss Computation ---\n",
    "    \n",
    "    # 1. EIGENVALUE LOSS: Find the SMALLEST k eigenvalues\n",
    "    rayleigh_matrix = U_orth.T @ (K @ U_orth)  # (k, k)\n",
    "    eigenvalues_approx = torch.diag(rayleigh_matrix)\n",
    "    \n",
    "    # Sort eigenvalues for diversity penalty\n",
    "    sorted_eigs, _ = torch.sort(eigenvalues_approx)\n",
    "    \n",
    "    # *** NEW: ZERO EIGENVALUE CONSTRAINT (Rigid body mode) ***\n",
    "    # The first eigenvalue should be exactly zero for Laplacian operators\n",
    "    zero_eig_loss = sorted_eigs[0] ** 2\n",
    "    \n",
    "    # 1a. TRACE LOSS: Minimize sum of eigenvalues (finds smallest modes)\n",
    "    # Exclude first eigenvalue from trace since we're constraining it separately\n",
    "    eig_loss_trace = torch.sum(sorted_eigs[1:]) / (k - 1)\n",
    "    \n",
    "    # 1b. DIVERSITY LOSS: Encourage eigenvalue separation without forcing large gaps\n",
    "    #     Use a soft penalty that encourages spread without hard constraints\n",
    "    #     Method: Penalize if eigenvalues are TOO CLOSE (within 1e-4)\n",
    "    gaps = sorted_eigs[1:] - sorted_eigs[:-1]\n",
    "    min_gap = 1e-4  # Minimum separation in normalized units (very small)\n",
    "    diversity_loss = torch.sum(torch.relu(min_gap - gaps)) / (k - 1)\n",
    "    \n",
    "    # 1c. Off-diagonal penalty: Force Rayleigh matrix to be diagonal\n",
    "    off_diag_mask = 1 - torch.eye(k, device=device, dtype=torch.float64)\n",
    "    eig_loss_offdiag = torch.sum((rayleigh_matrix * off_diag_mask)**2) / (k * (k-1))\n",
    "    \n",
    "    # Combined: Balance all objectives\n",
    "    # *** MODIFIED: Add zero eigenvalue loss with strong weight ***\n",
    "    lambda_zero = 100.0  # Strong constraint on first eigenvalue\n",
    "    eig_loss = lambda_zero * zero_eig_loss + 5.0 * eig_loss_trace + 2.0 * diversity_loss + eig_loss_offdiag\n",
    "    \n",
    "    # 2. ORTHOGONALITY LOSS: Residual check (should be ~0 due to SVD)\n",
    "    B_orth = U_orth.T @ (M @ U_orth)\n",
    "    orth_loss = torch.norm(B_orth - identity_k, p='fro')**2\n",
    "    \n",
    "    # 3. ORDERING LOSS: Encourage λ_1 ≤ λ_2 ≤ ... ≤ λ_k\n",
    "    #    (Already sorted above, so this just penalizes inversions)\n",
    "    ordering_loss = torch.sum(torch.relu(sorted_eigs[:-1] - sorted_eigs[1:])) / k\n",
    "    \n",
    "    # 4. STABILITY: Penalize if B becomes ill-conditioned\n",
    "    # If singular values of B vary too much, SVD orthogonalization becomes unstable\n",
    "    S_ratio = S.max() / (S.min() + 1e-10)\n",
    "    stability_loss = torch.relu(S_ratio - 1e3) / 1e3  # Penalize if condition number > 1000\n",
    "    \n",
    "    # Dynamic weighting\n",
    "    lambda_orth = get_lambda_orth(epoch)\n",
    "    lambda_order = 0.05\n",
    "    lambda_stability = 0.1\n",
    "    \n",
    "    # Total Loss\n",
    "    loss = eig_loss + lambda_orth * orth_loss + lambda_order * ordering_loss + lambda_stability * stability_loss\n",
    "    \n",
    "    # --- Backpropagation ---\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping for numerical stability\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get current eigenvalue estimates\n",
    "            approx_eigenvalues = torch.diag(rayleigh_matrix).cpu().numpy()\n",
    "            approx_eigenvalues.sort()\n",
    "            \n",
    "            # Scale back to original units for comparison with reference\n",
    "            approx_eigenvalues_original = approx_eigenvalues * (K_scale / M_scale).cpu().numpy()\n",
    "            \n",
    "            # Compute errors\n",
    "            abs_error = np.abs(approx_eigenvalues_original[:k] - eigvals[:k])\n",
    "            rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "            mean_rel_error = np.mean(rel_error)\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "        print(f\"Epoch {epoch:>6} | LR={current_lr:.6f} | λ_orth={lambda_orth:.2f} | \"\n",
    "              f\"Loss={loss.item():.6f}\")\n",
    "        print(f\"           | λ₁²={zero_eig_loss.item():.2e} | λ₁={sorted_eigs[0].item():.6f} | \"\n",
    "              f\"Eig={eig_loss.item():.6f} (trace:{eig_loss_trace.item():.3f}, \"\n",
    "              f\"div:{diversity_loss.item():.4f})\")\n",
    "        print(f\"           | Orth={orth_loss.item():.2e} | Order={ordering_loss.item():.6f} | \"\n",
    "              f\"Stab={stability_loss.item():.6f} | SVD: σ_max/σ_min={S_ratio.item():.2e}\")\n",
    "        print(f\"           | MeanRelErr={mean_rel_error:.4f} | \"\n",
    "              f\"λ∈[{approx_eigenvalues_original[0]:.4f}, {approx_eigenvalues_original[k-1]:.4f}]\")\n",
    "        print(f\"           | λ_spread={(approx_eigenvalues_original[-1] - approx_eigenvalues_original[0]):.4f}\")\n",
    "        \n",
    "        # Detailed eigenvalue comparison every 10k epochs\n",
    "        if epoch % (print_every * 5) == 0:\n",
    "            print(f\"           | Predicted (first 5): {approx_eigenvalues_original[:5].round(4)}\")\n",
    "            print(f\"           | Reference (first 5): {eigvals[:5].round(4)}\")\n",
    "            print(f\"           | Predicted (last  5): {approx_eigenvalues_original[-5:].round(4)}\")\n",
    "            print(f\"           | Reference (last  5): {eigvals[k-5:k].round(4)}\")\n",
    "        print()\n",
    "\n",
    "print(\"=== Training Complete ===\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U_final = model(X)\n",
    "    \n",
    "    # Final M-orthogonalization\n",
    "    B_final = U_final.T @ (M @ U_final)\n",
    "    V_final, S_final, _ = torch.linalg.svd(B_final)\n",
    "    S_inv_sqrt_final = torch.diag_embed(1.0 / torch.sqrt(torch.clamp(S_final, min=1e-7)))\n",
    "    B_inv_sqrt_final = V_final @ S_inv_sqrt_final @ V_final.T\n",
    "    U_orth_final = U_final @ B_inv_sqrt_final\n",
    "    \n",
    "    # Final matrices\n",
    "    final_rayleigh_matrix = U_orth_final.T @ (K @ U_orth_final)\n",
    "    final_ortho_matrix = U_orth_final.T @ (M @ U_orth_final)\n",
    "    \n",
    "    # Extract and sort eigenvalues\n",
    "    final_eigenvalues_scaled = torch.diag(final_rayleigh_matrix).cpu().numpy()\n",
    "    final_eigenvalues_scaled.sort()\n",
    "    \n",
    "    # Scale back to original units\n",
    "    final_eigenvalues = final_eigenvalues_scaled * (K_scale / M_scale).cpu().numpy()\n",
    "    \n",
    "    # Compute errors\n",
    "    abs_error = np.abs(final_eigenvalues - eigvals[:k])\n",
    "    rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Orthogonality check\n",
    "    ortho_residual = torch.norm(final_ortho_matrix - identity_k, p='fro').item()\n",
    "    ortho_diag = torch.diag(final_ortho_matrix).cpu().numpy()\n",
    "    print(f\"\\nOrthogonality Quality:\")\n",
    "    print(f\"  ||U^T M U - I||_F = {ortho_residual:.2e}\")\n",
    "    print(f\"  Diagonal range: [{ortho_diag.min():.6f}, {ortho_diag.max():.6f}] (should be ~1.0)\")\n",
    "    \n",
    "    # Rayleigh matrix structure\n",
    "    rayleigh_diag = torch.diag(final_rayleigh_matrix).cpu().numpy()\n",
    "    rayleigh_offdiag = (final_rayleigh_matrix - torch.diag(torch.diag(final_rayleigh_matrix))).cpu().numpy()\n",
    "    print(f\"\\nRayleigh Quotient Matrix:\")\n",
    "    print(f\"  Diagonal norm: {np.linalg.norm(rayleigh_diag):.6f}\")\n",
    "    print(f\"  Off-diagonal norm: {np.linalg.norm(rayleigh_offdiag, 'fro'):.6f} (should be small)\")\n",
    "    \n",
    "    # Eigenvalue comparison\n",
    "    print(f\"\\nEigenvalue Comparison (first 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(min(10, k)):\n",
    "        print(f\"{i+1:<6} {final_eigenvalues[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    print(f\"\\nEigenvalue Comparison (last 10 modes):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<12} {'Reference':<12} {'Abs Error':<12} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 66)\n",
    "    for i in range(max(0, k-10), k):\n",
    "        print(f\"{i+1:<6} {final_eigenvalues[i]:<12.6f} {eigvals[i]:<12.6f} \"\n",
    "              f\"{abs_error[i]:<12.6f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics (all {k} modes):\")\n",
    "    print(f\"  Mean Absolute Error:   {np.mean(abs_error):.6f}\")\n",
    "    print(f\"  Mean Relative Error:   {np.mean(rel_error):.4%}\")\n",
    "    print(f\"  Median Relative Error: {np.median(rel_error):.4%}\")\n",
    "    print(f\"  Max Relative Error:    {np.max(rel_error):.4%}\")\n",
    "    print(f\"  Modes with <5% error:  {np.sum(rel_error < 0.05)}/{k}\")\n",
    "    print(f\"  Modes with <10% error: {np.sum(rel_error < 0.10)}/{k}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
