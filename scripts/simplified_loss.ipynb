{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53342ca8-db05-456c-8e2b-5fc44e26698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "Epoch    1, total loss=2.419, loss_1: 0.001, diag loss: 0.139, off diag loss: 2.280\n",
      "Epoch 2500, total loss=0.379, loss_1: 0.018, diag loss: 0.066, off diag loss: 0.295\n",
      "Epoch 5000, total loss=0.327, loss_1: 0.018, diag loss: 0.067, off diag loss: 0.241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    141\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 142\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_1\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, diag loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiag_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, off diag loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moff_diag_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = Mesh('bunny.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)\n",
    "\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "N = X.shape[0]\n",
    "\n",
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 128\n",
    "\n",
    "def newton_schulz_orthogonalize(U, M, num_iters=5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        U: (N, k) tensor, the raw output of the network\n",
    "        M: (N, N) tensor, the mass matrix\n",
    "    Output:\n",
    "        U_orth: (N, k) tensor, such that U_orth.T @ M @ U_orth == Identity\n",
    "    \"\"\"\n",
    "    N, k = U.shape\n",
    "    \n",
    "    # 1. Compute the Gram matrix (Interaction metric)\n",
    "    # Shape: (k, k) - very small!\n",
    "    A = U.T @ M @ U\n",
    "    \n",
    "    # 2. Normalize spectral norm to ensure convergence\n",
    "    # Newton-Schulz only converges if norm(I - A) < 1. \n",
    "    # We divide A by its trace (approximate spectral norm) or a rough scalar estimate.\n",
    "    normA = torch.norm(A, p='fro') # Frobenius norm is a safe upper bound\n",
    "    scaling_factor = torch.sqrt(normA + 1e-6)\n",
    "    \n",
    "    Y = U / scaling_factor\n",
    "    A = A / (scaling_factor**2)\n",
    "    \n",
    "    # 3. Newton-Schulz Iteration\n",
    "    # We want to find X = A^{-1/2} using only matrix mul.\n",
    "    # Then U_orth = U @ X\n",
    "    \n",
    "    # Initialize X as Identity (approx inverse sqrt)\n",
    "    X = torch.eye(k, device=U.device, dtype=U.dtype)\n",
    "    \n",
    "    # Identity matrix for the update rule\n",
    "    Id = torch.eye(k, device=U.device, dtype=U.dtype)\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        # Update rule: X_{new} = 0.5 * X * (3I - A * X^2)\n",
    "        # This drives X to be the inverse square root of A\n",
    "        AX = A @ X\n",
    "        XAX = X @ AX\n",
    "        X = 0.5 * X @ (3 * Id - XAX)\n",
    "        \n",
    "    # 4. Apply the whitening matrix to U\n",
    "    U_orth = Y @ X\n",
    "    \n",
    "    return U_orth\n",
    "\n",
    "# Build the neural network that maps coordinates -> k outputs per node\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64, 64, 64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h, dtype=torch.double))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim, dtype=torch.double))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# Instantiate model and optimizer2503\n",
    "model = MLP().to(device)\n",
    "\n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "max_epochs = 5_000\n",
    "print_every = 1_000\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_start)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "for epoch in range(1, 22001): # , max_epochs+1\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    U = model(X)  # N x k\n",
    "\n",
    "    KU = K @ U\n",
    "    MU = M @ U\n",
    "\n",
    "    UKU = U.T @ KU\n",
    "    UMU = U.T @ MU        # k x k\n",
    "\n",
    "    rayleigh = UKU / (UMU + 1E-6)\n",
    "\n",
    "    loss_1 = torch.mean(torch.norm((KU - torch.diag(rayleigh) * MU )**2))\n",
    "    UMU_I = (UMU - torch.eye(k, device=device))**2\n",
    "    off_diag_loss = torch.max(UMU_I)\n",
    "    diag_loss = torch.mean(UMU_I)\n",
    "    orth_loss = diag_loss + off_diag_loss\n",
    "    loss = loss_1 + orth_loss\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if epoch % 2500 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}, total loss={loss.item():.3f}, loss_1: {loss_1.item():.3f}, diag loss: {diag_loss.item():.3f}, off diag loss: {off_diag_loss.item():.3f}\")\n",
    "\n",
    "    if loss.item() < 1E-6:\n",
    "        break\n",
    "\n",
    "\n",
    "np.set_printoptions(3, suppress=True)\n",
    "print(torch.max(UMU_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43d95a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2389868677623058\n"
     ]
    }
   ],
   "source": [
    "tmp = (UMU - torch.eye(k, device=device))**2\n",
    "tmp = tmp.detach().cpu().numpy()\n",
    "np.set_printoptions(3, suppress=True)\n",
    "# print(UMU.detach().cpu().numpy())\n",
    "print(torch.max(UMU_I).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba30ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "- are the points uniquely distributed --> try DBScan\n",
    "- network oneshot learning all eigenfunctions by the X points\n",
    "- try LFBGS optimizer as the loss is fully convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93cfd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = torch.cat(found_eigenvectors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "011c3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "UKU = U.T @ K @ U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dc17bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMU = U.T @ M @ U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8135b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -0.  0. ...  0.  0.  0.]\n",
      " [-0.  1.  0. ... -0. -0.  0.]\n",
      " [ 0. -0.  1. ...  0.  0. -0.]\n",
      " ...\n",
      " [ 0. -0.  0. ...  1.  0. -0.]\n",
      " [ 0. -0.  0. ...  0.  1.  0.]\n",
      " [ 0.  0. -0. ... -0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(3, suppress=True)\n",
    "print(UMU.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e5b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1558c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6d973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    " 5 x 2503 @ 2503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "M.cpu().numpy() @ eigvecs[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvecs.T @ M.cpu().numpy() @ eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b02d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
