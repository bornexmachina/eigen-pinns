{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh...\n",
      "Computing Laplacian...\n",
      "Computing eigenvalues (reference)...\n",
      "\n",
      "=== Matrix Conditioning ===\n",
      "Regularization: ε=0.0001\n",
      "Condition number: 1.47e+05\n",
      "Scaling factor: 1.58e+02\n",
      "\n",
      "=== Initializing Model ===\n",
      "Parameters: 128,821\n",
      "Fourier features: True\n",
      "Architecture: [256, 256, 128, 128] -> 50\n",
      "\n",
      "=== Pre-training Initialization ===\n",
      "Pre-train epoch 0: loss=0.012271\n",
      "Pre-train epoch 100: loss=0.000615\n",
      "Pre-train epoch 200: loss=0.000159\n",
      "Pre-train epoch 300: loss=0.000093\n",
      "Pre-train epoch 400: loss=0.000073\n",
      "Pre-training complete!\n",
      "\n",
      "================================================================================\n",
      "TRAINING START\n",
      "================================================================================\n",
      "Epochs: 150,000 | Device: cuda\n",
      "Stage 1: 0-50,000 (orthogonality focus)\n",
      "Stage 2: 50,000-150,000 (eigenvalue tuning)\n",
      "================================================================================\n",
      "\n",
      "[Stage1] Epoch      1 | LR=0.010000\n",
      "  Loss=498.846681 | Trace=1.397 | MSE=0.000000\n",
      "  Orth=6.89e+00 | OffDiag=1.29e-05\n",
      "  MeanRelErr=21299225.6156% | MedianRelErr=99.2254%\n",
      "  λ∈[0.001069, 0.089602] | Spread=0.088533\n",
      "\n",
      "[Stage1] Epoch   2000 | LR=0.009055\n",
      "  Loss=310.436109 | Trace=75.268 | MSE=0.000000\n",
      "  Orth=3.84e+00 | OffDiag=1.98e+00\n",
      "  MeanRelErr=41690397.1919% | MedianRelErr=65.8413%\n",
      "  λ∈[0.002092, 3.449844] | Spread=3.447752\n",
      "\n",
      "[Stage1] Epoch   4000 | LR=0.006580\n",
      "  Loss=309.359004 | Trace=76.949 | MSE=0.000000\n",
      "  Orth=3.78e+00 | OffDiag=1.94e+00\n",
      "  MeanRelErr=50703567.4312% | MedianRelErr=61.5895%\n",
      "  λ∈[0.002544, 3.127835] | Spread=3.125291\n",
      "\n",
      "  ✓ New best model saved (loss=298.761183)\n",
      "[Stage1] Epoch   6000 | LR=0.003520\n",
      "  Loss=295.191670 | Trace=72.254 | MSE=0.000000\n",
      "  Orth=3.84e+00 | OffDiag=3.23e-01\n",
      "  MeanRelErr=47973302.9194% | MedianRelErr=64.8637%\n",
      "  λ∈[0.002407, 2.752915] | Spread=2.750508\n",
      "\n",
      "[Stage1] Epoch   8000 | LR=0.001045\n",
      "  Loss=292.149780 | Trace=70.954 | MSE=0.000000\n",
      "  Orth=3.86e+00 | OffDiag=7.71e-02\n",
      "  MeanRelErr=48866355.8133% | MedianRelErr=64.4189%\n",
      "  λ∈[0.002452, 2.557484] | Spread=2.555032\n",
      "\n",
      "[Stage1] Epoch  10000 | LR=0.010000\n",
      "  Loss=291.357635 | Trace=70.684 | MSE=0.000000\n",
      "  Orth=3.87e+00 | OffDiag=1.31e-02\n",
      "  MeanRelErr=57140113.4369% | MedianRelErr=64.6773%\n",
      "  λ∈[0.002867, 2.511910] | Spread=2.509043\n",
      "  First 5 - Pred: [0.002867 0.03593  0.042747 0.054131 0.075448]\n",
      "  First 5 - True: [0.       0.007574 0.030308 0.068146 0.121208]\n",
      "  Last  5 - Pred: [2.42944  2.433139 2.465162 2.474446 2.51191 ]\n",
      "  Last  5 - True: [7.607393 7.614276 7.708104 7.724856 7.834566]\n",
      "\n",
      "  ✓ New best model saved (loss=291.357635)\n",
      "[Stage1] Epoch  12000 | LR=0.009758\n",
      "  Loss=303.789759 | Trace=81.683 | MSE=0.000000\n",
      "  Orth=3.60e+00 | OffDiag=1.52e+00\n",
      "  MeanRelErr=42970657.7364% | MedianRelErr=58.1835%\n",
      "  λ∈[0.002156, 3.387058] | Spread=3.384902\n",
      "\n",
      "[Stage1] Epoch  14000 | LR=0.009055\n",
      "  Loss=304.333831 | Trace=80.178 | MSE=0.000000\n",
      "  Orth=3.66e+00 | OffDiag=1.56e+00\n",
      "  MeanRelErr=37351668.1872% | MedianRelErr=59.8957%\n",
      "  λ∈[0.001874, 3.032612] | Spread=3.030738\n",
      "\n",
      "[Stage1] Epoch  16000 | LR=0.007960\n",
      "  Loss=299.884151 | Trace=80.263 | MSE=0.000000\n",
      "  Orth=3.62e+00 | OffDiag=1.07e+00\n",
      "  MeanRelErr=38392733.2693% | MedianRelErr=57.8535%\n",
      "  λ∈[0.001926, 2.807236] | Spread=2.805310\n",
      "\n",
      "[Stage1] Epoch  18000 | LR=0.006580\n",
      "  Loss=297.396832 | Trace=80.959 | MSE=0.000000\n",
      "  Orth=3.60e+00 | OffDiag=7.73e-01\n",
      "  MeanRelErr=36721445.4236% | MedianRelErr=58.8664%\n",
      "  λ∈[0.001843, 3.132752] | Spread=3.130909\n",
      "\n",
      "[Stage1] Epoch  20000 | LR=0.005050\n",
      "  Loss=294.813900 | Trace=81.855 | MSE=0.000000\n",
      "  Orth=3.57e+00 | OffDiag=4.29e-01\n",
      "  MeanRelErr=37340960.7141% | MedianRelErr=57.3841%\n",
      "  λ∈[0.001874, 2.929250] | Spread=2.927376\n",
      "  First 5 - Pred: [0.001874 0.010736 0.033531 0.071883 0.127292]\n",
      "  First 5 - True: [0.       0.007574 0.030308 0.068146 0.121208]\n",
      "  Last  5 - Pred: [2.594414 2.634027 2.666886 2.751187 2.92925 ]\n",
      "  Last  5 - True: [7.607393 7.614276 7.708104 7.724856 7.834566]\n",
      "\n",
      "[Stage1] Epoch  22000 | LR=0.003520\n",
      "  Loss=292.181298 | Trace=79.395 | MSE=0.000000\n",
      "  Orth=3.62e+00 | OffDiag=1.82e-01\n",
      "  MeanRelErr=36390694.4600% | MedianRelErr=60.0093%\n",
      "  λ∈[0.001826, 2.775769] | Spread=2.773943\n",
      "\n",
      "[Stage1] Epoch  24000 | LR=0.002140\n",
      "  Loss=291.166060 | Trace=77.705 | MSE=0.000000\n",
      "  Orth=3.66e+00 | OffDiag=1.15e-01\n",
      "  MeanRelErr=36414846.9210% | MedianRelErr=60.3872%\n",
      "  λ∈[0.001827, 2.653010] | Spread=2.651183\n",
      "\n",
      "  ✓ New best model saved (loss=290.685080)\n",
      "[Stage1] Epoch  26000 | LR=0.001045\n",
      "  Loss=290.240062 | Trace=76.967 | MSE=12.050669\n",
      "  Orth=3.68e+00 | OffDiag=1.79e-02\n",
      "  MeanRelErr=36359991.9066% | MedianRelErr=60.7610%\n",
      "  λ∈[0.001824, 2.559998] | Spread=2.558173\n",
      "\n",
      "[Stage1] Epoch  28000 | LR=0.000342\n",
      "  Loss=290.052068 | Trace=76.601 | MSE=12.106201\n",
      "  Orth=3.69e+00 | OffDiag=2.05e-03\n",
      "  MeanRelErr=36302079.0617% | MedianRelErr=60.9436%\n",
      "  λ∈[0.001821, 2.541327] | Spread=2.539506\n",
      "\n",
      "[Stage1] Epoch  30000 | LR=0.010000\n",
      "  Loss=290.008186 | Trace=76.365 | MSE=12.145244\n",
      "  Orth=3.70e+00 | OffDiag=7.04e-04\n",
      "  MeanRelErr=36266610.8526% | MedianRelErr=61.1309%\n",
      "  λ∈[0.001820, 2.515667] | Spread=2.513848\n",
      "  First 5 - Pred: [0.00182  0.009487 0.03214  0.069629 0.121698]\n",
      "  First 5 - True: [0.       0.007574 0.030308 0.068146 0.121208]\n",
      "  Last  5 - Pred: [2.437722 2.439969 2.469605 2.479371 2.515667]\n",
      "  Last  5 - True: [7.607393 7.614276 7.708104 7.724856 7.834566]\n",
      "\n",
      "  ✓ New best model saved (loss=290.008186)\n",
      "[Stage1] Epoch  32000 | LR=0.009939\n",
      "  Loss=298.723352 | Trace=84.995 | MSE=10.970567\n",
      "  Orth=3.49e+00 | OffDiag=8.21e-01\n",
      "  MeanRelErr=35050785.7163% | MedianRelErr=56.0415%\n",
      "  λ∈[0.001759, 2.937512] | Spread=2.935754\n",
      "\n",
      "[Stage1] Epoch  34000 | LR=0.009758\n",
      "  Loss=298.928759 | Trace=84.601 | MSE=11.012612\n",
      "  Orth=3.49e+00 | OffDiag=1.04e+00\n",
      "  MeanRelErr=35588042.9283% | MedianRelErr=55.7755%\n",
      "  λ∈[0.001786, 3.271875] | Spread=3.270089\n",
      "\n",
      "[Stage1] Epoch  36000 | LR=0.009460\n",
      "  Loss=299.985753 | Trace=84.250 | MSE=11.026123\n",
      "  Orth=3.50e+00 | OffDiag=1.24e+00\n",
      "  MeanRelErr=35936515.1221% | MedianRelErr=55.9673%\n",
      "  λ∈[0.001803, 2.841196] | Spread=2.839393\n",
      "\n",
      "[Stage1] Epoch  38000 | LR=0.009055\n",
      "  Loss=295.807449 | Trace=82.807 | MSE=11.370383\n",
      "  Orth=3.54e+00 | OffDiag=5.16e-01\n",
      "  MeanRelErr=36388959.1276% | MedianRelErr=57.5248%\n",
      "  λ∈[0.001826, 2.853421] | Spread=2.851595\n",
      "\n",
      "[Stage1] Epoch  40000 | LR=0.008550\n",
      "  Loss=298.267900 | Trace=84.301 | MSE=11.034337\n",
      "  Orth=3.52e+00 | OffDiag=7.47e-01\n",
      "  MeanRelErr=34630346.3337% | MedianRelErr=59.1804%\n",
      "  λ∈[0.001738, 3.133475] | Spread=3.131737\n",
      "  First 5 - Pred: [0.001738 0.010299 0.033246 0.073366 0.127967]\n",
      "  First 5 - True: [0.       0.007574 0.030308 0.068146 0.121208]\n",
      "  Last  5 - Pred: [2.835913 2.848352 2.930831 3.034162 3.133475]\n",
      "  Last  5 - True: [7.607393 7.614276 7.708104 7.724856 7.834566]\n",
      "\n",
      "[Stage1] Epoch  42000 | LR=0.007960\n",
      "  Loss=295.969309 | Trace=82.372 | MSE=11.370361\n",
      "  Orth=3.54e+00 | OffDiag=6.45e-01\n",
      "  MeanRelErr=34884667.0774% | MedianRelErr=58.7328%\n",
      "  λ∈[0.001750, 2.852325] | Spread=2.850575\n",
      "\n",
      "[Stage1] Epoch  44000 | LR=0.007297\n",
      "  Loss=295.903099 | Trace=82.979 | MSE=11.329545\n",
      "  Orth=3.52e+00 | OffDiag=7.95e-01\n",
      "  MeanRelErr=35703040.0177% | MedianRelErr=57.9517%\n",
      "  λ∈[0.001791, 2.747483] | Spread=2.745692\n",
      "\n",
      "[Stage1] Epoch  46000 | LR=0.006580\n",
      "  Loss=293.217818 | Trace=82.557 | MSE=11.437598\n",
      "  Orth=3.53e+00 | OffDiag=2.89e-01\n",
      "  MeanRelErr=36469559.1615% | MedianRelErr=59.0612%\n",
      "  λ∈[0.001830, 2.792102] | Spread=2.790272\n",
      "\n",
      "[Stage1] Epoch  48000 | LR=0.005824\n",
      "  Loss=292.779696 | Trace=82.427 | MSE=11.426398\n",
      "  Orth=3.52e+00 | OffDiag=3.09e-01\n",
      "  MeanRelErr=35406563.2165% | MedianRelErr=57.6725%\n",
      "  λ∈[0.001777, 2.630220] | Spread=2.628443\n",
      "\n",
      "[Stage1] Epoch  50000 | LR=0.005050\n",
      "  Loss=292.293736 | Trace=82.145 | MSE=11.441786\n",
      "  Orth=3.53e+00 | OffDiag=2.28e-01\n",
      "  MeanRelErr=35705187.3540% | MedianRelErr=58.5701%\n",
      "  λ∈[0.001792, 2.778802] | Spread=2.777010\n",
      "  First 5 - Pred: [0.001792 0.009901 0.032716 0.070853 0.125947]\n",
      "  First 5 - True: [0.       0.007574 0.030308 0.068146 0.121208]\n",
      "  Last  5 - Pred: [2.607417 2.613405 2.631642 2.758484 2.778802]\n",
      "  Last  5 - True: [7.607393 7.614276 7.708104 7.724856 7.834566]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ENTERING STAGE 2: Fine-tuning eigenvalues\n",
      "================================================================================\n",
      "\n",
      "[Stage2] Epoch  52000 | LR=0.000999\n",
      "  Loss=172.111308 | Trace=177.554 | MSE=0.522637\n",
      "  Orth=6.83e+00 | OffDiag=1.65e-02\n",
      "  MeanRelErr=67586550.5351% | MedianRelErr=18.0533%\n",
      "  λ∈[0.003391, 7.117472] | Spread=7.114081\n",
      "\n",
      "[Stage2] Epoch  54000 | LR=0.000996\n",
      "  Loss=168.260211 | Trace=178.225 | MSE=0.497690\n",
      "  Orth=6.79e+00 | OffDiag=1.81e-02\n",
      "  MeanRelErr=26525946.5751% | MedianRelErr=17.4880%\n",
      "  λ∈[0.001331, 7.132110] | Spread=7.130779\n",
      "\n",
      "  ✓ New best model saved (loss=166.717107)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 407\u001b[0m\n\u001b[1;32m    405\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    406\u001b[0m U \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m--> 407\u001b[0m loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigvals_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    410\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[2], line 272\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(U, K, M, eigvals_ref, epoch, config)\u001b[0m\n\u001b[1;32m    262\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (w_trace \u001b[38;5;241m*\u001b[39m trace_loss \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    263\u001b[0m               w_mse \u001b[38;5;241m*\u001b[39m eig_mse_loss \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    264\u001b[0m               w_div \u001b[38;5;241m*\u001b[39m diversity_loss \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m               w_order \u001b[38;5;241m*\u001b[39m ordering_loss \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    268\u001b[0m               w_smooth \u001b[38;5;241m*\u001b[39m smoothness_loss)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Return loss and components for logging\u001b[39;00m\n\u001b[1;32m    271\u001b[0m components \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtrace_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m: eig_mse_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiversity\u001b[39m\u001b[38;5;124m'\u001b[39m: diversity_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffdiag\u001b[39m\u001b[38;5;124m'\u001b[39m: off_diag_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morth\u001b[39m\u001b[38;5;124m'\u001b[39m: orth_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m: ordering_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth\u001b[39m\u001b[38;5;124m'\u001b[39m: smoothness_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    279\u001b[0m }\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, components\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'k': 50,  # Number of eigenmodes\n",
    "    'max_epochs': 150_000,  # Reduced from 300k due to better architecture\n",
    "    'stage1_epochs': 50_000,  # Focus on low modes + orthogonality\n",
    "    'stage2_epochs': 100_000,  # Fine-tune all modes\n",
    "    'lr_stage1': 0.01,\n",
    "    'lr_stage2': 0.001,\n",
    "    'lr_min': 0.0001,\n",
    "    'print_every': 2000,\n",
    "    'checkpoint_every': 5000,\n",
    "    'grad_clip': 1.0,\n",
    "    'accumulation_steps': 1,  # Set to 4 if you have memory issues\n",
    "    'early_stopping_patience': 50_000,\n",
    "    'early_stopping_threshold': 0.999,  # 0.1% improvement needed\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading mesh...\")\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "# Normalize vertices\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "verts_new = (m.verts - centroid) / std_max\n",
    "m = Mesh(verts=verts_new, connectivity=m.connectivity)\n",
    "\n",
    "print('Computing Laplacian...')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "print('Computing eigenvalues (reference)...')\n",
    "eigvals, eigvecs = linalg.eigh(K, M)\n",
    "\n",
    "# Convert to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "eigvals_torch = torch.from_numpy(eigvals[:CONFIG['k']]).to(device)\n",
    "eigvecs_torch = torch.from_numpy(eigvecs[:, :CONFIG['k']]).to(device)\n",
    "\n",
    "k = CONFIG['k']\n",
    "N = X.shape[0]\n",
    "\n",
    "# ============================================================================\n",
    "# MATRIX CONDITIONING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Matrix Conditioning ===\")\n",
    "epsilon = 1e-4\n",
    "K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "K_scale = torch.norm(K_reg, p='fro')\n",
    "\n",
    "# Normalize both matrices by same scale\n",
    "K = K_reg / K_scale\n",
    "M = M / K_scale\n",
    "\n",
    "print(f\"Regularization: ε={epsilon}\")\n",
    "print(f\"Condition number: {torch.linalg.cond(K).item():.2e}\")\n",
    "print(f\"Scaling factor: {K_scale:.2e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced MLP with:\n",
    "    - Fourier feature encoding\n",
    "    - Layer normalization\n",
    "    - Residual connections\n",
    "    - Increased capacity\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=50, hidden=[256, 256, 128, 128], \n",
    "                 use_fourier=True, n_fourier_features=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_fourier = use_fourier\n",
    "        self.n_fourier = n_fourier_features\n",
    "        \n",
    "        # Learnable frequency scales for Fourier features\n",
    "        if use_fourier:\n",
    "            self.freq_scale = nn.Parameter(torch.ones(in_dim) * 10.0)\n",
    "            input_dim = in_dim * (1 + 2 * n_fourier_features)  # original + sin + cos for each freq\n",
    "        else:\n",
    "            input_dim = in_dim\n",
    "        \n",
    "        # Build network with normalization and residual capability\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        last_dim = input_dim\n",
    "        for h in hidden:\n",
    "            self.layers.append(nn.Linear(last_dim, h))\n",
    "            self.norms.append(nn.LayerNorm(h))\n",
    "            last_dim = h\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(last_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Fourier feature mapping\n",
    "        if self.use_fourier:\n",
    "            features = [x]\n",
    "            for i in range(self.n_fourier):\n",
    "                freq = self.freq_scale * (2 ** i)  # Multiple frequency scales\n",
    "                features.append(torch.sin(freq * x))\n",
    "                features.append(torch.cos(freq * x))\n",
    "            h = torch.cat(features, dim=-1)\n",
    "        else:\n",
    "            h = x\n",
    "        \n",
    "        # Forward pass with residual connections\n",
    "        for i, (layer, norm) in enumerate(zip(self.layers, self.norms)):\n",
    "            h_new = layer(h)\n",
    "            h_new = norm(h_new)\n",
    "            h_new = torch.nn.functional.silu(h_new)\n",
    "            \n",
    "            # Residual connection (if dimensions match)\n",
    "            if i > 0 and h.shape[-1] == h_new.shape[-1]:\n",
    "                h = h_new + 0.1 * h  # Scaled residual\n",
    "            else:\n",
    "                h = h_new\n",
    "        \n",
    "        return self.output(h)\n",
    "\n",
    "# ============================================================================\n",
    "# ORTHOGONALIZATION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def gram_schmidt_batch(U, M, num_steps=1):\n",
    "    \"\"\"\n",
    "    M-orthogonalize columns of U using Gram-Schmidt process\n",
    "    \n",
    "    Args:\n",
    "        U: (N, k) matrix to orthogonalize\n",
    "        M: (N, N) mass matrix\n",
    "        num_steps: number of GS iterations (>1 for stability)\n",
    "    \"\"\"\n",
    "    Q = U.clone()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        for i in range(Q.shape[1]):\n",
    "            # Orthogonalize against all previous vectors\n",
    "            for j in range(i):\n",
    "                # M-inner product\n",
    "                numerator = Q[:, j] @ (M @ Q[:, i])\n",
    "                denominator = Q[:, j] @ (M @ Q[:, j]) + 1e-10\n",
    "                proj = numerator / denominator\n",
    "                Q[:, i] = Q[:, i] - proj * Q[:, j]\n",
    "            \n",
    "            # M-normalize\n",
    "            norm = torch.sqrt(Q[:, i] @ (M @ Q[:, i]) + 1e-10)\n",
    "            Q[:, i] = Q[:, i] / norm\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(U, K, M, eigvals_ref, epoch, config):\n",
    "    \"\"\"\n",
    "    Comprehensive loss function with multiple objectives\n",
    "    UNSUPERVISED: eigvals_ref only used for logging/comparison, NOT for training!\n",
    "    \"\"\"\n",
    "    # Compute Rayleigh quotients\n",
    "    UMU = U.T @ (M @ U)\n",
    "    UKU = U.T @ (K @ U)\n",
    "    \n",
    "    # Extract diagonal (eigenvalue estimates)\n",
    "    eigenvalues_approx = torch.diag(UKU)\n",
    "    sorted_eigs, sort_idx = torch.sort(eigenvalues_approx)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. EIGENVALUE LOSSES (UNSUPERVISED)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # 1a. Trace loss (minimize sum - finds smallest modes)\n",
    "    trace_loss = torch.sum(sorted_eigs)\n",
    "    \n",
    "    # 1b. Diversity loss (encourage eigenvalue separation)\n",
    "    # Use adaptive gaps based on current eigenvalue scale\n",
    "    gaps = sorted_eigs[1:] - sorted_eigs[:-1]\n",
    "    # Target minimum gap: 1% of current eigenvalue spread\n",
    "    current_spread = sorted_eigs[-1] - sorted_eigs[0] + 1e-8\n",
    "    min_gap = current_spread * 0.01 / k  # Adaptive to current scale\n",
    "    diversity_loss = torch.sum(torch.relu(min_gap - gaps))\n",
    "    \n",
    "    # 1c. Off-diagonal penalty (enforce diagonalization)\n",
    "    off_diag_mask = 1 - torch.eye(k, device=device, dtype=torch.float64)\n",
    "    off_diag_loss = torch.sum((UKU * off_diag_mask) ** 2)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. ORTHOGONALITY LOSS\n",
    "    # ========================================================================\n",
    "    \n",
    "    identity_k = torch.eye(k, device=device, dtype=torch.float64)\n",
    "    orth_loss = torch.norm(UMU - identity_k, p='fro') ** 2\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. ORDERING LOSS (maintain λ₁ ≤ λ₂ ≤ ... ≤ λₖ)\n",
    "    # ========================================================================\n",
    "    \n",
    "    ordering_loss = torch.sum(torch.relu(sorted_eigs[:-1] - sorted_eigs[1:])) / k\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. SMOOTHNESS REGULARIZATION (optional, for later stages)\n",
    "    # ========================================================================\n",
    "    \n",
    "    if epoch > config['stage1_epochs']:\n",
    "        U_sorted = U[:, sort_idx]\n",
    "        # Penalize large jumps between consecutive modes\n",
    "        smoothness_loss = torch.mean(torch.sum((U_sorted[:, 1:] - U_sorted[:, :-1]) ** 2, dim=0))\n",
    "    else:\n",
    "        smoothness_loss = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ADAPTIVE WEIGHTING (FULLY UNSUPERVISED)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Stage 1: Focus on orthogonality and finding low modes\n",
    "    if epoch <= config['stage1_epochs']:\n",
    "        w_trace = 5.0        # Strong push to find smallest eigenvalues\n",
    "        w_div = 2.0          # Encourage separation\n",
    "        w_offdiag = 10.0     # Strong diagonalization\n",
    "        w_orth = 10.0        # Strong orthogonality\n",
    "        w_order = 0.5        # Maintain ordering\n",
    "        w_smooth = 0.0\n",
    "    # Stage 2: Fine-tune, relax some constraints\n",
    "    else:\n",
    "        w_trace = 2.0        # Still minimize, but less aggressive\n",
    "        w_div = 1.0          # Maintain separation\n",
    "        w_offdiag = 15.0     # Even stronger diagonalization\n",
    "        w_orth = 5.0         # Maintain orthogonality\n",
    "        w_order = 0.2        # Maintain ordering\n",
    "        w_smooth = 0.1       # Add smoothness\n",
    "    \n",
    "    # Total loss (NO SUPERVISION)\n",
    "    total_loss = (w_trace * trace_loss +\n",
    "                  w_div * diversity_loss +\n",
    "                  w_offdiag * off_diag_loss +\n",
    "                  w_orth * orth_loss +\n",
    "                  w_order * ordering_loss +\n",
    "                  w_smooth * smoothness_loss)\n",
    "    \n",
    "    # Return loss and components for logging\n",
    "    components = {\n",
    "        'trace': trace_loss.item(),\n",
    "        'diversity': diversity_loss.item(),\n",
    "        'offdiag': off_diag_loss.item(),\n",
    "        'orth': orth_loss.item(),\n",
    "        'order': ordering_loss.item(),\n",
    "        'smooth': smoothness_loss.item(),\n",
    "    }\n",
    "    \n",
    "    return total_loss, components\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Initializing Model ===\")\n",
    "model = ImprovedMLP(in_dim=3, out_dim=k, use_fourier=True).double().to(device)\n",
    "\n",
    "# Xavier initialization for hidden layers, small weights for output\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if 'output' in name:\n",
    "            nn.init.normal_(p.data, std=1e-4)\n",
    "        elif 'freq_scale' not in name and p.ndim >= 2:  # Only for 2D+ tensors\n",
    "            nn.init.xavier_uniform_(p.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.zeros_(p.data)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Fourier features: {model.use_fourier}\")\n",
    "print(f\"Architecture: {[l.out_features for l in model.layers]} -> {k}\")\n",
    "\n",
    "# ============================================================================\n",
    "# NO PRE-TRAINING - Pure unsupervised learning!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Unsupervised Mode: No pre-training ===\")\n",
    "print(\"Model will discover eigenmodes from scratch using only K and M matrices\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: High learning rate, focus on orthogonality\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                        lr=CONFIG['lr_stage1'], \n",
    "                        weight_decay=1e-5)\n",
    "\n",
    "# Cosine annealing with restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10000, T_mult=2, eta_min=CONFIG['lr_min']\n",
    ")\n",
    "\n",
    "# Tracking\n",
    "loss_history = []\n",
    "best_loss = float('inf')\n",
    "no_improve_count = 0\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING START\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Epochs: {CONFIG['max_epochs']:,} | Device: {device}\")\n",
    "print(f\"Stage 1: 0-{CONFIG['stage1_epochs']:,} (orthogonality focus)\")\n",
    "print(f\"Stage 2: {CONFIG['stage1_epochs']:,}-{CONFIG['max_epochs']:,} (eigenvalue tuning)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG['max_epochs'] + 1):\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE TRANSITION\n",
    "    # ========================================================================\n",
    "    \n",
    "    if epoch == CONFIG['stage1_epochs'] + 1:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ENTERING STAGE 2: Fine-tuning eigenvalues\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        # Reduce learning rate for stage 2\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = CONFIG['lr_stage2']\n",
    "        \n",
    "        # Reset scheduler for stage 2\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=CONFIG['stage2_epochs'],\n",
    "            eta_min=CONFIG['lr_min']\n",
    "        )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAINING STEP\n",
    "    # ========================================================================\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    U = model(X)\n",
    "    \n",
    "    # Periodic explicit orthogonalization (every 1000 steps in stage 1)\n",
    "    if epoch <= CONFIG['stage1_epochs'] and epoch % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            U = gram_schmidt_batch(U, M, num_steps=2)\n",
    "        \n",
    "        # Quick fine-tune to match orthogonalized output\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            U_pred = model(X)\n",
    "            loss_proj = torch.mean((U_pred - U) ** 2)\n",
    "            loss_proj.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Compute loss\n",
    "    optimizer.zero_grad()\n",
    "    U = model(X)\n",
    "    loss, loss_components = compute_loss(U, K, M, eigvals_torch, epoch, CONFIG)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "    \n",
    "    # Optimizer step (with optional gradient accumulation)\n",
    "    if epoch % CONFIG['accumulation_steps'] == 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOGGING\n",
    "    # ========================================================================\n",
    "    \n",
    "    if epoch % CONFIG['print_every'] == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            U = model(X)\n",
    "            UKU = U.T @ (K @ U)\n",
    "            UMU = U.T @ (M @ U)\n",
    "            \n",
    "            approx_eigs = torch.diag(UKU).cpu().numpy()\n",
    "            approx_eigs.sort()\n",
    "            \n",
    "            # Compute errors\n",
    "            abs_error = np.abs(approx_eigs - eigvals[:k])\n",
    "            rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "            mean_rel_error = np.mean(rel_error)\n",
    "            median_rel_error = np.median(rel_error)\n",
    "            \n",
    "            # Orthogonality check\n",
    "            orth_residual = torch.norm(UMU - torch.eye(k, device=device, dtype=torch.float64), p='fro').item()\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            stage = \"Stage1\" if epoch <= CONFIG['stage1_epochs'] else \"Stage2\"\n",
    "            \n",
    "        print(f\"[{stage}] Epoch {epoch:>6} | LR={current_lr:.6f}\")\n",
    "        print(f\"  Loss={loss.item():.6f} | Trace={loss_components['trace']:.3f} | Div={loss_components['diversity']:.4f}\")\n",
    "        print(f\"  Orth={orth_residual:.2e} | OffDiag={loss_components['offdiag']:.2e}\")\n",
    "        print(f\"  MeanRelErr={mean_rel_error:.4%} | MedianRelErr={median_rel_error:.4%}\")\n",
    "        print(f\"  λ∈[{approx_eigs[0]:.6f}, {approx_eigs[-1]:.6f}] | Spread={approx_eigs[-1]-approx_eigs[0]:.6f}\")\n",
    "        \n",
    "        # Detailed comparison every 10k epochs\n",
    "        if epoch % (CONFIG['print_every'] * 5) == 0:\n",
    "            print(f\"  First 5 - Pred: {approx_eigs[:5].round(6)}\")\n",
    "            print(f\"  First 5 - True: {eigvals[:5].round(6)}\")\n",
    "            print(f\"  Last  5 - Pred: {approx_eigs[-5:].round(6)}\")\n",
    "            print(f\"  Last  5 - True: {eigvals[k-5:k].round(6)}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CHECKPOINTING & EARLY STOPPING\n",
    "    # ========================================================================\n",
    "    \n",
    "    if epoch % CONFIG['checkpoint_every'] == 0:\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'loss_history': loss_history,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "        \n",
    "        # Check for improvement\n",
    "        if loss.item() < best_loss * CONFIG['early_stopping_threshold']:\n",
    "            best_loss = loss.item()\n",
    "            no_improve_count = 0\n",
    "            torch.save(checkpoint, checkpoint_dir / 'best_model.pt')\n",
    "            print(f\"  ✓ New best model saved (loss={best_loss:.6f})\")\n",
    "        else:\n",
    "            no_improve_count += CONFIG['checkpoint_every']\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_count >= CONFIG['early_stopping_patience']:\n",
    "            print(f\"\\n⚠ Early stopping at epoch {epoch} (no improvement for {no_improve_count} epochs)\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading best model for evaluation...\")\n",
    "best_checkpoint = torch.load(checkpoint_dir / 'best_model.pt')\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U = model(X)\n",
    "    \n",
    "    # Final matrices\n",
    "    final_UKU = U.T @ (K @ U)\n",
    "    final_UMU = U.T @ (M @ U)\n",
    "    \n",
    "    # Eigenvalues\n",
    "    final_eigs = torch.diag(final_UKU).cpu().numpy()\n",
    "    final_eigs.sort()\n",
    "    \n",
    "    # Errors\n",
    "    abs_error = np.abs(final_eigs - eigvals[:k])\n",
    "    rel_error = abs_error / (np.abs(eigvals[:k]) + 1e-10)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Orthogonality\n",
    "    identity_k = torch.eye(k, device=device, dtype=torch.float64)\n",
    "    orth_residual = torch.norm(final_UMU - identity_k, p='fro').item()\n",
    "    orth_diag = torch.diag(final_UMU).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n Orthogonality Quality:\")\n",
    "    print(f\"  ||U^T M U - I||_F = {orth_residual:.2e}\")\n",
    "    print(f\"  Diagonal range: [{orth_diag.min():.6f}, {orth_diag.max():.6f}] (target: 1.0)\")\n",
    "    \n",
    "    # Rayleigh matrix\n",
    "    rayleigh_diag = torch.diag(final_UKU).cpu().numpy()\n",
    "    rayleigh_offdiag = (final_UKU - torch.diag(torch.diag(final_UKU))).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n Rayleigh Quotient Matrix:\")\n",
    "    print(f\"  Diagonal norm: {np.linalg.norm(rayleigh_diag):.6f}\")\n",
    "    print(f\"  Off-diagonal norm: {np.linalg.norm(rayleigh_offdiag, 'fro'):.2e} (should be ≈0)\")\n",
    "    \n",
    "    # Eigenvalue comparison\n",
    "    print(f\"\\n Eigenvalue Comparison (First 10):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<14} {'Reference':<14} {'Abs Error':<14} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i in range(min(10, k)):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<14.8f} {eigvals[i]:<14.8f} \"\n",
    "              f\"{abs_error[i]:<14.8f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    print(f\"\\n Eigenvalue Comparison (Last 10):\")\n",
    "    print(f\"{'Mode':<6} {'Predicted':<14} {'Reference':<14} {'Abs Error':<14} {'Rel Error':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i in range(max(0, k-10), k):\n",
    "        print(f\"{i+1:<6} {final_eigs[i]:<14.8f} {eigvals[i]:<14.8f} \"\n",
    "              f\"{abs_error[i]:<14.8f} {rel_error[i]:<12.4%}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n Overall Statistics ({k} modes):\")\n",
    "    print(f\"  Mean Absolute Error:     {np.mean(abs_error):.8f}\")\n",
    "    print(f\"  Mean Relative Error:     {np.mean(rel_error):.4%}\")\n",
    "    print(f\"  Median Relative Error:   {np.median(rel_error):.4%}\")\n",
    "    print(f\"  Max Relative Error:      {np.max(rel_error):.4%}\")\n",
    "    print(f\"  Modes with <1% error:    {np.sum(rel_error < 0.01)}/{k}\")\n",
    "    print(f\"  Modes with <5% error:    {np.sum(rel_error < 0.05)}/{k}\")\n",
    "    print(f\"  Modes with <10% error:   {np.sum(rel_error < 0.10)}/{k}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'final_eigenvalues': final_eigs,\n",
    "    'reference_eigenvalues': eigvals[:k],\n",
    "    'errors': {'absolute': abs_error, 'relative': rel_error},\n",
    "}, 'final_model.pt')\n",
    "\n",
    "print(\"\\n✓ Model saved to 'final_model.pt'\")\n",
    "print(\"✓ Checkpoints saved to 'checkpoints/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04a001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
