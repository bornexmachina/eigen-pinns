{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53342ca8-db05-456c-8e2b-5fc44e26698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n",
      "\n",
      "Using epsilon=0.0001, final condition number: 1.47e+05\n",
      "\n",
      "=== Matrix Normalization ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)\n",
    "\n",
    "\n",
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)\n",
    "N = X.shape[0]\n",
    "\n",
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Apply the regularization\n",
    "K_reg = K + epsilon * torch.eye(N, device=device)\n",
    "print(f\"\\nUsing epsilon={epsilon}, final condition number: {torch.linalg.cond(K_reg).item():.2e}\")\n",
    "\n",
    "print(\"\\n=== Matrix Normalization ===\")\n",
    "K_scale = torch.norm(K_reg, p='fro')\n",
    "\n",
    "K = K_reg / K_scale\n",
    "M = M / K_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018a8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1, total loss=49430.347857, orthogonal loss=24.5302, eigen loss=494.0582\n",
      "Epoch 1000, total loss=195127.295924, orthogonal loss=0.0000, eigen loss=1951.2730\n",
      "Epoch 2000, total loss=184466.079235, orthogonal loss=0.0000, eigen loss=1844.6608\n",
      "Epoch 3000, total loss=177873.340622, orthogonal loss=0.0000, eigen loss=1778.7334\n",
      "Epoch 4000, total loss=173375.323495, orthogonal loss=0.0000, eigen loss=1733.7532\n",
      "Epoch 5000, total loss=170370.003695, orthogonal loss=0.0000, eigen loss=1703.7000\n",
      "Epoch 6000, total loss=167046.180719, orthogonal loss=0.0000, eigen loss=1670.4618\n",
      "Epoch 7000, total loss=164315.378199, orthogonal loss=0.0000, eigen loss=1643.1538\n",
      "Epoch 8000, total loss=162971.924109, orthogonal loss=0.0000, eigen loss=1629.7192\n",
      "Epoch 9000, total loss=161534.146348, orthogonal loss=0.0000, eigen loss=1615.3415\n",
      "Epoch 10000, total loss=161518.334778, orthogonal loss=0.0000, eigen loss=1615.1833\n",
      "Epoch 11000, total loss=159758.849099, orthogonal loss=0.0000, eigen loss=1597.5885\n",
      "Epoch 12000, total loss=158427.028552, orthogonal loss=0.0000, eigen loss=1584.2703\n",
      "Epoch 13000, total loss=157744.193093, orthogonal loss=0.0000, eigen loss=1577.4419\n",
      "Epoch 14000, total loss=156918.292522, orthogonal loss=0.0000, eigen loss=1569.1829\n",
      "Epoch 15000, total loss=156028.419468, orthogonal loss=0.0000, eigen loss=1560.2842\n",
      "Epoch 16000, total loss=155144.012108, orthogonal loss=0.0000, eigen loss=1551.4401\n",
      "Epoch 17000, total loss=154481.774327, orthogonal loss=0.0000, eigen loss=1544.8177\n",
      "Epoch 18000, total loss=154037.562251, orthogonal loss=0.0000, eigen loss=1540.3756\n",
      "Epoch 19000, total loss=153520.094949, orthogonal loss=0.0000, eigen loss=1535.2009\n",
      "Epoch 20000, total loss=153276.707725, orthogonal loss=0.0000, eigen loss=1532.7671\n",
      "Epoch 21000, total loss=152859.939806, orthogonal loss=0.0000, eigen loss=1528.5994\n",
      "Epoch 22000, total loss=152618.080039, orthogonal loss=0.0000, eigen loss=1526.1808\n",
      "Epoch 23000, total loss=152199.995646, orthogonal loss=0.0000, eigen loss=1522.0000\n",
      "Epoch 24000, total loss=152007.313506, orthogonal loss=0.0000, eigen loss=1520.0731\n",
      "Epoch 25000, total loss=151820.765081, orthogonal loss=0.0000, eigen loss=1518.2077\n",
      "Epoch 26000, total loss=151643.256597, orthogonal loss=0.0000, eigen loss=1516.4326\n",
      "Epoch 27000, total loss=151619.593007, orthogonal loss=0.0000, eigen loss=1516.1959\n",
      "Epoch 28000, total loss=151267.073640, orthogonal loss=0.0000, eigen loss=1512.6707\n",
      "Epoch 29000, total loss=151107.880942, orthogonal loss=0.0000, eigen loss=1511.0788\n",
      "Epoch 30000, total loss=150943.952959, orthogonal loss=0.0000, eigen loss=1509.4395\n",
      "Epoch 31000, total loss=150789.534941, orthogonal loss=0.0000, eigen loss=1507.8953\n",
      "Epoch 32000, total loss=150714.552479, orthogonal loss=0.0000, eigen loss=1507.1455\n",
      "Epoch 33000, total loss=150578.669785, orthogonal loss=0.0000, eigen loss=1505.7867\n",
      "Epoch 34000, total loss=150461.350998, orthogonal loss=0.0000, eigen loss=1504.6135\n",
      "Epoch 35000, total loss=150383.877043, orthogonal loss=0.0000, eigen loss=1503.8388\n",
      "Epoch 36000, total loss=150273.292932, orthogonal loss=0.0000, eigen loss=1502.7329\n",
      "Epoch 37000, total loss=150193.566289, orthogonal loss=0.0000, eigen loss=1501.9357\n",
      "Epoch 38000, total loss=150124.969649, orthogonal loss=0.0000, eigen loss=1501.2497\n",
      "Epoch 39000, total loss=150051.411335, orthogonal loss=0.0000, eigen loss=1500.5141\n",
      "Epoch 40000, total loss=149911.784177, orthogonal loss=0.0000, eigen loss=1499.1178\n",
      "Epoch 41000, total loss=149860.675319, orthogonal loss=0.0000, eigen loss=1498.6068\n",
      "Epoch 42000, total loss=149793.443361, orthogonal loss=0.0000, eigen loss=1497.9344\n",
      "Epoch 43000, total loss=149686.211813, orthogonal loss=0.0000, eigen loss=1496.8621\n",
      "Epoch 44000, total loss=149676.654284, orthogonal loss=0.0000, eigen loss=1496.7665\n",
      "Epoch 45000, total loss=149597.973725, orthogonal loss=0.0000, eigen loss=1495.9797\n",
      "Epoch 46000, total loss=149540.857531, orthogonal loss=0.0000, eigen loss=1495.4086\n",
      "Epoch 47000, total loss=149454.047204, orthogonal loss=0.0000, eigen loss=1494.5405\n",
      "Epoch 48000, total loss=149408.046702, orthogonal loss=0.0000, eigen loss=1494.0805\n",
      "Epoch 49000, total loss=149300.547486, orthogonal loss=0.0000, eigen loss=1493.0055\n",
      "Epoch 50000, total loss=149224.092210, orthogonal loss=0.0000, eigen loss=1492.2409\n",
      "Epoch 51000, total loss=149128.119500, orthogonal loss=0.0000, eigen loss=1491.2812\n",
      "Epoch 52000, total loss=149041.245996, orthogonal loss=0.0000, eigen loss=1490.4125\n",
      "Epoch 53000, total loss=148935.274667, orthogonal loss=0.0000, eigen loss=1489.3527\n",
      "Epoch 54000, total loss=148889.457355, orthogonal loss=0.0000, eigen loss=1488.8946\n",
      "Epoch 55000, total loss=148858.087241, orthogonal loss=0.0000, eigen loss=1488.5809\n",
      "Epoch 56000, total loss=148742.232996, orthogonal loss=0.0000, eigen loss=1487.4223\n",
      "Epoch 57000, total loss=148724.900074, orthogonal loss=0.0000, eigen loss=1487.2490\n",
      "Epoch 58000, total loss=148635.569426, orthogonal loss=0.0000, eigen loss=1486.3557\n",
      "Epoch 59000, total loss=148576.456080, orthogonal loss=0.0000, eigen loss=1485.7646\n",
      "Epoch 60000, total loss=148526.348188, orthogonal loss=0.0000, eigen loss=1485.2635\n",
      "Epoch 61000, total loss=148481.624326, orthogonal loss=0.0000, eigen loss=1484.8162\n",
      "Epoch 62000, total loss=148429.041031, orthogonal loss=0.0000, eigen loss=1484.2904\n",
      "Epoch 63000, total loss=148390.187842, orthogonal loss=0.0000, eigen loss=1483.9019\n",
      "Epoch 64000, total loss=148346.913101, orthogonal loss=0.0000, eigen loss=1483.4691\n",
      "Epoch 65000, total loss=148317.854593, orthogonal loss=0.0000, eigen loss=1483.1785\n",
      "Epoch 66000, total loss=148277.253928, orthogonal loss=0.0000, eigen loss=1482.7725\n",
      "Epoch 67000, total loss=148229.941972, orthogonal loss=0.0000, eigen loss=1482.2994\n",
      "Epoch 68000, total loss=148192.138669, orthogonal loss=0.0000, eigen loss=1481.9214\n",
      "Epoch 69000, total loss=148164.086786, orthogonal loss=0.0000, eigen loss=1481.6409\n",
      "Epoch 70000, total loss=148106.541158, orthogonal loss=0.0000, eigen loss=1481.0654\n",
      "Epoch 71000, total loss=148060.569192, orthogonal loss=0.0000, eigen loss=1480.6057\n",
      "Epoch 72000, total loss=148027.418097, orthogonal loss=0.0000, eigen loss=1480.2742\n",
      "Epoch 73000, total loss=148001.582360, orthogonal loss=0.0000, eigen loss=1480.0158\n",
      "Epoch 74000, total loss=147958.459256, orthogonal loss=0.0000, eigen loss=1479.5846\n",
      "Epoch 75000, total loss=147930.837868, orthogonal loss=0.0000, eigen loss=1479.3084\n",
      "Epoch 76000, total loss=147891.645735, orthogonal loss=0.0000, eigen loss=1478.9165\n",
      "Epoch 77000, total loss=147866.781057, orthogonal loss=0.0000, eigen loss=1478.6678\n",
      "Epoch 78000, total loss=147832.763773, orthogonal loss=0.0000, eigen loss=1478.3276\n",
      "Epoch 79000, total loss=147802.912633, orthogonal loss=0.0000, eigen loss=1478.0291\n",
      "Epoch 80000, total loss=147772.520651, orthogonal loss=0.0000, eigen loss=1477.7252\n",
      "Epoch 81000, total loss=147738.596235, orthogonal loss=0.0000, eigen loss=1477.3860\n",
      "Epoch 82000, total loss=147741.331277, orthogonal loss=0.0000, eigen loss=1477.4133\n",
      "Epoch 83000, total loss=147680.784799, orthogonal loss=0.0000, eigen loss=1476.8078\n",
      "Epoch 84000, total loss=147652.758905, orthogonal loss=0.0000, eigen loss=1476.5276\n",
      "Epoch 85000, total loss=147629.761673, orthogonal loss=0.0000, eigen loss=1476.2976\n",
      "Epoch 86000, total loss=147600.869779, orthogonal loss=0.0000, eigen loss=1476.0087\n",
      "Epoch 87000, total loss=147574.380194, orthogonal loss=0.0000, eigen loss=1475.7438\n",
      "Epoch 88000, total loss=147551.948119, orthogonal loss=0.0000, eigen loss=1475.5195\n",
      "Epoch 89000, total loss=147520.409155, orthogonal loss=0.0000, eigen loss=1475.2041\n",
      "Epoch 90000, total loss=147497.137714, orthogonal loss=0.0000, eigen loss=1474.9714\n",
      "Epoch 91000, total loss=147468.994458, orthogonal loss=0.0000, eigen loss=1474.6899\n",
      "Epoch 92000, total loss=147446.009297, orthogonal loss=0.0000, eigen loss=1474.4601\n",
      "Epoch 93000, total loss=147419.511914, orthogonal loss=0.0000, eigen loss=1474.1951\n",
      "Epoch 94000, total loss=147395.379554, orthogonal loss=0.0000, eigen loss=1473.9538\n",
      "Epoch 95000, total loss=147371.517417, orthogonal loss=0.0000, eigen loss=1473.7152\n",
      "Epoch 96000, total loss=147347.978633, orthogonal loss=0.0000, eigen loss=1473.4798\n",
      "Epoch 97000, total loss=147324.821554, orthogonal loss=0.0000, eigen loss=1473.2482\n",
      "Epoch 98000, total loss=147301.715723, orthogonal loss=0.0000, eigen loss=1473.0172\n",
      "Epoch 99000, total loss=147279.701806, orthogonal loss=0.0000, eigen loss=1472.7970\n",
      "Epoch 100000, total loss=147256.866880, orthogonal loss=0.0000, eigen loss=1472.5687\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network that maps coordinates -> k outputs per node\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h, dtype=torch.double))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim, dtype=torch.double))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = MLP().to(device)\n",
    "# Initialize all layers (Xavier), final layer small\n",
    "for name, p in model.named_parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "for p in model.net[-1].parameters():  # last Linear\n",
    "    if p.ndim == 2:\n",
    "        nn.init.normal_(p, std=1e-3)\n",
    "    else:\n",
    "        nn.init.zeros_(p)\n",
    "\n",
    "# Helper: given A (N x k), return M-orthonormalized U = A (A^T M A)^{-1/2}\n",
    "def m_orthonormalize(A, M):\n",
    "    # A: (N, k), M: (N, N)\n",
    "    # compute B = A^T M A (k x k)\n",
    "    B = A.T @ (M @ A)  # k x k\n",
    "    # symmetrize B\n",
    "    B = 0.5*(B + B.T)\n",
    "    # compute inverse sqrt of B via eigendecomposition (k small)\n",
    "    s, Q = torch.linalg.eigh(B)  # s are eigenvalues\n",
    "    # regularize small eigenvalues\n",
    "    s_clamped = torch.clamp(s, min=1e-12)\n",
    "    inv_sqrt = Q @ torch.diag(1.0/torch.sqrt(s_clamped)) @ Q.T\n",
    "    U = A @ inv_sqrt\n",
    "    return U\n",
    "\n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "max_epochs = 100_000\n",
    "print_every = 1_000\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_start)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    A = model(X)  # N x k\n",
    "    U = m_orthonormalize(A, M)  # U is M-orthonormal\n",
    "\n",
    "    UKU = U.T @ (K @ U)\n",
    "    UMU = U.T @ (M @ U)        # k x k\n",
    "    \n",
    "    orth_loss = torch.norm(UMU - torch.eye(k, device=device), p='fro')**2\n",
    "    eig_loss = torch.norm(UKU, p='fro')**2\n",
    "\n",
    "    loss = 100 * eig_loss + orth_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}, total loss={loss.item():.6f}, orthogonal loss={orth_loss.item():.4f}, eigen loss={eig_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46439ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of U.t @ K @ U is: torch.Size([50, 50])\n",
      "\n",
      "Learned Ritz values (from U^T K U): [0.006524 0.073555 0.229321 0.262386 0.283282]\n",
      "Reference eigenvalues (first k):    [0.       0.007574 0.030308 0.068146 0.121208]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    A_final = model(X)\n",
    "\n",
    "    U_final= m_orthonormalize(A_final, M)\n",
    "    UKU = U_final.T @ (K @ U_final)\n",
    "\n",
    "    print(f\"The shape of U.t @ K @ U is: {UKU.shape}\")\n",
    "\n",
    "    approx_eigs = np.round(torch.diag(UKU).cpu().numpy(), 8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "    # For better comparison, we can compute Ritz values from subspace U by solving small generalized eigenproblem\n",
    "    # (U^T K U) c = mu (U^T M U) c, but U^T M U = I so just eig of UK_final\n",
    "    mu, Wsmall = np.linalg.eigh(UKU.cpu().numpy())\n",
    "    mu = np.real(mu)\n",
    "    # sort\n",
    "    idx = np.argsort(mu)\n",
    "    mu = mu[idx]\n",
    "    print(\"\\nLearned Ritz values (from U^T K U):\", np.round(mu[:5], 6))\n",
    "    print(\"Reference eigenvalues (first k):   \", np.round(eigvals[:5], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105bee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- approximation: 0.0065 actual: 0.0 ---\n",
      "--- approximation: 0.0736 actual: 0.0076 ---\n",
      "--- approximation: 0.2293 actual: 0.0303 ---\n",
      "--- approximation: 0.2624 actual: 0.0681 ---\n",
      "--- approximation: 0.2833 actual: 0.1212 ---\n",
      "--- approximation: 0.3811 actual: 0.1892 ---\n",
      "--- approximation: 0.4652 actual: 0.2722 ---\n",
      "--- approximation: 0.7194 actual: 0.3705 ---\n",
      "--- approximation: 1.0522 actual: 0.4834 ---\n",
      "--- approximation: 1.1064 actual: 0.6113 ---\n",
      "--- approximation: 1.1753 actual: 0.754 ---\n",
      "--- approximation: 1.7005 actual: 0.9117 ---\n",
      "--- approximation: 1.8233 actual: 1.0836 ---\n",
      "--- approximation: 1.904 actual: 1.27 ---\n",
      "--- approximation: 2.2072 actual: 1.4713 ---\n",
      "--- approximation: 2.4038 actual: 1.687 ---\n",
      "--- approximation: 2.4726 actual: 1.9172 ---\n",
      "--- approximation: 2.6417 actual: 2.1605 ---\n",
      "--- approximation: 2.8411 actual: 2.419 ---\n",
      "--- approximation: 3.1089 actual: 2.6903 ---\n",
      "--- approximation: 3.5563 actual: 2.9745 ---\n",
      "--- approximation: 3.8684 actual: 3.2753 ---\n",
      "--- approximation: 4.2156 actual: 3.587 ---\n",
      "--- approximation: 4.2874 actual: 3.9143 ---\n",
      "--- approximation: 4.3491 actual: 4.2555 ---\n",
      "--- approximation: 4.8871 actual: 4.6071 ---\n",
      "--- approximation: 5.4646 actual: 4.9696 ---\n",
      "--- approximation: 5.7512 actual: 5.3506 ---\n",
      "--- approximation: 6.2142 actual: 5.7442 ---\n",
      "--- approximation: 6.4931 actual: 6.1454 ---\n",
      "--- approximation: 6.6403 actual: 6.5674 ---\n",
      "--- approximation: 7.2263 actual: 6.9955 ---\n",
      "--- approximation: 7.32 actual: 7.2276 ---\n",
      "--- approximation: 7.3611 actual: 7.2332 ---\n",
      "--- approximation: 7.3697 actual: 7.2643 ---\n",
      "--- approximation: 7.3997 actual: 7.301 ---\n",
      "--- approximation: 7.4435 actual: 7.3609 ---\n",
      "--- approximation: 7.4754 actual: 7.3633 ---\n",
      "--- approximation: 7.4956 actual: 7.3767 ---\n",
      "--- approximation: 7.5369 actual: 7.4047 ---\n",
      "--- approximation: 7.5937 actual: 7.4222 ---\n",
      "--- approximation: 7.704 actual: 7.4495 ---\n",
      "--- approximation: 7.7304 actual: 7.4556 ---\n",
      "--- approximation: 7.7599 actual: 7.5081 ---\n",
      "--- approximation: 7.8256 actual: 7.5268 ---\n",
      "--- approximation: 7.8817 actual: 7.6074 ---\n",
      "--- approximation: 7.9567 actual: 7.6143 ---\n",
      "--- approximation: 7.9961 actual: 7.7081 ---\n",
      "--- approximation: 8.0652 actual: 7.7249 ---\n",
      "--- approximation: 8.7234 actual: 7.8346 ---\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(mu, eigvals):\n",
    "    print(f\"--- approximation: {np.round(i, 4)} actual: {np.round(j, 4)} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41e28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"loss\": loss,\n",
    "}, \"simple_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41252f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "loss = checkpoint[\"loss\"]\n",
    "\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
