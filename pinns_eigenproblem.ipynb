{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53342ca8-db05-456c-8e2b-5fc44e26698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import linalg\n",
    "from Mesh import Mesh\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc22b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors (double precision for better numerical stability)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70dc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Laplacian\n",
      "Computing eigen values\n"
     ]
    }
   ],
   "source": [
    "m = Mesh('data/coil_1.2_MM.obj')\n",
    "\n",
    "centroid = m.verts.mean(0)\n",
    "std_max = m.verts.std(0).max()\n",
    "\n",
    "verts_new = (m.verts - centroid)/std_max\n",
    "\n",
    "m = Mesh(verts = verts_new, connectivity = m.connectivity)\n",
    "\n",
    "print('Computing Laplacian')\n",
    "K, M = m.computeLaplacian()\n",
    "\n",
    "# following Finite Elements methodology \n",
    "# K is stiffness matrix, M is mass matrix\n",
    "# The problem to solve becomes \n",
    "# K*u = lambda * M*u\n",
    "print('Computing eigen values')\n",
    "eigvals, eigvecs = linalg.eigh(K,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42393004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send all relevant numpy arrays to torch tensors\n",
    "K = torch.from_numpy(K).to(device)\n",
    "M = torch.from_numpy(M).to(device)\n",
    "X = torch.from_numpy(m.verts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e635ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the paper we used 50 eigenvalues so set k to 50\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018a8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1, loss=204.857112\n",
      "Epoch 1000, loss=272.831905\n",
      "Epoch 2000, loss=261.983075\n",
      "Epoch 3000, loss=256.291715\n",
      "Epoch 4000, loss=251.767148\n",
      "Epoch 5000, loss=248.897578\n",
      "Epoch 6000, loss=246.573155\n",
      "Epoch 7000, loss=244.963168\n",
      "Epoch 8000, loss=243.130853\n",
      "Epoch 9000, loss=241.793273\n",
      "Epoch 10000, loss=240.961139\n",
      "Epoch 11000, loss=239.896776\n",
      "Epoch 12000, loss=239.040704\n",
      "Epoch 13000, loss=238.536430\n",
      "Epoch 14000, loss=237.844869\n",
      "Epoch 15000, loss=237.333890\n",
      "Epoch 16000, loss=236.907581\n",
      "Epoch 17000, loss=236.504316\n",
      "Epoch 18000, loss=236.078709\n",
      "Epoch 19000, loss=235.984312\n",
      "Epoch 20000, loss=235.521537\n",
      "Epoch 21000, loss=235.367362\n",
      "Epoch 22000, loss=235.051700\n",
      "Epoch 23000, loss=234.925706\n",
      "Epoch 24000, loss=234.647112\n",
      "Epoch 25000, loss=234.523454\n",
      "Epoch 26000, loss=234.294163\n",
      "Epoch 27000, loss=234.180143\n",
      "Epoch 28000, loss=234.053797\n",
      "Epoch 29000, loss=233.918480\n",
      "Epoch 30000, loss=233.760884\n",
      "Epoch 31000, loss=233.615474\n",
      "Epoch 32000, loss=233.639725\n",
      "Epoch 33000, loss=233.394583\n",
      "Epoch 34000, loss=233.350127\n",
      "Epoch 35000, loss=233.256226\n",
      "Epoch 36000, loss=233.141458\n",
      "Epoch 37000, loss=233.122411\n",
      "Epoch 38000, loss=232.991761\n",
      "Epoch 39000, loss=233.030547\n",
      "Epoch 40000, loss=232.884943\n",
      "Epoch 41000, loss=232.857123\n",
      "Epoch 42000, loss=232.711559\n",
      "Epoch 43000, loss=232.664653\n",
      "Epoch 44000, loss=232.568916\n",
      "Epoch 45000, loss=232.537151\n",
      "Epoch 46000, loss=232.490979\n",
      "Epoch 47000, loss=232.396144\n",
      "Epoch 48000, loss=232.371831\n",
      "Epoch 49000, loss=232.296716\n",
      "Epoch 50000, loss=232.244392\n",
      "Epoch 51000, loss=232.208943\n",
      "Epoch 52000, loss=232.180353\n",
      "Epoch 53000, loss=232.114517\n",
      "Epoch 54000, loss=232.110296\n",
      "Epoch 55000, loss=232.032138\n",
      "Epoch 56000, loss=231.992525\n",
      "Epoch 57000, loss=231.957954\n",
      "Epoch 58000, loss=231.913839\n",
      "Epoch 59000, loss=231.873645\n",
      "Epoch 60000, loss=231.843234\n",
      "Epoch 61000, loss=231.799424\n",
      "Epoch 62000, loss=231.767935\n",
      "Epoch 63000, loss=231.743093\n",
      "Epoch 64000, loss=231.704988\n",
      "Epoch 65000, loss=231.682518\n",
      "Epoch 66000, loss=231.661393\n",
      "Epoch 67000, loss=231.621636\n",
      "Epoch 68000, loss=231.608428\n",
      "Epoch 69000, loss=231.565016\n",
      "Epoch 70000, loss=231.534415\n",
      "Epoch 71000, loss=231.507206\n",
      "Epoch 72000, loss=231.497554\n",
      "Epoch 73000, loss=231.459831\n",
      "Epoch 74000, loss=231.435329\n",
      "Epoch 75000, loss=231.413315\n",
      "Epoch 76000, loss=231.385795\n",
      "Epoch 77000, loss=231.362382\n",
      "Epoch 78000, loss=231.346665\n",
      "Epoch 79000, loss=231.316996\n",
      "Epoch 80000, loss=231.297957\n",
      "Epoch 81000, loss=231.272992\n",
      "Epoch 82000, loss=231.254459\n",
      "Epoch 83000, loss=231.233122\n",
      "Epoch 84000, loss=231.209666\n",
      "Epoch 85000, loss=231.189483\n",
      "Epoch 86000, loss=231.169850\n",
      "Epoch 87000, loss=231.151588\n",
      "Epoch 88000, loss=231.134794\n",
      "Epoch 89000, loss=231.116112\n",
      "Epoch 90000, loss=231.092503\n",
      "Epoch 91000, loss=231.073797\n",
      "Epoch 92000, loss=231.057393\n",
      "Epoch 93000, loss=231.037742\n",
      "Epoch 94000, loss=231.022017\n",
      "Epoch 95000, loss=231.002962\n",
      "Epoch 96000, loss=230.985074\n",
      "Epoch 97000, loss=230.968264\n",
      "Epoch 98000, loss=230.951301\n",
      "Epoch 99000, loss=230.936062\n",
      "Epoch 100000, loss=230.919205\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network that maps coordinates -> k outputs per node\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=k, hidden=[64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h, dtype=torch.double))\n",
    "            layers.append(nn.SiLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim, dtype=torch.double))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # returns (N, k)\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = MLP().to(device)\n",
    "# Initialize all layers (Xavier), final layer small\n",
    "for name, p in model.named_parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "for p in model.net[-1].parameters():  # last Linear\n",
    "    if p.ndim == 2:\n",
    "        nn.init.normal_(p, std=1e-3)\n",
    "    else:\n",
    "        nn.init.zeros_(p)\n",
    "\n",
    "# Helper: given A (N x k), return M-orthonormalized U = A (A^T M A)^{-1/2}\n",
    "def m_orthonormalize(A, M):\n",
    "    # A: (N, k), M: (N, N)\n",
    "    # compute B = A^T M A (k x k)\n",
    "    B = A.T @ (M @ A)  # k x k\n",
    "    # symmetrize B\n",
    "    B = 0.5*(B + B.T)\n",
    "    # compute inverse sqrt of B via eigendecomposition (k small)\n",
    "    s, Q = torch.linalg.eigh(B)  # s are eigenvalues\n",
    "    # regularize small eigenvalues\n",
    "    s_clamped = torch.clamp(s, min=1e-12)\n",
    "    inv_sqrt = Q @ torch.diag(1.0/torch.sqrt(s_clamped)) @ Q.T\n",
    "    U = A @ inv_sqrt\n",
    "    return U\n",
    "\n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "max_epochs = 100_000\n",
    "print_every = 1_000\n",
    "loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_start)\n",
    "decay_factor = (lr_end / lr_start) ** (1 / max_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    A = model(X)  # N x k\n",
    "    U = m_orthonormalize(A, M)  # U is M-orthonormal\n",
    "\n",
    "    UKU = U.T @ (K @ U)\n",
    "    # symmetrize for numerical safety\n",
    "    UKU = 0.5*(UKU + UKU.T)\n",
    "    loss = torch.trace(UKU)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if epoch % print_every == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}, loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46439ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of U.t @ K @ U is: torch.Size([50, 50])\n",
      "\n",
      "Learned Ritz values (from U^T K U): [0.000266 0.19767  0.232172 0.279417 0.343546]\n",
      "Reference eigenvalues (first k):    [0.       0.007574 0.030308 0.068146 0.121208]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    A_final = model(X)\n",
    "\n",
    "    U_final= m_orthonormalize(A_final, M)\n",
    "    UKU = U_final.T @ (K @ U_final)\n",
    "\n",
    "    print(f\"The shape of U.t @ K @ U is: {UKU.shape}\")\n",
    "\n",
    "    approx_eigs = np.round(torch.diag(UKU).cpu().numpy(), 8)\n",
    "    # For better comparison, we can compute Ritz values from subspace U by solving small generalized eigenproblem\n",
    "    # (U^T K U) c = mu (U^T M U) c, but U^T M U = I so just eig of UK_final\n",
    "    mu, Wsmall = np.linalg.eigh(UKU.cpu().numpy())\n",
    "    mu = np.real(mu)\n",
    "    # sort\n",
    "    idx = np.argsort(mu)\n",
    "    mu = mu[idx]\n",
    "    print(\"\\nLearned Ritz values (from U^T K U):\", np.round(mu[:5], 6))\n",
    "    print(\"Reference eigenvalues (first k):   \", np.round(eigvals[:5], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =  plt.figure(figsize=(10,10)).add_subplot(projection='3d')\n",
    "surf = ax.plot_trisurf(m.verts[:,0], m.verts[:,1], m.verts[:,2], triangles = m.connectivity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "A_final = model(X)\n",
    "U_final = m_orthonormalize(A_final, M)\n",
    "\n",
    "\n",
    "Ksmall = U_final.T @ K @ U_final\n",
    "Msmall = U_final.T @ M @ U_final\n",
    "L = torch.linalg.cholesky(Msmall)\n",
    "Linv = torch.linalg.inv(L)\n",
    "Khat = Linv @ Ksmall @ Linv.T\n",
    "mu, _ = torch.linalg.eigh(Khat)\n",
    "\n",
    "\n",
    "mu = mu.cpu().numpy()\n",
    "idx = np.argsort(mu)\n",
    "mu = mu[idx]\n",
    "\n",
    "\n",
    "print(f\"The shape of U.T @ K @ U is: {U_final.T @ K @ U_final.shape}\")\n",
    "print(\"\\nLearned Ritz values (from U^T K U):\", np.round(mu[:5], 6))\n",
    "print(\"Reference eigenvalues (first k): \", np.round(eigvals[:5], 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltapinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
